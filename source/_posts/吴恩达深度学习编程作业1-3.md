---
title: 吴恩达深度学习编程作业1-3
comments: true
toc: true
top: false
mathjax: true

categories:
- [神经网络,学习笔记]
tags: [神经网络]

date: 2020-06-11 22:33:21
---

吴恩达深度学习课程《神经网络和深度学习》第三周（浅层神经网络）的编程作业。

<!-- more -->

## 说明

感谢作者[阿宽](https://blog.csdn.net/u013733326)提供作业题目[【中文】【吴恩达课后编程作业】Course 1 - 神经网络和深度学习 - 第三周作业](https://blog.csdn.net/u013733326/article/details/79702148)

数据集以及题目可在该博文中查看与下载。

题目为建立 单隐藏层的神经网络 对平面数据进行分类。

下载的文件中：

- testCases 提供一些测试案例来评估正确性，可打开该文件来查看一些测试用例（我这里就不写测试函数了，原博文中有编写）。
- planar_utils 提供在该作业中需要使用的一些功能。

需要的库：

- numpy 数据科学计算的库
- matplotlib 绘图
- sklearn 机器学习工具包
- 

## 导包

```python
import numpy as np
import matplotlib.pyplot as plt
from testCases import *
import sklearn
import sklearn.datasets
import sklearn.linear_model
from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets
plt.rcParams['axes.unicode_minus']=False
plt.rcParams['font.family']='SimHei'
%matplotlib inline

# 设置一个随机数种子，与原博文保持一致
np.random.seed(1)
```



## 加载数据集

查看一下X与Y的维度和类型

```python
X,Y=load_planar_dataset()
print("X的维度为：",X.shape,"X的类型为：",type(X))
print("Y的维度为：",Y.shape,"Y的类型为：",type(Y))
```

结果如图：

[![image-20200611052153391](https://gitee.com/lluuiq/blog_img/raw/master/img/20200611052154.png)](https://gitee.com/lluuiq/blog_img/raw/master/img/20200611052154.png)

  

可知一共有400个样本。其中X的维度中的2为对应的平面图的X轴、Y轴坐标。

Y的取值为0或1，为样本的标签。  

## 将数据集可视化

```python
plt.scatter(X[0, :], X[1, :], c=np.squeeze(Y), s=40, cmap=plt.cm.Spectral)
```

  

结果如图：

[![image-20200611051939893](https://gitee.com/lluuiq/blog_img/raw/master/img/20200611051941.png)](https://gitee.com/lluuiq/blog_img/raw/master/img/20200611051941.png)

其中y=0的标签在图中为红色点。

y=1的标签在图中为蓝色点。  

## 查看逻辑回归的准确率

```python
# 使用sklearn库建立逻辑回归模型
clf = sklearn.linear_model.LogisticRegressionCV()
clf.fit(X.T,Y.T)

plot_decision_boundary(lambda x: clf.predict(x), X, np.squeeze(Y)) #绘制决策边界
plt.title("逻辑回归") #图标题
LR_predictions  = clf.predict(X.T) #预测结果
print ("逻辑回归的准确性：",float((np.dot(Y, LR_predictions) + np.dot(1 - Y,1 - LR_predictions)) / float(Y.size) * 100),
       "%")
```

结果如图：

[![image-20200611054524905](https://gitee.com/lluuiq/blog_img/raw/master/img/20200611054526.png)](https://gitee.com/lluuiq/blog_img/raw/master/img/20200611054526.png)

47%表示的为预测正确的所占百分比。

由结果图可知，逻辑回归的线性决策边界并不能很好的区分这些数据，故正确率仅有47%。  

## 构建单隐藏层神经网络

### 流程：

- 定义神经网络结构
- 初始化模型参数
- 进行梯度下降（正向、反向传播、计算代价函数、更新参数）

其中正向传播与反向传播的公式：

正向传播（左），反向传播（右）

[![image-20200611065022546](https://gitee.com/lluuiq/blog_img/raw/master/img/20200611065023.png)](https://gitee.com/lluuiq/blog_img/raw/master/img/20200611065023.png)

可以把这些功能整合到一个模型函数中。

接下来先根据步骤来逐一编写实现代码。  

### 定义神经网络结构

在这一步定义好神经网络的输入层单元的数量、隐藏层单元的数量、输出层单元的数量。

```python
def layer_sizes(X , Y):
    """
    参数：
     X - 输入特征,维度为（输入特征的数量，样本数）
     Y - 标签，维度为（输出的数量，样本数）
    
    返回：
     n_x - 输入层单元的数量
     n_h - 隐藏层单元的数量
     n_y - 输出层单元的数量
    """
    n_x = X.shape[0] 
    # 手动定义为4个隐藏单元
    n_h = 4 
    n_y = Y.shape[0] 
    
    return (n_x,n_h,n_y)
```

### 定义模型参数初始化函数

- 定义了随机数种子为2，确保结果一致。
- 不像第二周作业一样用0初始化，改为随机数初始化，不然梯度下降失去作用。

```python
def initialize_parameters( n_x , n_h ,n_y):
    """
    参数：
        n_x - 输入层单元的数量
        n_h - 隐藏层单元的数量
        n_y - 输出层单元的数量

    返回：
        params - 包含参数的字典：
            W1 - 权重矩阵,维度为（n_h，n_x）
            b1 - 偏向量，维度为（n_h，1）
            W2 - 权重矩阵，维度为（n_y，n_h）
            b2 - 偏向量，维度为（n_y，1）
    """
    
    # 定义随机数种子，确保结果一致
    np.random.seed(2)
    
    # 使用随机数初始化
    W1 = np.random.randn(n_h,n_x) * 0.01
    b1 = np.zeros(shape=(n_h, 1))
    W2 = np.random.randn(n_y,n_h) * 0.01
    b2 = np.zeros(shape=(n_y, 1))

    parameters = {
        "W1" : W1,
        "b1" : b1,
        "W2" : W2,
        "b2" : b2 
    }

    return parameters
```

### 定义正向传播函数

- 这里将初始化函数的返回值（字典作为传入参数）
- 隐藏层使用的激活函数是tanh。输出层使用sigmoid，因为想要输出结果的取值范围在[0,1]·。

```python
def forward_propagation( X , parameters ):
    """
    参数：
         X - 维度为（n_x，样本数）的输入特征。
         parameters - 初始化函数（initialization_parameters）的输出
    
    返回：
         A2 - 使用sigmoid()函数计算的第二次激活后的数值
         cache - 包含“Z1”，“A1”，“Z2”和“A2”的字典类型变量
     """
    # 读取初始化后的参数
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    
    # 正向传播，最终输出A2
    Z1 = np.dot(W1 , X) + b1
    A1 = np.tanh(Z1) # 隐藏层使用numpy自带的tanh函数
    Z2 = np.dot(W2 , A1) + b2
    A2 = sigmoid(Z2) # 输出层使用sigmoid激活函数

    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}
    
    return (A2, cache)
```

### 定义计算代价函数的函数

```python
def compute_cost(A2,Y,parameters):
    """
    计算交叉熵代价函数
    J=1/m * ∑损失函数
    损失函数= -[Y*log(A2)+(1-Y)*log(1-A2)]
    这里 ∑损失函数 可以用向量来表示。
    
    参数：
         A2 - 正向传播最后的输出结果，即损失函数公式中的y帽
         Y - 标签
         parameters - 初始化模型参数函数的字典返回值
    
    返回：
         代价 - 交叉熵成本给出方程（13）
    """
    # 获取样本数
    m = X.shape[1]
    
    # 获取模型参数W1与W2
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    
    #计算代价
    logprobs  = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))
    cost = - np.sum(logprobs) / m
    cost = float(np.squeeze(cost))
    
    return cost
```

### 定义反向传播函数

这里需要注意的是tanh函数 A=g(Z) 的导数为 1−A2

```python
def backward_propagation(parameters,cache,X,Y):
    """
    参数：
     parameters - 包含我们的参数的一个字典类型的变量。
     cache - 包含“Z1”，“A1”，“Z2”和“A2”的字典类型的变量。
     X - 输入数据，维度为（2，数量）
     Y - “True”标签，维度为（1，数量）
    
    返回：
     grads - 包含W和b的导数一个字典类型的变量。
    """
    # 获取样本数
    m = X.shape[1]
    
    # 获取模型参数W1与w2
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    
    # 获取激活函数的输出A1与A2
    A1 = cache["A1"]
    A2 = cache["A2"]
    
    # 计算导数
    dZ2= A2 - Y
    dW2 = (1 / m) * np.dot(dZ2, A1.T)
    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)
    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))
    dW1 = (1 / m) * np.dot(dZ1, X.T)
    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)
    
    grads = {
        "dW1": dW1,
        "db1": db1,
        "dW2": dW2,
        "db2": db2
    }
    return grads
```

### 定义更新参数的函数

```python
def update_params(parameters,grads,lr=1.2):
    """
    参数：
     parameters - 包含模型参数的字典类型的变量。
     grads - 包含导数的字典类型的变量。
     lr(learning rate) - 学习速率
    
    返回：
     parameters - 更新后的参数的 字典类型的变量。
    """
    # 读取原模型参数
    W1,W2 = parameters["W1"],parameters["W2"]
    b1,b2 = parameters["b1"],parameters["b2"]
    
    # 读取模型参数的导数
    dW1,dW2 = grads["dW1"],grads["dW2"]
    db1,db2 = grads["db1"],grads["db2"]
    
    # 更新参数
    W1 = W1 - lr * dW1
    b1 = b1 - lr * db1
    W2 = W2 - lr * dW2
    b2 = b2 - lr * db2
    
    parameters = {
        "W1": W1,
        "b1": b1,
        "W2": W2,
        "b2": b2
    }
    return parameters
```

### 打包成模型函数

```python
def nn_model(X,Y,n_h,num_iterations,print_cost=False):
    """
    参数：
        X - 输入特征,维度为（2，样本数）
        Y - 标签，维度为（1，样本数）
        n_h - 隐藏层的数量
        num_iterations - 梯度下降的迭代次数
        print_cost - 如果为True，则每1000次迭代打印一次成本数值
    
    返回：
        parameters - 模型的参数，用于预测。
     """
     
    np.random.seed(3) #指定随机种子
    
    # 调用设置网络结构函数
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]
    
    #
    parameters  = initialize_parameters(n_x,n_h,n_y)
    W1 = parameters ["W1"]
    b1 = parameters ["b1"]
    W2 = parameters ["W2"]
    b2 = parameters ["b2"]
    
    for i in range(num_iterations):
        A2 , cache = forward_propagation(X,parameters )
        cost = compute_cost(A2,Y,parameters )
        grads = backward_propagation(parameters ,cache,X,Y)
        parameters = update_params(parameters ,grads,lr = 0.5)
        
        if print_cost:
            if i%1000 == 0:
                print("第 ",i," 次循环，成本为："+str(cost))
                
    return parameters
```

### 出现的问题：

根据[【中文】【吴恩达课后编程作业】Course 1 - 神经网络和深度学习 - 第三周作业](https://blog.csdn.net/u013733326/article/details/79702148)的步骤，在测试模型代码时发现与原文不同。

这是我的结果：

[![image-20200611081307164](https://gitee.com/lluuiq/blog_img/raw/master/img/20200611082222.png)](https://gitee.com/lluuiq/blog_img/raw/master/img/20200611082222.png)

这是原文的结果：

[![image-20200611081331758](https://gitee.com/lluuiq/blog_img/raw/master/img/20200611082252.png)](https://gitee.com/lluuiq/blog_img/raw/master/img/20200611082252.png)

前面的测试结果都一致，这里以为是自己哪一步出了问题导致在打包好的模型函数里调用错误，以至于把代码全改回和原文一致，结果还是不一样。

最后看评论和与同学进行讨论发现都存在此问题，但用在训练集与测试集时是正常的。

推测原因应该是此时 numpy相同的随机数种子，生成的随机数却不一样导致的吧。所以现在生成的随机数用在该段代码中会出现错误，如结果提示中的除数为0。

## 进行预测

### 编写预测函数

```python
def predict(parameters,X):
    """
    参数：
     parameters - 包含模型参数的字典类型的变量。
     X - 输入特征（n_x，m）
    
    返回:
     predictions - 预测的向量（红色：0 /蓝色：1）
     """
    # 计算A2
    A2 , cache = forward_propagation(X,parameters)
    # np.round 求平均
    predictions = np.round(A2)
    
    return predictions
```

### 查看准确率

根据博主最后提出的更改隐藏单元数问题，这里设置隐藏单元为5准确率最高，但出于对比，还是设置为了4。

```python
parameters = nn_model(X, Y, n_h = 4, num_iterations=10000, print_cost=True)

#绘制边界
plot_decision_boundary(lambda x: predict(parameters, x.T), X, np.squeeze(Y))
plt.title("隐藏单元数为" + str(4)+"时的决策边界")

predictions = predict(parameters, X)
print ('准确率:', float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100),'%')
```

结果：

[![image-20200611082210060](https://gitee.com/lluuiq/blog_img/raw/master/img/20200611082213.png)](https://gitee.com/lluuiq/blog_img/raw/master/img/20200611082213.png)