---
title: 吴恩达深度学习编程作业2-3
comments: true
toc: true
top: false
mathjax: true
categories:
  - - 神经网络
    - 学习笔记
tags:
  - 神经网络
date: 2020-07-04 09:28:35
---

吴恩达深度学习课程《改善深层神经网络：超参数调试、正则化以及优化》第三周（超参数调试、Batch正则化和程序框架）的编程作业。

使用tensorflow2.1完成，既是完成编程作业，同时也是入门学习tensorflow2。

<!-- more -->

## 说明

> 题目与参考作业的地址：[【中文】【吴恩达课后编程作业】Course 2 - 改善深层神经网络 - 第三周作业 - TensorFlow入门](https://blog.csdn.net/u013733326/article/details/79971488) 
>
> 相关数据集与前提代码在该博文中下载。

<br>

**原文使用的是tensorflow1.x版本，本文使用的是tensorflow2.1**

为对比原文，采取与原文一致的过程，而使用tensorflow2.1实现。

<br>

作业要求：

学习tensorflow的初始化变量、建立一个会话、训练算法、实现一个神经网络。

tensorflow的安装可以参考我的另一篇文章：[TensorFlow2.1 安装以及开启GPU加速](https://lluuiq.com/post/202007030500/)

## 导包

```python
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.python.framework import ops
import matplotlib.pyplot as plt
%matplotlib inline
 
plt.rcParams['font.sans-serif']=['SimHei'] # 显示中文标签
plt.rcParams['axes.unicode_minus']=False # 显示负号

import h5py
import tf_utils
import time

np.random.seed(1)
```

## 入门引导

#### one

损失函数loss的公式：$L(\hat{y},y) = (\hat{y}^{(i)} - y^{(i)})^{2}$

```python
# 定义常量张量 y_hat 与 y
y_hat = tf.constant(36, name='y_hat')  # y_hat = 36
y = tf.constant(39, name='y')  # y= 39

# 定义变量张量loss
loss = tf.Variable((y-y_hat)**2, name='loss')  #loss = (39-36)的平方，即9
print(loss)
```

运行结果：

![image-20200704115614905](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704115614905.png)

可以看到loss的值为9（numpy=9）。

与 tf1.X 相比，tf2不用再建立会话了。

<br>

tensorflow参与运算的单位是张量，易于理解的话，一维数组就是一维张量，二维数组即矩阵就是二维张量，N*N的数组就是N维张量。

`tf.constant` 创建一个常量的张量，无法改变其值。

`tf.Variable`创建一个变量的张量。

参数有 `value,dtype=None,shape=None,name,verify_shape=Flase`。

其中只有value是必须的值。

#### two

创建两个常量张量a与b，a=2，b=10

c 等于a*b = 20

```python
a = tf.constant(2)  
b = tf.constant(10)
c = tf.multiply(a,b)
print(c)
```

运行结果：

![image-20200704115602832](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704115602832.png)

tf2里就可以直接输出。

`tf.multiply(a,b)`作用是计算 a*b ，并返回值。

#### three

原文展示了tf1.x的占位符（placeholder）用法，这个特性在tf2里被取消了 。

### 线性函数

计算 $Y=W*X+b$，其中W与X是随机矩阵，b为随机向量。

规定W维度为（4，3），X维度为（3，1），b维度为（4，1）。可计算出Y的维度为（4，1）

```python
def linear_function():
    """
    实现一个线性功能：
        初始化W，类型为tensor的随机变量，维度为(4,3)
        初始化X，类型为tensor的随机变量，维度为(3,1)
        初始化b，类型为tensor的随机变量，维度为(4,1)
    返回：
        # result - 运行了session后的结果，运行的是Y = WX + b
        Y - 执行Y = WX + b 的结果，若需要ndarray需要使用 .numpy()转换
    """

    np.random.seed(1)  # 指定随机种子

    X = np.random.randn(3, 1)
    W = np.random.randn(4, 3)
    b = np.random.randn(4, 1)

    Y = tf.add(tf.matmul(W, X), b)  # tf.matmul是矩阵乘法
    # Y = tf.matmul(W,X) + b #也可以以写成这样子

    """
        原文中的创建会话可以不使用了
    """
#     #创建一个session并运行它
#     sess = tf.Session()
#     result = sess.run(Y)

#     #session使用完毕，关闭它
#     sess.close()

    return Y.numpy()
```

<br>

然后运行函数

```python
Y = linear_function()
print(Y)
```

![image-20200704123233692](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704123233692.png)

这里在函数返回值时 使用了  `Var.numpy()` 将tensor类型的Var（即tensorflow中的数据类型 -- 张量）转为numpy的ndarray。

可以使用 `tf.convert_to_tensor(Var)` 再将Var转为tensor。

```python
tf.convert_to_tensor(Y)
```

![image-20200704123300271](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704123300271.png)

当然这里与numpy()一样可以用变量接收返回值 `Y = tf.convert_to_tensor(Y) `

### 计算sigmoid

tf2直接省去了tf1的创建会话，所以使用起来简单了很多（甚至不用定义函数来执行了，但这里为了与原文对比所以还是定义了函数）

```python
def sigmoid(z):
    """
    实现使用sigmoid函数计算z

    参数：
        z - 输入的值，标量或矢量

    返回：
        result - 用sigmoid计算z的值

    """

#     #创建一个占位符x，名字叫“x”
#     x = tf.placeholder(tf.float32,name="x")

#     #计算sigmoid(z)
#     sigmoid = tf.sigmoid(x)

#     #创建一个会话，使用方法二
#     with tf.Session() as sess:
#         result = sess.run(sigmoid,feed_dict={x:z})

    """
        tf1的一堆花里胡哨的东西 都可以去掉了。
    """
    
    # tf2的sigmoid必须传入float，否则报错。
    z = float(z)
    result = tf.sigmoid(z)

    return result.numpy()
```

这里要使用float类型作为参数，否则报错。

同上，返回值使用`numpy()`返回ndarry，使得与原文的返回类型一致。

<br>

测试一下：

```python
print("sigmoid(0) = " + str(sigmoid(0)))
print("sigmoid(12) = " + str(sigmoid(12)))
```

![image-20200704123611350](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704123611350.png)

与原文区别就是`sigmoid(12)`变成了 0.9999938 而不是 0.999994 ，精度少了一位，基本没啥区别。



### 计算代价（成本）

tensorflow自带了计算成本的函数，直接用就是。

`cross_entropy` 交叉熵

```python
tf.nn.sigmoid_cross_entropy_with_logits(
    labels=None, # Y 
    logits=None, # Y帽
    name=None
)
```

### 使用独热编码（0，1编码）

什么是独热编码？如原文所举的例子

一个标签是[1 2 3 0 2 1]，有6个元素（样本），m = 6。

而分类的类别个数有4个（0, 1, 2, 3），C=4。

![image-20200704133014814](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704133014814.png)

其实就相当于把标签的每个元素都拓展成值为0或1的向量。对应原来的值，将向量中对应下标的元素置为1。

这样做的优势就在于将原本的向量扩充到了欧式空间，并且多元分类转为了二元分类问题， 扩充了特征。

```python
tf.one_hot(
    indices, 
    depth, 
    on_value=None, 
    off_value=None, 
    axis=None, 
    dtype=None, 
    name=None
)
```

<br>

定义一个函数，参数为标签与C，返回该标签的独热编码

```python
def one_hot_matrix(lables, C):
    """
    创建一个矩阵，其中第i行对应第i个类号，第j列对应第j个训练样本
    所以如果第j个样本对应着第i个标签，那么entry (i,j)将会是1

    参数：
        lables - 标签向量
        C - 分类数

    返回：
        one_hot - 独热矩阵

    """

    # 创建一个tf.constant，赋值为C，名字叫C
    C = tf.constant(C, name="C")

    # 使用tf.one_hot，注意一下axis
    one_hot_matrix = tf.one_hot(indices=lables, depth=C, axis=0)

    '''
        原文TF1的内容
    '''
#     #创建一个session
#     sess = tf.Session()

#     #运行session
#     one_hot = sess.run(one_hot_matrix)

#     #关闭session
#     sess.close()

    return one_hot_matrix.numpy()
```

测试一下：

```python
labels = np.array([1, 2, 3, 0, 2, 1])
one_hot_matrix = one_hot_matrix(labels, C=4)
print(one_hot_matrix)
```

运行结果：

![image-20200704134506113](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704134506113.png)

其实tf简化后没必要再定义函数了 ？ 

![image-20200704134919021](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704134919021.png)

### 初始化为0和1

学习如何用0或者1初始化一个向量，即`tf.zeros()`与`tf.ones()`

```python
shape = 3
one = tf.ones(shape).numpy()
zero = tf.zeros(shape).numpy()
print("one: ",one)
print("zero: ",zero)
```

![image-20200704135230451](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704135230451.png)

## 使用TensorFlow构建你的第一个神经网络

### 说明：

原文提到实现模型（tensorflow1.X）需要两个步骤：

1. 创建计算图
2. 运行计算图

那么来看下tensorflow2里如何实现模型

 <br>

**任务目标：**根据图片上的手势来识别数字。

**训练集：** 从0到5的数字的1080张图片，像素64*64，每个数字有180张图片。

**测试集：** 从0到5的数字的120张图片，像素64*64，每个数字有5张图片。

![hands](https://img-blog.csdn.net/20180417110431384?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM3MzMzMjY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 加载数据集

使用 `tf_utils`文件里的函数读取数据

```python
X_train_orig , Y_train_orig , X_test_orig , Y_test_orig , classes = tf_utils.load_dataset()
```

查看一下数据的维度

```shape
print(X_train_orig.shape)
print(Y_train_orig.shape)
print(X_test_orig.shape)
print(Y_test_orig.shape)
```

![image-20200704140346416](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704140346416.png)

接下来要做的为：

1. 事情和之前的作业一样，对输入特征 X 进行归一化以及转为 `( 3*64*64，样本数(m) )` 的矩阵。
2. 将标签Y转为独热矩阵。

这部分没什么变化，直接用原文的代码即可。

```python
# 维度转为 (3*64*64,m)
X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0],-1).T #每一列就是一个样本
X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0],-1).T

# 归一化数据
X_train = X_train_flatten / 255
X_test = X_test_flatten / 255

# 转为独热矩阵 （调用了tf_utils的代码）
Y_train = tf_utils.convert_to_one_hot(Y_train_orig,6)
Y_test = tf_utils.convert_to_one_hot(Y_test_orig,6)

print("训练集样本数 = " + str(X_train.shape[1]))
print("测试集样本数 = " + str(X_test.shape[1]))
print("X_train.shape: " + str(X_train.shape))
print("Y_train.shape: " + str(Y_train.shape))
print("X_test.shape: " + str(X_test.shape))
print("Y_test.shape: " + str(Y_test.shape))
```

![image-20200704140944728](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704140944728.png)

### 创建placeholders

tf2没有占位符，跳过

### 初始化参数

tf2里去掉了`.contrib.layers`，而且`get_variable`需要使用`tf.compat.v1.get_variable`。至于区别因为现在刚入门，暂时不去细想了。

这里使用tf2的keras接口调用来实现。详细API参考官方文档：[Module: tf.keras.initializers](https://tensorflow.google.cn/api_docs/python/tf/keras/initializers?hl=en)

![image-20200704175859478](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704175859478.png)

原文中使用的是xavier，并且使用默认值 uniform=False，故这里应该使用 `tf.keras.initializers.GlorotUniform`

![image-20200704180146194](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704180146194.png)

可以看到，参数只有一个seed，定义随机数种子。

<br>

再看一下用法

![image-20200704180622698](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704180622698.png)

先初始化一个张量看看情况

```python
shape = (25,12288)
seed = 1
initializer = tf.keras.initializers.GlorotUniform(seed)
W1 = tf.Variable(initializer(shape=shape),name='W1')
print(W1)
```

![image-20200704181054555](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704181054555.png)

再试试zero初始化参数b，这里`tf.zeros_initializer()`其实在tf2里也可以用，但是既然用keras了，就用keras的接口吧

```python
# tf.Variable(tf.zeros_initializer()(shape=(25, 1)), name='b1')
tf.Variable(tf.keras.initializers.Zeros()(shape=(25, 1)), name='b1')
```

![image-20200704182349147](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704182349147.png)

可以看到，W1张量的name为W1，shape=(25, 12288)，dtype=float32，与原文一致，至于具体的值就不一定一致了。

这里b我没有用变量接收返回值，反正也只是测试。可以看到信息与原文也一致。

<br>

初次尝试成功了，接下来就来实现原文中的函数，原文是定义一个函数，没有传入参数，返回3层模型参数的tensor字典。

```python
def initialize_parameters():
    """
    初始化神经网络的参数，参数的维度如下：
        W1 : [25, 12288]
        b1 : [25, 1]
        W2 : [12, 25]
        b2 : [12, 1]
        W3 : [6, 12]
        b3 : [6, 1]

    返回：
        parameters - 包含了W和b的字典


    """

#     tf.set_random_seed(1) #指定随机种子

#     W1 = tf.get_variable("W1",[25,12288],initializer=tf.contrib.layers.xavier_initializer(seed=1))
#     b1 = tf.get_variable("b1",[25,1],initializer=tf.zeros_initializer())
#     W2 = tf.get_variable("W2", [12, 25], initializer = tf.contrib.layers.xavier_initializer(seed=1))
#     b2 = tf.get_variable("b2", [12, 1], initializer = tf.zeros_initializer())
#     W3 = tf.get_variable("W3", [6, 12], initializer = tf.contrib.layers.xavier_initializer(seed=1))
#     b3 = tf.get_variable("b3", [6, 1], initializer = tf.zeros_initializer())

    # 定义随机种子
    seed = 1

    # 初始化模型参数，这里省去了initializer赋值操作。
    W1 = tf.Variable(tf.keras.initializers.GlorotUniform(seed)
                     (shape=(25, 12288)), name='W1')
    b1 = tf.Variable(tf.keras.initializers.Zeros()(shape=(25, 1)), name='b1')

    W2 = tf.Variable(tf.keras.initializers.GlorotUniform(seed)
                     (shape=(12, 25)), name='W2')
    b2 = tf.Variable(tf.keras.initializers.Zeros()(shape=(12, 1)), name='b2')

    W3 = tf.Variable(tf.keras.initializers.GlorotUniform(seed)
                     (shape=(6, 12)), name='W3')
    b3 = tf.Variable(tf.keras.initializers.Zeros()(shape=(6, 1)), name=' b3')

    parameters = {
        "W1": W1,
        "b1": b1,
        "W2": W2,
        "b2": b2,
        "W3": W3,
        "b3": b3
    }

    return parameters
```

<br>

**测试：**

`tf.reset_default_graph()`改为`tf.compat.v1.reset_default_graph()`，使其使用1.X 版本的reset_default_graph函数

```python
tf.compat.v1.reset_default_graph()  # 用于清除默认图形堆栈并重置全局默认图形。

parameters = initialize_parameters()
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
```

运行结果：

![image-20200704183445919](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200704183445919.png)

因为版本的区别，tensorflow2输出tensor会有个numpy信息直接查看值。

对比tensor信息可以看到与原文一致。

### 正向传播

将原本的计算代码改为使用tensorflow实现

```python
def forward_propagation(X, parameters):
    """
    实现一个模型的前向传播，模型结构为LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX

    参数：
        X - 输入数据的占位符，维度为（输入节点数量，样本数量）
        parameters - 包含了W和b的参数的字典

    返回：
        Z3 - 最后一个LINEAR节点的输出

    """

    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    W3 = parameters['W3']
    b3 = parameters['b3']

    Z1 = tf.add(tf.matmul(W1, X), b1)        # Z1 = np.dot(W1, X) + b1
    # Z1 = tf.matmul(W1,X) + b1             #也可以这样写
    A1 = tf.nn.relu(Z1)                    # A1 = relu(Z1)
    Z2 = tf.add(tf.matmul(W2, A1), b2)     # Z2 = np.dot(W2, a1) + b2
    A2 = tf.nn.relu(Z2)                    # A2 = relu(Z2)
    Z3 = tf.add(tf.matmul(W3, A2), b3)     # Z3 = np.dot(W3,Z2) + b3

    return Z3
```

进行测试：因为没有占位符，所以与其定义一个张量来测试，不如直接使用X_train

```python
tf.compat.v1.reset_default_graph()  # 用于清除默认图形堆栈并重置全局默认图形。

parameters = initialize_parameters()
Z3 = forward_propagation(X_train, parameters)
print("Z3 = " + str(Z3))
```

<br>

我在运行时有报错

![image-20200709143051271](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200709143051271.png)

这里需要注意 `tf.matmul(a,b)`函数中的a与b要求数据类型一样。查看模型参数parameters的数据类型，是float32

![image-20200709143201311](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200709143201311.png)

再查看一下X_train

![image-20200709143414661](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200709143414661.png)

可以看到一个是float32，一个是float64，故将数据集转为float32

```python
X_train = tf.cast(X_train,dtype=tf.float32)
Y_train = tf.cast(Y_train,dtype=tf.float32)
X_test = tf.cast(X_test,dtype=tf.float32)
Y_test = tf.cast(Y_test,dtype=tf.float32)
```

![image-20200709143839262](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200709143839262.png)

再进行测试，结果如下

![image-20200709143929024](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200709143929024.png)

对比原文的结果 `Z3 = Tensor("Add_2:0", shape=(6, ?), dtype=float32)`，可以看到shape、float32一致

**注：**原文的shape为`(6,?)`，是因为占位符中的维度为`(n_x,None)`与`(n_y,None)`,而生成占位符时传入的参数仅有`n_x`与`n_y`，即并没有指定样本数，所以Z3的样本数为`?`，实际上Z3的样本数等于输入特征X的样本数，X_train维度为`(12288,1080)`，所以Z3的shape应该为`(6,1080)`。6为输出层隐藏单元数量，即W3、b3的shape[0]

### 计算代价

### 

```python
def compute_cost(Z3, Y):
    """
    计算成本

    参数：
        Z3 - 前向传播的结果
        Y - 标签，一个占位符，和Z3的维度相同

    返回：
        cost - 成本值

    """

    logits = tf.transpose(Z3)  # 转置
    labels = tf.transpose(Y)  # 转置
	
    # reduce_mean()计算平均值
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
        logits=logits, labels=labels))

    return cost
```

测试一下：

```python
tf.compat.v1.reset_default_graph()

parameters = initialize_parameters()
Z3 = forward_propagation(X_train,parameters)
cost = compute_cost(Z3,Y_train)
print("cost = " + str(cost))
```

结果：

![image-20200709145329359](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200709145329359.png)

原文中并没有给出代价的值。

### 反向传播

先看下tensorflow2中的Adam梯度下降函数`tf.keras.optimizers.Adam`：

![image-20200709184254160](C:\Users\84452\AppData\Roaming\Typora\typora-user-images\image-20200709184254160.png)

tensorflow使用一行代码即可实现反向传播。



## 总结

### 创建tensor

```python
Var = tf.constant(value,dtype=None,shape=None,name="Name",verify_shape=Flase) # 常量
Var = tf.Variable(value,dtype=None,shape=None,name="Name",verify_shape=Flase) # 变量
```

### 计算

```python
tf.add(a,b)   # a + b
tf.multiply(a,b)   # a * b
```

### ndarray与tensor的转换

```python
Var.numpy()
tf.convert_to_tensor(Var)
```

### tf2没有占位符特性

### 计算sigmoid

```python
z = float(z)
result = tf.sigmoid(z)
```

### 计算代价（成本）

```python
tf.nn.sigmoid_cross_entropy_with_logits(
    labels=None, # Y 
    logits=None, # Y帽
    name=None
)
```

### 初始化模型参数

使用keras接口

```python
seed = 1

# W1
initializer = tf.keras.initializers.GlorotUniform(seed)
W1 = tf.Variable(initializer(shape=shape_W, name='W1'))
# =>     W1 = tf.Variable(tf.keras.initializers.GlorotUniform(seed)(shape=shape_W), name='W1')
                 
# b1
b1 = tf.Variable(tf.keras.initializers.Zeros()(shape=shape_b), name='b1')
```

### 正向传播

改用tensorflow进行计算

```python
Z1 = tf.add(tf.matmul(W1, X), b1)        # Z1 = np.dot(W1, X) + b1
# Z1 = tf.matmul(W1,X) + b1             #也可以这样写
A1 = tf.nn.relu(Z1)                    # A1 = relu(Z1)
Z2 = tf.add(tf.matmul(W2, A1), b2)     # Z2 = np.dot(W2, a1) + b2
A2 = tf.nn.relu(Z2)                    # A2 = relu(Z2)
Z3 = tf.add(tf.matmul(W3, A2), b3)     # Z3 = np.dot(W3,Z2) + b3
```

`matmul(a,b)`要求a与b的数据类型相同, `tf.nn.relu`为神经网络的relu激活函数

### 计算代价

```python
logits = tf.transpose(Z3)  # 转置
labels = tf.transpose(Y)  # 转置

# reduce_mean()计算平均值
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))

return cost
```

先将Z3（$\hat{Y}$）与标签Y转置，然后用`tf.nn.softmax_cross_entropy_with_logits`计算代价，使用`reduce_mean`取平均值