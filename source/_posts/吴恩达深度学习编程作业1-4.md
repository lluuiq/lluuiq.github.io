---
title: 吴恩达深度学习编程作业1-4
comments: true
toc: true
top: false
mathjax: true

categories:
- [神经网络,学习笔记]
tags: [神经网络]

date: 2020-06-14 22:33:21
---

吴恩达深度学习课程《神经网络和深度学习》第三周（深层神经网络）的编程作业。

<!-- more -->

## 说明

题目与参考作业的地址：[【中文】【吴恩达课后编程作业】Course 1 - 神经网络和深度学习 - 第四周作业(1&2)](https://blog.csdn.net/u013733326/article/details/79767169)

相关数据集与前提代码在该博文中下载。

## 查看下载的资料

下载后如图所示：

[![image-20200614025709724](https://gitee.com/lluuiq/blog_img/raw/master/img/20200621080116.png)](https://gitee.com/lluuiq/blog_img/raw/master/img/20200621080116.png)

打开datasets，可以发现是h5文件，与第二周的作业文件相同，是猫的图片数据

[![image-20200614025724028](https://gitee.com/lluuiq/blog_img/raw/master/img/20200621080119.png)](https://gitee.com/lluuiq/blog_img/raw/master/img/20200621080119.png)

文件因为内容过多，就不截图了。

- 打开dnn_utils.py，可以看到提供了神经网络的ReLU激活函数与sigmoid函数（正向与反向）
- 打开lr_utils.py，可以看到是第二周作业的读取训练集、测试集数据的代码
- 打开testCases.py， 该文件提供了各个函数的测试用参数（测试用的，该文章中没用到）

根据以上可知，这次作业是使用深层神经网络来完成第二周的作业：识别猫的图片。



## 读取数据

与第二周的作业步骤相同。

```python
# 加载 lr_utils的load_dataset函数
from lr_utils import load_dataset
# 定义变量来获取load_dataset函数的返回值
train_set_x , train_set_y , test_set_x , test_set_y , classes = load_dataset()

print("训练集特征的维度：",train_set_x.shape,"训练集特征的类型",type(train_set_x))
print("训练集标签的维度：",train_set_y.shape,"训练集标签的类型",type(train_set_y))
print("测试集特征的维度：",test_set_x.shape,"测试集特征的类型",type(test_set_x))
print("测试集标签的维度：",test_set_y.shape,"测试集标签的类型",type(test_set_y))
print("classes的维度：",classes.shape,"classes的类型",type(classes))
```



## 数据处理

与第二周的作业步骤相同。

```python
train_set_x_flatten = train_set_x.reshape(train_set_x.shape[0],-1).T
test_set_x_flatten = test_set_x.reshape(test_set_x.shape[0],-1).T

train_set_x_flatten_normalization = train_set_x_flatten/255
test_set_x_flatten_normalization = test_set_x_flatten/255

print("处理后的训练集维度：",train_set_x_flatten_normalization.shape)
print("处理后的测试集维度：",test_set_x_flatten_normalization.shape)
```

处理后图片的存储矩阵就变为（64*64*3，样本个数）



## 构建神经网络

### 导入numpy

```python
import numpy as np
```



### 初始化模型参数函数

```python
def initialization_parameters(layers):
    '''
        参数：
            layers 隐藏层列表，元素为对应层的隐藏单元个数
        返回：
            params 存储W[i] 与b[i] 的字典
    '''
    
    params = {}

    # 遍历隐藏层
    for i in range(1, len(layers)):
        # 为每一层初始化模型参数
        params['W'+str(i)] = np.random.randn(layers[i],
                                             layers[i-1]) / np.sqrt(layers[i-1])
        params['b'+str(i)] = np.zeros((layers[i], 1))

        assert(params['W'+str(i)].shape == (layers[i], layers[i-1]))
        assert(params['b'+str(i)].shape == (layers[i], 1))

    # 返回值字典 内容为[W1,b1,W2,b2,W3,b3 ..... WL,bL]
    return params
```



这里传入一个变量layers，用来保存隐藏层的信息，元素为各层的隐藏单元数目，不包括输入层。

例如：创建一个5层的神经网络，对应隐层分别为 5、5、4、4、1。

则 layers=[5,5,4,4,1] ，图如下

[![image-20200615000700220](https://gitee.com/lluuiq/blog_img/raw/master/img/20200621080124.png)](https://gitee.com/lluuiq/blog_img/raw/master/img/20200621080124.png)



这样，`len(layers)`为神经网络的层数。`leyers[i]`为第`i`层的隐藏单元数目。

用params来存储各隐藏层的模型参数。

因为与无隐层、单隐层不同，这是一个有L个隐层的神经网络，所以使用遍历的方式来逐层初始化。

**注：** 这里生成随机数后不再 *0.01，该为 `/ np.sqrt(layers[i-1])`，否则深层的话代价函数更新不下去。



### 定义正向传播函数

```python
import dnn_utils as dnn

def forward(X, params, activations):
    ''' 
        参数：
            X 输入特征，即A[i-1]
            params 模型参数字典
            activation 激活函数列表
        返回：
            cache 存储A[i] 与 Z[i] 的字典
    '''

    # 定义激活函数字典，根据输入的activation来选择对应的激活函数
    activation_function = {
        "relu": dnn.relu,
        "sigmoid": dnn.sigmoid
    }

    # 获取层数
    L = len(activations)

    # cache['A0']初始化为输入特征X
    cache = {
        'A0': X
    }
    
    # 遍历隐藏层进行正向传播
    for i in range(1, L+1):
        
        # 获取当前层的W与Z
        W = params['W'+str(i)]
        b = params['b'+str(i)]
        
        #获取当前层的上一层的A，即当前层的输入值
        A_pre = cache['A'+str(i-1)]
		
        # 获取当前层选择的激活函数
        activation = activations[i-1]
		
        # 计算Z
        Z = np.dot(W, A_pre)+b
        # 使用dnn_utils中的激活函数来计算当前层的A
        cache['A'+str(i)], cache['Z'+str(i)
                                 ] = activation_function[activation](Z)
        
        # 断言
        assert cache['Z'+str(i)].shape == Z.shape, 'error：维度错误'
        assert cache['A'+str(i)].shape == cache['Z'+str(i)
                                                ].shape, 'error: A与Z的维度不相等'

    return cache
```



- 导入dnn_utils来使用其中的激活函数。

- 传入的参数有一个activations，是一个列表，其元素为从第一个隐藏层开始的各层使用的激活函数，因为本次作业的dnn_utils文件只有ReLU与sigmoid激活函数，所以值为relu或sigmoid。

- activation_function字典的value值为dnn_utils文件中的函数，这样后面执行`activation = activations[i-1]`后，只需使用activation_function[activation](Z)来将Z传入激活函数即可 。

  关于activation_function[activation]，假如是使用ReLU函数，那么activation=“relu”，结果为activation_function[“relu”]，即dnn.relu，后面加上(Z)，即组成dnn.relu(Z)，调用函数。

- 遍历隐藏层，根据激活函数列表来实现对应的激活函数。



### 定义代价函数

```python
def compute_cost(AL,Y):
    """
    计算交叉熵代价函数
    J=1/m * ∑损失函数
    损失函数= -[Y*log(A2)+(1-Y)*log(1-A2)]
    这里 ∑损失函数 可以用向量来表示。
    
    参数：
         AL - 正向传播最后的输出结果，即损失函数公式中的y帽
         Y - 标签
         params - 初始化模型参数函数的字典返回值
    
    返回：
         代价 - 交叉熵成本给出方程（13）
    """
    # 获取样本数
    m = AL.shape[1]
    
    #计算代价
    logprobs  = np.multiply(np.log(AL), Y) + np.multiply((1 - Y), np.log(1 - AL))
    cost = - np.sum(logprobs) / m
    cost = float(np.squeeze(cost))
    
    return cost
```



用于计算每一次迭代的代价。



### 定义反向传播函数

```python
def model_backward(X, Y, params, cache, activations):
    '''
        参数： 
            Y 训练集标签
            params 模型参数字典
            cache A与Z的字典
            activations 激活函数列表
        返回：
            grads 存储dA[i] dW[i] db[i]
    '''

    # 获取样本数
    m = Y.shape[1]

    # 获取层数
    L = len(activations)

    # 获取输出层的A （正向传播的输出）
    AL = cache['A'+str(L)]
#     AL = np.random.randn(1, 2)

    # 激活函数字典
    activation_function = {
        "relu": dnn.relu_backward,
        "sigmoid": dnn.sigmoid_backward
    }

    cache['A0'] = X # 将cache['A0']定义为输入特征X
    
    # 初始化dAL
    grads = {
        'dA'+str(L): -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
    }

    # 反向传播从后往前计算，所以range(L,0,-1)，使用-1来实现区间(0,L]的反向遍历
    for i in range(L, 0, -1):

        # 根据传入的激活函数列表选择对应的激活函数
        activation = activation_function[activations[i-1]]

        # 获取当前层的W与Z
        W = params['W'+str(i)]
        Z = cache['Z'+str(i)]
        
        #获取当前层的上一层的A，即当前层的输入值
        A_pre = cache['A'+str(i-1)]

        # 计算导数dA与dZ
        dA = grads['dA'+str(i)]
        dZ = activation(dA, Z)
        
        # 计算dW与db 并将dW、db、dA存到grads中
        grads['dW'+str(i)] = np.dot(dZ, A_pre.T)/m
        grads['db'+str(i)] = np.sum(dZ, axis=1, keepdims=True)/m
        grads['dA'+str(i-1)] = np.dot(W.T, dZ)
    return grads
```



这里同样使用了activations，与正向不同的是这里是根据激活函数列表来选择对应的激活函数的求导函数。

其余关于activations的部分与正向传播相同。



### 定义更新参数函数

```python
def update_params(params,grads,L,lr):
    '''
        参数： 
            parameters 模型参数字典
            grads  反向传播得到的导数字典
            lr 学习率
            L 神经网络的层数
        返回值：
            params 更新后的模型参数字典
    '''
    for i in range(1,L+1):
        params['W'+str(i)] = params['W'+str(i)] - lr * grads['dW'+str(i)]
        params['b'+str(i)] = params['b'+str(i)] - lr * grads['db'+str(i)]
    return params
```



### 打包训练模型

```python
def nn_model(X, Y, layers, activations, lr, num_iterations, print_cost=False):
    '''
        深层神经网络模型
        参数：
            X 训练集输入特征
            Y 训练集标签
            layers 神经网络隐藏层列表(不包含输入层)，元素为各隐藏层的隐藏单元数量
            activations 激活函数列表，元素为对应层的激活函数（作业里只有relu和sigmoid）
            lr 学习率
            num_iterations 迭代次数
            seed 随机数种子
            print_cost 是否输出代价，默认为否
        返回：
            parameters 各层更新后的模型参数 字典

    '''

    nx = X.shape[0]  # 获取输入特征的维数
    ny = Y.shape[0]  # 获取标签的维数
    assert layers[-1] == ny, 'error：请确定输出层的单元数与标签的维数相等'

    L = len(layers)  # 获取神经网络层数（隐藏层的层数）
    assert len(activations) == L, 'error: 请确定激活函数的个数与隐藏层的层数相等'

    layers.insert(0, nx) # 输入层的节点数 n0

    print("初始化模型参数")
    params = initialization_parameters(layers)

    # 梯度下降
    for i in range(1,num_iterations+1):
        # 正向传播
        cache = forward(X, params, activations)
        
        # 计算代价
        AL = cache['A'+str(L)]
        cost = compute_cost(AL, Y)
        
        # 输出代价
        if print_cost and i%100 ==0:
            print("第",i,"次迭代，当前代价为：",cost)
        
        # 反向传播
        grads = model_backward(X,Y, params, cache, activations)
        
        # 更新参数
        params = update_params(params,grads,L,lr)
        
    return params
```



调用模型函数，传入训练集输入特征X、输出特征Y 与神经网络结构layers、激活函数列表、学习率、迭代次数即可获得训练完成后的模型参数W与b。



## 预测

### 定义预测函数

```python
def predict(X,params,activations):
    '''
        预测函数，调用正向传播函数来得到Y帽，即预测值
        
        参数：
            X 测试集输入特征
            params 由神经网络模型得到的模型参数
            activations 激活函数列表
        返回：
            Y 预测的标签
    '''
    
    # 执行正向传播，params存储的为更新完成后的W与b
    cache = forward(X,params,activations)
    
    # 获取层数
    L = len(activations)
    
    # 获取正向传播后最后的预测值
    A = cache['A'+str(L)]
    
    # 初始化标签Y
    Y = np.zeros((1, X.shape[1]))
    
    # 遍历所有预测值，若预测大于0.5则标签为1，否则为0
    for i in range(A.shape[1]):
        Y[0][i] = 1 if A[0][i] > 0.5 else 0
    return Y
```

接下来只需训练模型得到params，将测试集输入特征与params、激活函数列表传入预测函数即可得到预测的标签。



### 训练模型

```python
# 设置随机数种子
np.random.seed(1)
# 定义神经网络结构
layers = [20,7,5,1]
# 定义各层使用的激活函数
activations = ['relu','relu','relu','sigmoid']
# 定义学习率
lr = 0.0075
# 定义迭代次数
num_iterations = 2500
# 执行模型函数，获得更新后的模型参数字典params
params = nn_model(train_set_x_flatten_normalization, train_set_y, layers, activations, lr, num_iterations, print_cost=True)
```

执行后结果如图：

[![image-20200621075429192](https://gitee.com/lluuiq/blog_img/raw/master/img/20200621075431.png)](https://gitee.com/lluuiq/blog_img/raw/master/img/20200621075431.png)

### 进行预测

```python
# 预测训练集
train_predict_y = predict(train_set_x_flatten_normalization, params, activations)
# 预测测试集
test_predict_y = predict(test_set_x_flatten_normalization, params, activations)
```



### 查看准确率

```python
print("训练集准确率：", format(100-np.mean(np.abs(train_predict_y-train_set_y))*100), "%")
print("测试集准确率：", format(100-np.mean(np.abs(test_predict_y-test_set_y))*100), "%")
```

[![image-20200621075516324](https://gitee.com/lluuiq/blog_img/raw/master/img/20200621075517.png)](https://gitee.com/lluuiq/blog_img/raw/master/img/20200621075517.png)



可以看出训练集的准确率远高于测试集，明显有过拟合现象。