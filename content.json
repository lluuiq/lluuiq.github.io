{"meta":{"title":"lluuiq's blog","subtitle":"","description":"","author":"lluuiq","url":"https://lluuiq.com","root":"/"},"pages":[{"title":"404 Not Found","date":"2020-06-24T20:33:37.071Z","updated":"2020-06-24T20:33:37.071Z","comments":true,"path":"404.html","permalink":"https://lluuiq.com/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"关于","date":"2020-06-24T20:33:37.235Z","updated":"2020-06-24T20:33:37.235Z","comments":true,"path":"about/index.html","permalink":"https://lluuiq.com/about/index.html","excerpt":"","text":"test"},{"title":"友人帐","date":"2020-08-14T15:02:50.856Z","updated":"2020-08-14T15:02:50.856Z","comments":true,"path":"friends/index.html","permalink":"https://lluuiq.com/friends/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2020-06-24T20:33:37.235Z","updated":"2020-06-24T20:33:37.235Z","comments":true,"path":"categories/index.html","permalink":"https://lluuiq.com/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2020-06-24T20:33:37.235Z","updated":"2020-06-24T20:33:37.235Z","comments":true,"path":"tags/index.html","permalink":"https://lluuiq.com/tags/index.html","excerpt":"","text":""},{"title":"","date":"2020-06-24T20:33:37.235Z","updated":"2020-06-24T20:33:37.235Z","comments":true,"path":"mylist/index.html","permalink":"https://lluuiq.com/mylist/index.html","excerpt":"","text":""}],"posts":[{"title":"keras笔记-基础","slug":"keras笔记-基础","date":"2020-07-14T13:24:25.000Z","updated":"2020-07-15T10:14:48.206Z","comments":true,"path":"post/202007142124/","link":"","permalink":"https://lluuiq.com/post/202007142124/","excerpt":"记录下入门keras的笔记，参考书籍 《Python深度学习》","text":"记录下入门keras的笔记，参考书籍 《Python深度学习》 电影评论二分类问题说明使用keras自带的数据集 imdb。 任务：根据评论建立模型预测一条评论是正面还是负面 数据集： 输入特征x为单词的索引组成的列表 假设 it is very good，对应索引为 it : 5, is : 3 very : 10, good: 36 那么该条数据的x为[5,3,10,36] 标签y为该条数据是正面还是负面，正面为1，负面为0 假设有3个样本，[[it,is,very,good],[i,love,it],[it is boring]] 那么y为[1,1,0] 导入库并读取数据1234from keras.datasets import imdbimport numpy as np(train_data, train_labels),(test_data, test_labels) = imdb.load_data(num_words=10000) 读取imdb的数据集，下载速度挺慢的，还容易出错，可以下载离线数据放在C:\\Users\\用户名\\.keras\\datasets中，然后再读取 num_words指定一条数据最多含有一万个单词。 1234567print('训练集特征的维度:', train_data.shape, \" &amp; 训练集特征的类型:\", type(train_data))print('训练集标签的维度:', train_labels.shape)print('测试集特征的维度:', test_data.shape)print('测试集标签的维度:', test_labels.shape)print(\"训练集特征第一条数据:\",train_data[0])print(\"训练集标签第一条元素:\",train_labels[0]) 查看数据集的维度以及详细的值。 可以看到一共有25000个样本（25000行），没有显示列的个数是因为每一条数据的单词数都不一样。 标签实际为一维数组，即列表。 1234for i in range(5): print(\"第\",i,\"条数据的单词数:\",len(train_data[i])) print(\"查看标签:\",train_labels) 数据集向量化要往神经网络中输入数据来建立模型，但神经网络的第一层是固定的输入维度，所以应该将输入特征转为统一的维度。 12345def vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) # 维度：(sequences的长度，10000) for i, sequence in enumerate(sequences): results[i, sequence] = 1. # 1. 将数据类型变为float32 return results 创建一个元素均为0的张量 results，维度为（sequences的长度，10000） 这样新的张量每一行还是对应着原来的数据，但列数统一扩充到了一万。 然后遍历原来的输入特征，enumerate()将列表转为 索引,值 的形式。 所以 i 就对应着行索引，sequence对应着该行的值，即一条输入特征数据。 results[i,sequence] = 1` ，指定了results的第 i 行，将sequence中元素作为列索引，将results中对应列索引的元素置为1。 一开始并不理解这个函数的作用，但自定义一个数据然后来测试一下就知道其作用了。 测试部分: 12345def vectorize_sequences(sequences, dimension=10): # dimension改为10 results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results 这里设置dimension=10，即单词的最大长度为10。 12a=[[1,2,5],[2,3,1,4,3]]vectorize_sequences(a) 创建一个二维数组，使用向量化函数，可以看到变成了(3,10)的维度（3为a的行数，10为定义的dimension） 并且a[0]=[1,2,5,6,7] ，所以输出的新张量列索引为1,2,5,6,7的元素都置为了1。 将dimension改回10000，然后向量化输入特征 12x_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data) 接着将标签的数据类型转为float32 12y_train = np.asarray(train_labels).astype('float32')y_test = np.asarray(test_labels).astype('float32') 查看数据信息 12345678print(\"向量化后的维度：\")print('训练集特征:', x_train.shape, \" &amp; 训练集特征的类型:\", type(x_train))print('训练集标签:', y_train.shape)print('测试集特征:', x_test.shape)print('测试集标签:', y_test.shape)print('第一行特征:',x_train[0])print('第一行标签:',y_train[0]) 建模使用keras构建网络的流程为： 创建模型对象 添加网络层 编译网络 训练模型 先创建一个模型对象 1234from keras import models# Sequential是最基础的神经网络，即一层一层的那种network = models.Sequential() 然后添加网络层 123456from keras import layers# 添加层，构建网络network.add(layers.Dense(16, activation='relu', input_shape=(10000,)))network.add(layers.Dense(16, activation='relu'))network.add(layers.Dense(1, activation='sigmoid')) 第一层指定input_shape，即输入特征的维度（不设置这个也可以） 最后一层为输出层，隐藏单元设置为1，激活函数使用sigmoid 添加好网络层后，编译该网络层 123456# 编译模型（建立模型）network.compile( optimizer&#x3D;&#39;rmsprop&#39;, loss&#x3D;&#39;binary_crossentropy&#39;, metrics&#x3D;[&#39;acc&#39;]) 定义优化器为RMSprop，损失函数为二元交叉熵。 在进行训练前，先分割一下训练集与验证集，先拿验证集来对网络进行一次模拟考试 1234567891011# 将train_X的前1W条给x_val，后1W条给partial_x_trainx_val = x_train[:10000]partial_x_train = x_train[10000:]y_val = y_train[:10000]partial_y_train = y_train[10000:]print(x_val.shape)print(partial_x_train.shape)print(y_val.shape)print(partial_y_train.shape) 最后训练模型，将分隔好的训练集、验证集数据传入，定义epochs=20，mini-batch=512。 12345678910epochs = 20batch_size = 512history = network.fit( partial_x_train, partial_y_train, epochs=epochs, # 训练周期 batch_size=batch_size, # mini-batch尺寸 validation_data=(x_val, y_val) # 验证集) 输出的信息包括 当前代数/训练周期、进度条、训练时间，以及当代训练集的损失、精度，当代验证集的损失、精度 这里用history接收fit()函数的返回值，其是一个字典，用来记录损失与经度的值 可以看到 key为acc时，值为之前训练时的精度列表。 绘制损失函数与经度函数12345678910111213141516171819202122232425262728293031import matplotlib.pyplot as pltitems = history.history # 为items赋值 值为history字典loss = items['loss']val_loss = items['val_loss']acc = items['acc']val_acc = items['val_acc']epochs_ls = range(1, epochs+1) # 将周期变为 [1,2,3,....,epochs+1]的range对象''' 查看损失图像 '''plt.plot(epochs_ls, loss, 'bo', label='Training loss')plt.plot(epochs_ls, val_loss, 'b', label='Validation loss')plt.title('Train and validation loss')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.show()'''查看精确度图像'''plt.plot(epochs_ls, acc, 'bo', label='Training accuracy')plt.plot(epochs_ls, val_acc, 'b', label='Validation accuracy')plt.title('Train and Validation Accuracy')plt.xlabel('Epochs')plt.ylabel('Accuracy')plt.legend()plt.show() 观察图像可以发现，随着训练周期的增加，训练集的精度在提高，损失函数在下降。但是验证集的精度几乎没什么变化，损失还在不断提高。这是明显的过拟合现象 ，这次编程实践并没有涉及到过拟合，故放在后面的相关实践中再使用。 重新训练一个模型从损失函数图像可以看出，在第三代时训练集与验证集的损失差距就比较大了，故重新建立一个网络，设置epochs=4来试一下。 12345678910111213141516171819202122model = models.Sequential()model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))model.compile( optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])epochs = 4batch_size = 512history2 = model.fit( partial_x_train, partial_y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val) ) 这次使用history2来接收返回的字典，并绘制新建模型的损失函数与精度函数 图像其实就是前4代的放大版。 评估模型12results = model.evaluate(x_test,y_test)print(\"results:\",results) 损失0.3，精度0.87(约为0.88) 预测1model.predict(x_test) 输出的结果为 预测每一行的数据label值为1的概率，一般0.5以上基本可以判断为正面，当然概率越大，模型就越确信是一条正面评论 总结 电影评论分类（二元分类） 新闻分类（多分类问题） 房价预测（回归问题预测连续值） 数据预处理 向量化 向量化与标签的one-hot编码 标准化 验证方式 分割训练集与验证集 分割训练集与验证集 K折交叉验证 输出层激活函数 sigmoid softmax 无 （线性输出） 损失函数 二元交叉熵 分类交叉熵 均方误差MSE 评估指标 精确度 精确度 平均绝对误差MAE","categories":[{"name":"神经网络","slug":"神经网络","permalink":"https://lluuiq.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"笔记","slug":"神经网络/笔记","permalink":"https://lluuiq.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"https://lluuiq.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"keras","slug":"keras","permalink":"https://lluuiq.com/tags/keras/"}]},{"title":"吴恩达深度学习编程作业2-3","slug":"吴恩达深度学习编程作业2-3","date":"2020-07-04T01:28:35.000Z","updated":"2020-07-09T10:58:45.824Z","comments":true,"path":"post/202007040928/","link":"","permalink":"https://lluuiq.com/post/202007040928/","excerpt":"吴恩达深度学习课程《改善深层神经网络：超参数调试、正则化以及优化》第三周（超参数调试、Batch正则化和程序框架）的编程作业。 使用tensorflow2.1完成，既是完成编程作业，同时也是入门学习tensorflow2。","text":"吴恩达深度学习课程《改善深层神经网络：超参数调试、正则化以及优化》第三周（超参数调试、Batch正则化和程序框架）的编程作业。 使用tensorflow2.1完成，既是完成编程作业，同时也是入门学习tensorflow2。 说明 题目与参考作业的地址：【中文】【吴恩达课后编程作业】Course 2 - 改善深层神经网络 - 第三周作业 - TensorFlow入门 相关数据集与前提代码在该博文中下载。 原文使用的是tensorflow1.x版本，本文使用的是tensorflow2.1 为对比原文，采取与原文一致的过程，而使用tensorflow2.1实现。 作业要求： 学习tensorflow的初始化变量、建立一个会话、训练算法、实现一个神经网络。 tensorflow的安装可以参考我的另一篇文章：TensorFlow2.1 安装以及开启GPU加速 导包123456789101112131415import numpy as npimport pandas as pdimport tensorflow as tffrom tensorflow.python.framework import opsimport matplotlib.pyplot as plt%matplotlib inline plt.rcParams['font.sans-serif']=['SimHei'] # 显示中文标签plt.rcParams['axes.unicode_minus']=False # 显示负号import h5pyimport tf_utilsimport timenp.random.seed(1) 入门引导one损失函数loss的公式：$L(\\hat{y},y) = (\\hat{y}^{(i)} - y^{(i)})^{2}$ 1234567# 定义常量张量 y_hat 与 yy_hat = tf.constant(36, name='y_hat') # y_hat = 36y = tf.constant(39, name='y') # y= 39# 定义变量张量lossloss = tf.Variable((y-y_hat)**2, name='loss') #loss = (39-36)的平方，即9print(loss) 运行结果： 可以看到loss的值为9（numpy=9）。 与 tf1.X 相比，tf2不用再建立会话了。 tensorflow参与运算的单位是张量，易于理解的话，一维数组就是一维张量，二维数组即矩阵就是二维张量，N*N的数组就是N维张量。 tf.constant 创建一个常量的张量，无法改变其值。 tf.Variable创建一个变量的张量。 参数有 value,dtype=None,shape=None,name,verify_shape=Flase。 其中只有value是必须的值。 two创建两个常量张量a与b，a=2，b=10 c 等于a*b = 20 1234a = tf.constant(2) b = tf.constant(10)c = tf.multiply(a,b)print(c) 运行结果： tf2里就可以直接输出。 tf.multiply(a,b)作用是计算 a*b ，并返回值。 three原文展示了tf1.x的占位符（placeholder）用法，这个特性在tf2里被取消了 。 线性函数计算 $Y=W*X+b$，其中W与X是随机矩阵，b为随机向量。 规定W维度为（4，3），X维度为（3，1），b维度为（4，1）。可计算出Y的维度为（4，1） 12345678910111213141516171819202122232425262728293031def linear_function(): \"\"\" 实现一个线性功能： 初始化W，类型为tensor的随机变量，维度为(4,3) 初始化X，类型为tensor的随机变量，维度为(3,1) 初始化b，类型为tensor的随机变量，维度为(4,1) 返回： # result - 运行了session后的结果，运行的是Y = WX + b Y - 执行Y = WX + b 的结果，若需要ndarray需要使用 .numpy()转换 \"\"\" np.random.seed(1) # 指定随机种子 X = np.random.randn(3, 1) W = np.random.randn(4, 3) b = np.random.randn(4, 1) Y = tf.add(tf.matmul(W, X), b) # tf.matmul是矩阵乘法 # Y = tf.matmul(W,X) + b #也可以以写成这样子 \"\"\" 原文中的创建会话可以不使用了 \"\"\"# #创建一个session并运行它# sess = tf.Session()# result = sess.run(Y)# #session使用完毕，关闭它# sess.close() return Y.numpy() 然后运行函数 12Y = linear_function()print(Y) 这里在函数返回值时 使用了 Var.numpy() 将tensor类型的Var（即tensorflow中的数据类型 – 张量）转为numpy的ndarray。 可以使用 tf.convert_to_tensor(Var) 再将Var转为tensor。 1tf.convert_to_tensor(Y) 当然这里与numpy()一样可以用变量接收返回值 Y = tf.convert_to_tensor(Y) 计算sigmoidtf2直接省去了tf1的创建会话，所以使用起来简单了很多（甚至不用定义函数来执行了，但这里为了与原文对比所以还是定义了函数） 12345678910111213141516171819202122232425262728293031def sigmoid(z): \"\"\" 实现使用sigmoid函数计算z 参数： z - 输入的值，标量或矢量 返回： result - 用sigmoid计算z的值 \"\"\"# #创建一个占位符x，名字叫“x”# x = tf.placeholder(tf.float32,name=\"x\")# #计算sigmoid(z)# sigmoid = tf.sigmoid(x)# #创建一个会话，使用方法二# with tf.Session() as sess:# result = sess.run(sigmoid,feed_dict=&#123;x:z&#125;) \"\"\" tf1的一堆花里胡哨的东西 都可以去掉了。 \"\"\" # tf2的sigmoid必须传入float，否则报错。 z = float(z) result = tf.sigmoid(z) return result.numpy() 这里要使用float类型作为参数，否则报错。 同上，返回值使用numpy()返回ndarry，使得与原文的返回类型一致。 测试一下： 12print(\"sigmoid(0) = \" + str(sigmoid(0)))print(\"sigmoid(12) = \" + str(sigmoid(12))) 与原文区别就是sigmoid(12)变成了 0.9999938 而不是 0.999994 ，精度少了一位，基本没啥区别。 计算代价（成本）tensorflow自带了计算成本的函数，直接用就是。 cross_entropy 交叉熵 12345tf.nn.sigmoid_cross_entropy_with_logits( labels=None, # Y logits=None, # Y帽 name=None) 使用独热编码（0，1编码）什么是独热编码？如原文所举的例子 一个标签是[1 2 3 0 2 1]，有6个元素（样本），m = 6。 而分类的类别个数有4个（0, 1, 2, 3），C=4。 其实就相当于把标签的每个元素都拓展成值为0或1的向量。对应原来的值，将向量中对应下标的元素置为1。 这样做的优势就在于将原本的向量扩充到了欧式空间，并且多元分类转为了二元分类问题， 扩充了特征。 123456789tf.one_hot( indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None) 定义一个函数，参数为标签与C，返回该标签的独热编码 123456789101112131415161718192021222324252627282930313233def one_hot_matrix(lables, C): \"\"\" 创建一个矩阵，其中第i行对应第i个类号，第j列对应第j个训练样本 所以如果第j个样本对应着第i个标签，那么entry (i,j)将会是1 参数： lables - 标签向量 C - 分类数 返回： one_hot - 独热矩阵 \"\"\" # 创建一个tf.constant，赋值为C，名字叫C C = tf.constant(C, name=\"C\") # 使用tf.one_hot，注意一下axis one_hot_matrix = tf.one_hot(indices=lables, depth=C, axis=0) ''' 原文TF1的内容 '''# #创建一个session# sess = tf.Session()# #运行session# one_hot = sess.run(one_hot_matrix)# #关闭session# sess.close() return one_hot_matrix.numpy() 测试一下： 123labels = np.array([1, 2, 3, 0, 2, 1])one_hot_matrix = one_hot_matrix(labels, C=4)print(one_hot_matrix) 运行结果： 其实tf简化后没必要再定义函数了 ？ 初始化为0和1学习如何用0或者1初始化一个向量，即tf.zeros()与tf.ones() 12345shape = 3one = tf.ones(shape).numpy()zero = tf.zeros(shape).numpy()print(\"one: \",one)print(\"zero: \",zero) 使用TensorFlow构建你的第一个神经网络说明：原文提到实现模型（tensorflow1.X）需要两个步骤： 创建计算图 运行计算图 那么来看下tensorflow2里如何实现模型 任务目标：根据图片上的手势来识别数字。 训练集： 从0到5的数字的1080张图片，像素64*64，每个数字有180张图片。 测试集： 从0到5的数字的120张图片，像素64*64，每个数字有5张图片。 加载数据集使用 tf_utils文件里的函数读取数据 1X_train_orig , Y_train_orig , X_test_orig , Y_test_orig , classes = tf_utils.load_dataset() 查看一下数据的维度 1234print(X_train_orig.shape)print(Y_train_orig.shape)print(X_test_orig.shape)print(Y_test_orig.shape) 接下来要做的为： 事情和之前的作业一样，对输入特征 X 进行归一化以及转为 ( 3*64*64，样本数(m) ) 的矩阵。 将标签Y转为独热矩阵。 这部分没什么变化，直接用原文的代码即可。 123456789101112131415161718# 维度转为 (3*64*64,m)X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0],-1).T #每一列就是一个样本X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0],-1).T# 归一化数据X_train = X_train_flatten / 255X_test = X_test_flatten / 255# 转为独热矩阵 （调用了tf_utils的代码）Y_train = tf_utils.convert_to_one_hot(Y_train_orig,6)Y_test = tf_utils.convert_to_one_hot(Y_test_orig,6)print(\"训练集样本数 = \" + str(X_train.shape[1]))print(\"测试集样本数 = \" + str(X_test.shape[1]))print(\"X_train.shape: \" + str(X_train.shape))print(\"Y_train.shape: \" + str(Y_train.shape))print(\"X_test.shape: \" + str(X_test.shape))print(\"Y_test.shape: \" + str(Y_test.shape)) 创建placeholderstf2没有占位符，跳过 初始化参数tf2里去掉了.contrib.layers，而且get_variable需要使用tf.compat.v1.get_variable。至于区别因为现在刚入门，暂时不去细想了。 这里使用tf2的keras接口调用来实现。详细API参考官方文档：Module: tf.keras.initializers 原文中使用的是xavier，并且使用默认值 uniform=False，故这里应该使用 tf.keras.initializers.GlorotUniform 可以看到，参数只有一个seed，定义随机数种子。 再看一下用法 先初始化一个张量看看情况 12345shape = (25,12288)seed = 1initializer = tf.keras.initializers.GlorotUniform(seed)W1 = tf.Variable(initializer(shape=shape),name='W1')print(W1) 再试试zero初始化参数b，这里tf.zeros_initializer()其实在tf2里也可以用，但是既然用keras了，就用keras的接口吧 12# tf.Variable(tf.zeros_initializer()(shape=(25, 1)), name='b1')tf.Variable(tf.keras.initializers.Zeros()(shape=(25, 1)), name='b1') 可以看到，W1张量的name为W1，shape=(25, 12288)，dtype=float32，与原文一致，至于具体的值就不一定一致了。 这里b我没有用变量接收返回值，反正也只是测试。可以看到信息与原文也一致。 初次尝试成功了，接下来就来实现原文中的函数，原文是定义一个函数，没有传入参数，返回3层模型参数的tensor字典。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def initialize_parameters(): \"\"\" 初始化神经网络的参数，参数的维度如下： W1 : [25, 12288] b1 : [25, 1] W2 : [12, 25] b2 : [12, 1] W3 : [6, 12] b3 : [6, 1] 返回： parameters - 包含了W和b的字典 \"\"\"# tf.set_random_seed(1) #指定随机种子# W1 = tf.get_variable(\"W1\",[25,12288],initializer=tf.contrib.layers.xavier_initializer(seed=1))# b1 = tf.get_variable(\"b1\",[25,1],initializer=tf.zeros_initializer())# W2 = tf.get_variable(\"W2\", [12, 25], initializer = tf.contrib.layers.xavier_initializer(seed=1))# b2 = tf.get_variable(\"b2\", [12, 1], initializer = tf.zeros_initializer())# W3 = tf.get_variable(\"W3\", [6, 12], initializer = tf.contrib.layers.xavier_initializer(seed=1))# b3 = tf.get_variable(\"b3\", [6, 1], initializer = tf.zeros_initializer()) # 定义随机种子 seed = 1 # 初始化模型参数，这里省去了initializer赋值操作。 W1 = tf.Variable(tf.keras.initializers.GlorotUniform(seed) (shape=(25, 12288)), name='W1') b1 = tf.Variable(tf.keras.initializers.Zeros()(shape=(25, 1)), name='b1') W2 = tf.Variable(tf.keras.initializers.GlorotUniform(seed) (shape=(12, 25)), name='W2') b2 = tf.Variable(tf.keras.initializers.Zeros()(shape=(12, 1)), name='b2') W3 = tf.Variable(tf.keras.initializers.GlorotUniform(seed) (shape=(6, 12)), name='W3') b3 = tf.Variable(tf.keras.initializers.Zeros()(shape=(6, 1)), name=' b3') parameters = &#123; \"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3 &#125; return parameters 测试： tf.reset_default_graph()改为tf.compat.v1.reset_default_graph()，使其使用1.X 版本的reset_default_graph函数 1234567tf.compat.v1.reset_default_graph() # 用于清除默认图形堆栈并重置全局默认图形。parameters = initialize_parameters()print(\"W1 = \" + str(parameters[\"W1\"]))print(\"b1 = \" + str(parameters[\"b1\"]))print(\"W2 = \" + str(parameters[\"W2\"]))print(\"b2 = \" + str(parameters[\"b2\"])) 运行结果： 因为版本的区别，tensorflow2输出tensor会有个numpy信息直接查看值。 对比tensor信息可以看到与原文一致。 正向传播将原本的计算代码改为使用tensorflow实现 12345678910111213141516171819202122232425262728def forward_propagation(X, parameters): \"\"\" 实现一个模型的前向传播，模型结构为LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SOFTMAX 参数： X - 输入数据的占位符，维度为（输入节点数量，样本数量） parameters - 包含了W和b的参数的字典 返回： Z3 - 最后一个LINEAR节点的输出 \"\"\" W1 = parameters['W1'] b1 = parameters['b1'] W2 = parameters['W2'] b2 = parameters['b2'] W3 = parameters['W3'] b3 = parameters['b3'] Z1 = tf.add(tf.matmul(W1, X), b1) # Z1 = np.dot(W1, X) + b1 # Z1 = tf.matmul(W1,X) + b1 #也可以这样写 A1 = tf.nn.relu(Z1) # A1 = relu(Z1) Z2 = tf.add(tf.matmul(W2, A1), b2) # Z2 = np.dot(W2, a1) + b2 A2 = tf.nn.relu(Z2) # A2 = relu(Z2) Z3 = tf.add(tf.matmul(W3, A2), b3) # Z3 = np.dot(W3,Z2) + b3 return Z3 进行测试：因为没有占位符，所以与其定义一个张量来测试，不如直接使用X_train 12345tf.compat.v1.reset_default_graph() # 用于清除默认图形堆栈并重置全局默认图形。parameters = initialize_parameters()Z3 = forward_propagation(X_train, parameters)print(\"Z3 = \" + str(Z3)) 我在运行时有报错 这里需要注意 tf.matmul(a,b)函数中的a与b要求数据类型一样。查看模型参数parameters的数据类型，是float32 再查看一下X_train 可以看到一个是float32，一个是float64，故将数据集转为float32 1234X_train = tf.cast(X_train,dtype=tf.float32)Y_train = tf.cast(Y_train,dtype=tf.float32)X_test = tf.cast(X_test,dtype=tf.float32)Y_test = tf.cast(Y_test,dtype=tf.float32) 再进行测试，结果如下 对比原文的结果 Z3 = Tensor(&quot;Add_2:0&quot;, shape=(6, ?), dtype=float32)，可以看到shape、float32一致 注：原文的shape为(6,?)，是因为占位符中的维度为(n_x,None)与(n_y,None),而生成占位符时传入的参数仅有n_x与n_y，即并没有指定样本数，所以Z3的样本数为?，实际上Z3的样本数等于输入特征X的样本数，X_train维度为(12288,1080)，所以Z3的shape应该为(6,1080)。6为输出层隐藏单元数量，即W3、b3的shape[0] 计算代价123456789101112131415161718192021def compute_cost(Z3, Y): \"\"\" 计算成本 参数： Z3 - 前向传播的结果 Y - 标签，一个占位符，和Z3的维度相同 返回： cost - 成本值 \"\"\" logits = tf.transpose(Z3) # 转置 labels = tf.transpose(Y) # 转置 # reduce_mean()计算平均值 cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( logits=logits, labels=labels)) return cost 测试一下： 123456tf.compat.v1.reset_default_graph()parameters = initialize_parameters()Z3 = forward_propagation(X_train,parameters)cost = compute_cost(Z3,Y_train)print(\"cost = \" + str(cost)) 结果： 原文中并没有给出代价的值。 反向传播先看下tensorflow2中的Adam梯度下降函数tf.keras.optimizers.Adam： tensorflow使用一行代码即可实现反向传播。 总结创建tensor12Var = tf.constant(value,dtype=None,shape=None,name=\"Name\",verify_shape=Flase) # 常量Var = tf.Variable(value,dtype=None,shape=None,name=\"Name\",verify_shape=Flase) # 变量 计算12tf.add(a,b) # a + btf.multiply(a,b) # a * b ndarray与tensor的转换12Var.numpy()tf.convert_to_tensor(Var) tf2没有占位符特性计算sigmoid12z = float(z)result = tf.sigmoid(z) 计算代价（成本）12345tf.nn.sigmoid_cross_entropy_with_logits( labels=None, # Y logits=None, # Y帽 name=None) 初始化模型参数使用keras接口 123456789seed = 1# W1initializer = tf.keras.initializers.GlorotUniform(seed)W1 = tf.Variable(initializer(shape=shape_W, name='W1'))# =&gt; W1 = tf.Variable(tf.keras.initializers.GlorotUniform(seed)(shape=shape_W), name='W1') # b1b1 = tf.Variable(tf.keras.initializers.Zeros()(shape=shape_b), name='b1') 正向传播改用tensorflow进行计算 123456Z1 = tf.add(tf.matmul(W1, X), b1) # Z1 = np.dot(W1, X) + b1# Z1 = tf.matmul(W1,X) + b1 #也可以这样写A1 = tf.nn.relu(Z1) # A1 = relu(Z1)Z2 = tf.add(tf.matmul(W2, A1), b2) # Z2 = np.dot(W2, a1) + b2A2 = tf.nn.relu(Z2) # A2 = relu(Z2)Z3 = tf.add(tf.matmul(W3, A2), b3) # Z3 = np.dot(W3,Z2) + b3 matmul(a,b)要求a与b的数据类型相同, tf.nn.relu为神经网络的relu激活函数 计算代价1234567logits = tf.transpose(Z3) # 转置labels = tf.transpose(Y) # 转置# reduce_mean()计算平均值cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))return cost 先将Z3（$\\hat{Y}$）与标签Y转置，然后用tf.nn.softmax_cross_entropy_with_logits计算代价，使用reduce_mean取平均值","categories":[{"name":"神经网络","slug":"神经网络","permalink":"https://lluuiq.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"学习笔记","slug":"神经网络/学习笔记","permalink":"https://lluuiq.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"https://lluuiq.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]},{"title":"jupyter notebook配置笔记","slug":"jupyter notebook配置笔记","date":"2020-07-04T01:10:06.000Z","updated":"2020-07-04T03:23:27.476Z","comments":true,"path":"post/202007040910/","link":"","permalink":"https://lluuiq.com/post/202007040910/","excerpt":"记录下jupyter notebook的配置。","text":"记录下jupyter notebook的配置。 修改快捷键在Help中可以查看快捷键（上）与修改快捷键（下） 默认的就挺好用，但后面有安装一个autopep8插件，用来格式化代码，个人习惯的快捷键Ctrl-Shift-F与默认的查看命令按键冲突 故把Ctrl-Shift-F从中删除，用于后面的格式化代码。 插件拓展库12pip install jupyter_contrib_nbextensionspip install jupyter_nbextensions_configurator 12jupyter nbextensions_configurator install --userjupyter nbextensions_configurator enable --user 安装完成后，即可在jupyter的页面看到 Nbextensions 选项卡（或许需要重新打开jupyter） 使用filter筛选插件时全小写，大写识别不出来 Snippets Menu自定义代码模块 可以在对应的库中选择Setup（基本导入），或者根据要求生成代码。 My favorites 为自定义的代码块，在 Nbextensions 中选择Snippets Menu来设置 json内容如下 12345678910111213141516171819202122&#123; \"name\" : \"My favorites(自定义名称)\", \"sub-menu\" : [ &#123; \"name\" : \"代码块名称\", \"snippet\" : [\"import numpy as np\", \"import pandas as pd\", \"import tensorflow as tf\", \"import matplotlib.pyplot as plt\", \"%matplotlib inline\", \" \", \"plt.rcParams['font.sans-serif']=['SimHei'] # 显示中文标签\", \"plt.rcParams['axes.unicode_minus']=False # 显示负号\"] &#125;, &#123; \"name\" : \"第二个代码块\", \"snippet\" : [ 其余内容同上 ] &#125; ]&#125; json格式，注意逗号。 ExecuteTime可以在cell执行完毕后显示运行使用的时间与执行的时间 Notify可以在需要长时间运行代码时使用，运行完毕后提醒自己。 Table of Contentsmarkdown文本开启目录 Variable Inspector显示所有变量信息 Autopep8格式化代码工具，点击即可格式化cell内的代码。 可以在设置界面更改快捷键。 我因为搭建了 tensorflow虚拟环境，所以报了以下错，需要安装autopep8包，安装完成后即可使用。 切换到当前虚拟环境然后执行 1conda install autopep8 或者使用 Anaconda Navigator来安装 Collapsible Headings按标题可以折叠markdown以及以下的代码，方便浏览代码。 Freeze可以冻结一个cell，使其不能运行与删除等。 免得误操作。 Gist-it可以将ipython代码添加到github的gist仓库，需要在红框位置添加密钥 密钥的获得方式： 进入github.com/settings/tokens，点击Generate new token 生成一个密钥 Note类似名称，随意写，能区分是用来干什么的就行 然后在选项中勾选gist，最后点击 Generate token即可。 将生成的密钥复制，并粘贴进之前红框的位置。点击代码文件页面的github的图标。 若没有上传过则 Gist ID 是空的，会新建一个Gist仓库。 有个缺点就是修改不方便，因为上传的是ipython，所以只能打开ipython进行编辑再上传来更新，直接在gist上编辑是类似以下格式 修改样式jupyterthemes的主题配置简单，但是和一些拓展有些bug而且并不能完全符合自己的喜好。 故使用自定义CSS的方法来配置。同路径下有个custom.js可以配置js脚本 配置文件路径： C:\\Users\\用户名\\.jupyter\\custom\\custom.css 关于修改方法：建议打开jupyter页面，然后按F12，在这里测试样式，然后最后再写进代码里保存，刷新页面查看。 关于单元格字体的样式，修改对象： ) 关于代码区域的字体样式，修改对象： 定义了变量来存储值，修改只需要修改:root里面的值即可，详细的根据自己自定义的样式更改。 分享一下自己的设置，直接添加到最后面即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/******** DIY ********//* 定义变量 */:root &#123; /* 不透明度 */ --opacity: 0.9; /* 代码区域的字体样式 */ --code_font_size: 16; --code_font_family: consolas; /* 单元格字体样式 */ --cell_font_size: 14; --cell_font_family: consolas; /* 代码区行距 */ --line_height: 1.4; /* 背景图片url */ --background_image_url: url(https://gitee.com/lluuiq/blog_img/raw/master/img/932b3092a3fccecb49bb7b7880c3f24a9ed76d83.jpg@1320w_742h.png); /* 网页背景颜色 */ --bg_color: gray; /* 笔记本的宽度 （默认居中）*/ --width: 80%;&#125;/* cell单元格区域 */div.code_cell &#123; /* 字体样式 */ font-size: calc(var(--cell_font_size) * 1px); font-family: var(--code_font_family); /* 行距 */ line-height: calc(var(--line_height) * 1em);&#125;/* 代码区域 */.CodeMirror &#123; /* 字体样式 */ font-size: calc(var(--code_font_size) * 1px); font-family: var(--code_font_family);&#125;/* 注释 */.cm-comment &#123; /* 去掉斜体*/ font-style: normal !important; /* 样式与代码区一样 */ font-family: var(--code_font_family); font-size: var(--code_font_size);&#125;/* 目录 */#toc-wrapper &#123; /* 背景颜色 */ background-color: #ffffff; /* 不透明度 */ opacity: var(--opacity);&#125;/* 变量信息区域 */.varInspector-float-wrapper &#123; opacity: var(--opacity) !important;&#125;/* 笔记本区域 */div#notebook &#123; /* 不透明度 */ opacity: var(--opacity); /* 宽度缩小 */ width: var(--width);&#125;/* 笔记本区域的边框角弧度 */#notebook-container &#123; border-radius: 10px;&#125;/* 页面 */#site &#123; /* 背景图片 ，与背景颜色二选一 */ background: var(--background_image_url) no-repeat fixed; /* 背景颜色 */ /* background-color: var(--bg_color); */ background-size: cover;&#125; 效果图：","categories":[{"name":"笔记","slug":"笔记","permalink":"https://lluuiq.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Anaconda3","slug":"Anaconda3","permalink":"https://lluuiq.com/tags/Anaconda3/"},{"name":"jupyter","slug":"jupyter","permalink":"https://lluuiq.com/tags/jupyter/"}]},{"title":"TensorFlow2.1 安装以及开启GPU加速","slug":"TensorFlow2.1 安装以及开启GPU加速","date":"2020-07-02T21:00:19.000Z","updated":"2020-07-04T01:15:20.313Z","comments":true,"path":"post/202007030500/","link":"","permalink":"https://lluuiq.com/post/202007030500/","excerpt":"记录下TensorFlow2的安装以及开启GPU加速功能。 使用Anaconda3的conda进行安装。","text":"记录下TensorFlow2的安装以及开启GPU加速功能。 使用Anaconda3的conda进行安装。 查看GPU驱动版本与升级首先确定系统环境变量path里有下方路径，没有的话添加进去。 1C:\\Program Files\\NVIDIA Corporation\\NVSMI 然后在终端运行 nvidia-smi 这里可以看到我的NVIDIA-SMI版本为382.05 需要升级 打开 GeForce Experience更新驱动。 安装tensorflow-gpu并配置 使用conda创建一个虚拟环境并安装tensorflow-gpu 1conda create -n tf2 tensorflow-gpu 其中 -n 后面的 tf2是环境的名称，可自定义。 中间出现输入 Y/N 的时候可以检查一下要安装的包，确认一下tensorflow版本为2.1。 并不推荐换国内源安装，我换源安装结果显示的是tensorflow1.x版本。 安装完成后 启用新环境 1conda activate tf2 在新环境中安装一下ipython 1pip install ipython 安装 nb_code，使得 jupyter notebook可以切换kernel 首先关闭tf2环境，在tf2环境时输入 1conda deactivate 然后安装nb_code 1pip install nb_conda 接着修改一下配置文件，Anaconda3\\Lib\\site-packages\\nb_conda\\envmanager.py 找到如下所示的代码块 修改为 123return &#123; \"environments\": [root_env] + [get_info(env) for env in info['envs'] if env !=root_env['dir']]&#125; 然后就可以打开jupyter notebook切换kernel了。 jupyter notebook 测试tensorflow在新建notebook时选择环境 或者在已有的notebook中修改环境 查看版本： 12import tensorflow as tftf.__version__ 查看是否启用加速 12from tensorflow.python.client import device_libprint(device_lib.list_local_devices()) pycharm测试tensorflow新建一个project，选择已存在的环境，可以看到当前指定的为默认环境，而不是tf2环境，在后方点击 ... 图标选择。 我这里点开后自动选择了虚拟环境，如果没有参考该路径选择自己的。并勾选全工程生效。 等待更新 输入 12import tensorflow as tfprint(tf.__version__) 执行后即可看到运行成功。","categories":[{"name":"笔记","slug":"笔记","permalink":"https://lluuiq.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"TensorFlow2","slug":"TensorFlow2","permalink":"https://lluuiq.com/tags/TensorFlow2/"},{"name":"神经网络","slug":"神经网络","permalink":"https://lluuiq.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]},{"title":"Linux云服务器配置Anaconda3","slug":"Linux云服务器配置Anaconda3","date":"2020-07-02T20:55:02.000Z","updated":"2020-07-04T01:13:21.666Z","comments":true,"path":"post/202007030455/","link":"","permalink":"https://lluuiq.com/post/202007030455/","excerpt":"记录一下Linux云服务器上搭建Anaconda3并进行配置的过程，开启远程访问。","text":"记录一下Linux云服务器上搭建Anaconda3并进行配置的过程，开启远程访问。 说明Linux服务器：Centos7 Anaconda3版本：2020.02-Linux-x86_64 Anaconda3下载 下载Anaconda3并上传到服务器在[说明](# 说明)中的下载地址里，找到Linux版本。 （选择适合自己的安装包，我这里Linux是x86_64） 将下载的安装包上传到服务器（Xshell、FinalShell等软件） 安装Anaconda31bash Anaconda3-2020.02-Linux-x86_64.sh 安装包的名字对应自己下载的版本即可。 回车 疯狂回车 输入yes 按回车安装到当前目录 /root/anaconda3 输入yes 最后安装完成后，输入ipython即可启动ipython 输入exit()或使用ctrl+z退出 配置 /root/.bashrc 文件使用Anaconda3的python3.7Anaconda3附带的是python3.7，在安装好Anaconda3后，输入python还是linux自带的python2。故对版本进行配置。 添加如下代码（若安装路径不一样修改为自己的路径 ） 1export PATH=\"/root/anaconda3/bin:$PATH\" 是否显示(base)添加Anaconda3的python后会在指令行前面会出现(base) 表示启用了conda。如果不想显示这个(base)的话，在/root/.bashrc中添加 1conda deactivate 配置alias加入下方代码 1alias python2=\"/usr/bin/python2.7\" python2为自定义的别名，可以使用py2或者其余自己喜欢的别名替代 更新.bashrc每次修改.bashrc后都要输入source ~/.bashrc 进行更新才能生效。 远程访问jupyter打开ipython，然后按顺序输入以下代码 1from notebook.auth import passwd 1passwd() 然后输入一个登陆jupyter的密码，再输入一次确认。 输入后，会输出一个跟密钥一样的字符串 然后退出ipython。到anaconda3的安装目录下的etc/jupyter，执行 1jupyter notebook --generate-config 会输入信息显示配置文件的位置，编辑/root/.jupyter/jupyter_notebook_config.py 最后一行如果是root用户就加上，否则不用加。 12345678c.NotebookApp.ip = '*' # 设置使用此服务器的ip进行访问c.NotebookApp.password = u'sha1:XXXX' # 输入之前生成的keyc.NotebookApp.open_browser = False # 关闭启动jupyter时打开浏览器c.NotebookApp.port = XXXX # 设置使用的端口c.NotebookApp.enable_mathjax = True # 启用 MathJaxc.NotebookApp.notebook_dir = '你的目录' #设置共享目录c.NotebookApp.allow_remote_access = Truec.NotebookApp.allow_root = True # 允许在root用户下运行 效果图： 接下来到服务器商那里放行设置的端口，如果有安装宝塔，宝塔上也要放行。 然后输入jupyter notebook，开启jupyter 其余设备上打开浏览器，输入 IP:port 即可访问，如 192.168.0.123:8888 输入密码即可进入。 修改密码方法为输入 jupyter notebook password，然后像设置密码一样输入密码，最后会生成json配置文件。 然后打开生成的json文件，在.py配置文件中使用新生成的key json文件的优先级大于py，所以会覆盖。","categories":[{"name":"笔记","slug":"笔记","permalink":"https://lluuiq.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Anaconda3","slug":"Anaconda3","permalink":"https://lluuiq.com/tags/Anaconda3/"}]},{"title":"吴恩达深度学习编程作业2-2","slug":"吴恩达深度学习编程作业2-2","date":"2020-06-25T15:16:42.000Z","updated":"2020-06-26T22:41:34.686Z","comments":true,"path":"post/202006252316/","link":"","permalink":"https://lluuiq.com/post/202006252316/","excerpt":"吴恩达深度学习课程《改善深层神经网络：超参数调试、正则化以及优化》第二周（优化算法）的编程作业。","text":"吴恩达深度学习课程《改善深层神经网络：超参数调试、正则化以及优化》第二周（优化算法）的编程作业。 说明题目与参考作业的地址：【中文】【吴恩达课后编程作业】Course 2 - 改善深层神经网络 - 第二周作业 相关数据集与前提代码在该博文中下载。 本次作业的目的是测试在mini-batch梯度下降中：动量梯度下降、Adam梯度下降，与未优化的梯度下降进行对比 。 导入库函数123456789101112131415import numpy as npimport matplotlib.pyplot as plt%matplotlib inlineplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'import scipy.ioimport mathimport sklearnimport sklearn.datasetsimport opt_utils # 存储常用函数的库import testCase # 测试用库 未优化的梯度下降函数12345678910111213141516171819202122232425def update_parameters_with_gd(parameters,grads,learning_rate): \"\"\" 使用梯度下降更新参数 参数： parameters - 字典，包含了要更新的参数： parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl grads - 字典，包含了每一个梯度值用以更新参数 grads['dW' + str(l)] = dWl grads['db' + str(l)] = dbl learning_rate - 学习率 返回值： parameters - 字典，包含了更新后的参数 \"\"\" L = len(parameters) // 2 #神经网络的层数 #更新每个参数 for l in range(L): parameters[\"W\" + str(l +1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)] parameters[\"b\" + str(l +1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)] return parameters 之前学习的梯度下降算法，即 $w = w - α*dW$ 。 mini-batch梯度下降函数说明mini-batch梯度下降是先将所有样本 根据指定的size来划分为若干样本子集。然后对样本子集进行梯度下降。 步骤为： 打乱训练集 分割样本 处理剩余样本 打乱训练集首先将训练集打乱 ，即采取随机划分。 12345permutation = list(np.random.permutation(m)) # 返回一个长度为m的 元素不重复的 随机排序的数组，且里面的数是0到m-1# 通过切片操作按列排序shuffled_X = X[:,permutation]shuffled_Y = Y[:,permutation].reshape((1,m)) 假设X为(2,3)的矩阵，Y为(1,3)的矩阵，则m=3，permutation为维度(1,3)的不重复随机数组，可能是1,0,2，可能是2,0,1等等。 然后X[:,permutation] 会根据permutation对X的列进行重新排序，新的列索引等于permutation对应的元素。 分割样本12345678910111213141516# 用于存储分割后的样本子集mini_batches = []# 根据划分尺寸mini_batch_size来确定有多少个样本子集num_complete_minibatches = math.floor(m / mini_batch_size) # 遍历样本子集for k in range(0,num_complete_minibatches): # 根据mini_batch_size来分割数据集 mini_batch_X = shuffled_X[:,k * mini_batch_size:(k+1)*mini_batch_size] mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k+1)*mini_batch_size] # mini_batch 存储分割后的数据集 mini_batch = (mini_batch_X,mini_batch_Y) # 加到mini_batches列表中 mini_batches.append(mini_batch) 使用math.floor函数来对数据进行向下取整，所以 可能会有部分剩余的样本。 在循环中 shuffled_X是随机打乱后的样本，shuffled_X[:,k * mini_batch_size:(k+1)*mini_batch_size] 进行分割。 假如一共有3个子集，即k=0，1，2。 然后设置mini_batch_size为64。 当k=0时， mini_batch_X = shuffled_X[:,0*64:1*64] = shuffled_X[:,0:64] 当k=1时， mini_batch_X = shuffled_X[:,1*64:2*64] = shuffled_X[:,64:128] 当k=2时， mini_batch_X = shuffled_X[:,2*64:3*64] = shuffled_X[:,128:192] 根据python列表的切片操作来进行分割。 标签Y也同理进行操作。 处理剩余样本123456789# 若样本划分后 有剩余样本if m % mini_batch_size != 0: # 获取最后剩余的部分 mini_batch_X = shuffled_X[:,mini_batch_size * num_complete_minibatches:] mini_batch_Y = shuffled_Y[:,mini_batch_size * num_complete_minibatches:] # 存储最后剩余的部分 mini_batch = (mini_batch_X,mini_batch_Y) mini_batches.append(mini_batch) m%mini_batch_size若不等于0，若明有剩余的样本，则对剩余的样本进行处理。 根据之前的切片操作，可知num_complete_minibatches * mini_batch_size可以获得剩余部分的第一个位置。 假设有240个样本，即m=240。mini_batch_size = 64，则一共有 floor(240/64) = 3， 所以num_complete_minibatches=3 。 如图下图一样，进行分割后，可以通过 mini_batch_size * num_complete_minibatches 即 64 * 3 = 192 来获得剩余样本的第一个数据的位置。 完整函数代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def random_mini_batches(X,Y,mini_batch_size=64,seed=0): \"\"\" 从（X，Y）中创建一个随机的mini-batch列表 参数： X - 输入数据，维度为(输入节点数量，样本的数量) Y - 对应的是X的标签，【1 | 0】（蓝|红），维度为(1,样本的数量) mini_batch_size - 每个mini-batch的样本数量 返回： mini-bacthes - 一个同步列表，维度为（mini_batch_X,mini_batch_Y） \"\"\" np.random.seed(seed) #指定随机种子 m = X.shape[1] # 获取样本数 # ===== 打乱训练集 ===== # # 返回一个长度为m的 元素不重复的 随机排序的数组，且里面的数是0到m-1 permutation = list(np.random.permutation(m)) # 通过切片操作按列排序 shuffled_X = X[:,permutation] shuffled_Y = Y[:,permutation].reshape((1,m)) # ===== 分割样本 ====== # mini_batches = [] # 用于存储分割后的样本子集 # 根据划分尺寸mini_batch_size来确定有多少个样本子集 num_complete_minibatches = math.floor(m / mini_batch_size) # 遍历样本子集 for k in range(0,num_complete_minibatches): # 根据mini_batch_size来分割数据集 mini_batch_X = shuffled_X[:,k * mini_batch_size:(k+1)*mini_batch_size] mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k+1)*mini_batch_size] # mini_batch 存储分割后的数据集 mini_batch = (mini_batch_X,mini_batch_Y) # 加到mini_batches列表中 mini_batches.append(mini_batch) # ===== 处理剩余样本 ===== # # 若样本划分后 有剩余样本 if m % mini_batch_size != 0: # 获取最后剩余的部分 mini_batch_X = shuffled_X[:,mini_batch_size * num_complete_minibatches:] mini_batch_Y = shuffled_Y[:,mini_batch_size * num_complete_minibatches:] # 存储最后剩余的部分 mini_batch = (mini_batch_X,mini_batch_Y) mini_batches.append(mini_batch) return mini_batches 动量梯度下降函数初始化Vdw与Vdb1234567891011121314151617181920212223def initialize_velocity(parameters): \"\"\" 初始化速度，v是一个字典： - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" - values:与相应的梯度/参数维度相同的值为零的矩阵。 参数： parameters - 一个字典，包含了以下参数： parameters[\"W\" + str(l)] = Wl parameters[\"b\" + str(l)] = bl 返回: v - 一个字典变量，包含了以下参数： v[\"dW\" + str(l)] = dWl的速度 v[\"db\" + str(l)] = dbl的速度 \"\"\" L = len(parameters) // 2 #神经网络的层数 v = &#123;&#125; for l in range(L): v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)]) v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)]) return v 这里使用np.zeros_like来生成与 传入参数相同shape的 元素均为0的 ndarray。 因为$V_{dW}$与dW的维度相同，而dW的维度又与W相同，故直接使得与W同维度即可。 更新参数函数动量梯度下降更新参数的公式： $ V_{dW} = β * V_{dW} + ( 1 - β) * dW $ $ V_{db} = β * V_{db} + ( 1 - β) * db $ $W = W - α*V_{dW}$ $b = b - α*V_{db}$ 要做的就是遍历每一层，对每一层都执行该操作 123456789101112131415161718192021222324252627282930313233def update_parameters_with_momentun(parameters,grads,v,beta,learning_rate): \"\"\" 使用动量更新参数 参数： parameters - 一个字典类型的变量，包含了以下字段： parameters[\"W\" + str(l)] = Wl parameters[\"b\" + str(l)] = bl grads - 一个包含梯度值的字典变量，具有以下字段： grads[\"dW\" + str(l)] = dWl grads[\"db\" + str(l)] = dbl v - 包含当前速度的字典变量，具有以下字段： v[\"dW\" + str(l)] = ... v[\"db\" + str(l)] = ... beta - 超参数，动量，实数 learning_rate - 学习率，实数 返回： parameters - 更新后的参数字典 v - 包含了更新后的速度变量 \"\"\" # 获取层数 L = len(parameters) // 2 for l in range(L): #计算速度 v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads[\"dW\" + str(l + 1)] v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads[\"db\" + str(l + 1)] #更新参数 parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)] parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)] return parameters,v Adam梯度下降函数说明Adam是动量梯度下降与RMSprop算法的结合。公式为： 初始化： ​ $V_{dW}、V_{db}、S_{dW}、S_{db} = 0$ 动量： ​ $ V_{dW} = β1 * V{dW} + ( 1 - β_1) * dW $ ​ $ V_{db} = β1 * V{db} + ( 1 - β_1) * db $ RMSprop： ​ $ S_{dW} = β2 * S{dW} + ( 1 - β_2) * (dW)^2 $ ​ $ S_{db} = β2 * S{db} + ( 1 - β_2) * (db)^2 $ 计算偏差修正： ​ $V^{corrected}{dW} = \\frac{V{dW}}{1 - β_1^t}$ ​ $V^{corrected}{db} = \\frac{V{db}}{1 - β_1^t}$ ​ $S^{corrected}{dW} = \\frac{S{dW}}{1 - β_2^t}$ ​ $S^{corrected}{db} = \\frac{S{db}}{1 - β_2^t}$ 更新参数： ​ $W = W - α * \\frac{V^{corrected}{dW}}{ \\sqrt{S^{corrected}{dW} + ε}} $ ​ $b = b - α * \\frac{V^{corrected}{db}}{ \\sqrt{S^{corrected}{db} + ε}} $ 初始化Vdw、Vdb、Sdw、Sdb1234567891011121314151617181920212223242526272829303132def initialize_adam(parameters): \"\"\" 初始化v和s，它们都是字典类型的变量，都包含了以下字段： - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" - values：与对应的梯度/参数相同维度的值为零的numpy矩阵 参数： parameters - 包含了以下参数的字典变量： parameters[\"W\" + str(l)] = Wl parameters[\"b\" + str(l)] = bl 返回： v - 包含梯度的指数加权平均值，字段如下： v[\"dW\" + str(l)] = ... v[\"db\" + str(l)] = ... s - 包含平方梯度的指数加权平均值，字段如下： s[\"dW\" + str(l)] = ... s[\"db\" + str(l)] = ... \"\"\" L = len(parameters) // 2 v = &#123;&#125; s = &#123;&#125; for l in range(L): v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)]) v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)]) s[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)]) s[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)]) return (v,s) 与动量梯度下降中的初始化类似，区别就是多了个字典s。 更新参数函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def update_parameters_with_adam(parameters,grads,v,s,t,learning_rate=0.01,beta1=0.9,beta2=0.999,epsilon=1e-8): \"\"\" 使用Adam更新参数 参数： parameters - 包含了以下字段的字典： parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl grads - 包含了梯度值的字典，有以下key值： grads['dW' + str(l)] = dWl grads['db' + str(l)] = dbl v - Adam的变量，第一个梯度的移动平均值，是一个字典类型的变量 s - Adam的变量，平方梯度的移动平均值，是一个字典类型的变量 t - 当前迭代的次数 learning_rate - 学习率 beta1 - 动量，超参数,用于第一阶段，使得曲线的Y值不从0开始（参见天气数据的那个图） beta2 - RMSprop的一个参数，超参数 epsilon - 防止除零操作（分母为0） 返回： parameters - 更新后的参数 v - 第一个梯度的移动平均值，是一个字典类型的变量 s - 平方梯度的移动平均值，是一个字典类型的变量 \"\"\" L = len(parameters) // 2 v_corrected = &#123;&#125; #偏差修正后的值 s_corrected = &#123;&#125; #偏差修正后的值 for l in range(L): # ===== 动量 ===== # v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads[\"dW\" + str(l + 1)] v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads[\"db\" + str(l + 1)] # ===== RMSprop ===== # s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.square(grads[\"dW\" + str(l + 1)]) s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.square(grads[\"db\" + str(l + 1)]) # ===== 计算偏差修正 ===== # v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1,t)) v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1,t)) s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2,t)) s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2,t)) # ===== 更新参数 ===== # parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * (v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon)) parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * (v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)) return (parameters,v,s) 加载数据集通过opt_utils文件读取数据 。 1train_X, train_Y = opt_utils.load_dataset(is_plot=True) 定义模型使用之前实现过的一个三层神经网络（然而我并没有用同样的结构，所以直接用该博主的了） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091def model(X,Y,layers_dims,optimizer,learning_rate=0.0007, mini_batch_size=64,beta=0.9,beta1=0.9,beta2=0.999, epsilon=1e-8,num_epochs=10000,print_cost=True,is_plot=True): \"\"\" 可以运行在不同优化器模式下的3层神经网络模型。 参数： X - 输入数据，维度为（2，输入的数据集里面样本数量） Y - 与X对应的标签 layers_dims - 包含层数和节点数量的列表 optimizer - 字符串类型的参数，用于选择优化类型，【 \"gd\" | \"momentum\" | \"adam\" 】 learning_rate - 学习率 mini_batch_size - 每个小批量数据集的大小 beta - 用于动量优化的一个超参数 beta1 - 用于计算梯度后的指数衰减的估计的超参数 beta1 - 用于计算平方梯度后的指数衰减的估计的超参数 epsilon - 用于在Adam中避免除零操作的超参数，一般不更改 num_epochs - 整个训练集的遍历次数，（视频2.9学习率衰减，1分55秒处，视频中称作“代”）,相当于之前的num_iteration print_cost - 是否打印误差值，每遍历1000次数据集打印一次，但是每100次记录一个误差值，又称每1000代打印一次 is_plot - 是否绘制出曲线图 返回： parameters - 包含了学习后的参数 \"\"\" L = len(layers_dims) # 获取层数 costs = [] # 用于存储代价 t = 0 # 每学习完一个minibatch就增加1 seed = 10 # 随机种子 # 使用opt_utils文件的初始化函数来进行 模型参数的初始化 parameters = opt_utils.initialize_parameters(layers_dims) # 根据传入的参数 optimizer来选择优化器 if optimizer == \"gd\": pass #不使用任何优化器，直接使用梯度下降法 elif optimizer == \"momentum\": v = initialize_velocity(parameters) #使用动量 elif optimizer == \"adam\": v, s = initialize_adam(parameters)#使用Adam优化 else: print(\"optimizer参数错误，程序退出。\") exit(1) # 开始学习，num_epochs为周期，用于mini-batch，相当于原来的迭代次数 for i in range(num_epochs): # 定义随机 minibatches,我们在每次遍历数据集之后增加种子以重新排列数据集，使每次数据的顺序都不同 seed = seed + 1 # 初始化mini-batch列表 minibatches = random_mini_batches(X,Y,mini_batch_size,seed) # 遍历 mini-batch列表，即遍历样本子集 for minibatch in minibatches: # 获取当前minibatch，以minibatch_X代替之前学习的梯度下降法中的X，minibatch_Y代替Y (minibatch_X,minibatch_Y) = minibatch # 正向传播 A3 , cache = opt_utils.forward_propagation(minibatch_X,parameters) # 计算代价 cost = opt_utils.compute_cost(A3 , minibatch_Y) # 反向传播 grads = opt_utils.backward_propagation(minibatch_X,minibatch_Y,cache) # 更新参数 if optimizer == \"gd\": parameters = update_parameters_with_gd(parameters,grads,learning_rate) elif optimizer == \"momentum\": parameters, v = update_parameters_with_momentun(parameters,grads,v,beta,learning_rate) elif optimizer == \"adam\": t = t + 1 parameters , v , s = update_parameters_with_adam(parameters,grads,v,s,t,learning_rate,beta1,beta2,epsilon) # 记录代价 if i % 100 == 0: costs.append(cost) #是否打印误差值 if print_cost and i % 1000 == 0: print(\"第\" + str(i) + \"次遍历整个数据集，当前误差值：\" + str(cost)) #是否绘制曲线图 if is_plot: plt.plot(costs) plt.ylabel('cost') plt.xlabel('epochs (per 100)') plt.title(\"Learning rate = \" + str(learning_rate)) plt.show() return parameters 进行测试未优化的梯度下降1234567891011121314# 定义神经网络结构layers_dims = [train_X.shape[0],5,2,1]parameters = model(train_X, train_Y, layers_dims, optimizer=\"gd\",is_plot=True)#预测preditions = opt_utils.predict(train_X,train_Y,parameters)#绘制分类图plt.title(\"Model with Gradient Descent optimization\")axes = plt.gca()axes.set_xlim([-1.5, 2.5])axes.set_ylim([-1, 1.5])opt_utils.plot_decision_boundary(lambda x: opt_utils.predict_dec(parameters, x.T), train_X, train_Y) 动量梯度下降12345678910111213layers_dims = [train_X.shape[0],5,2,1]#使用动量的梯度下降parameters = model(train_X, train_Y, layers_dims, beta=0.9,optimizer=\"momentum\",is_plot=True)#预测preditions = opt_utils.predict(train_X,train_Y,parameters)#绘制分类图plt.title(\"Model with Momentum optimization\")axes = plt.gca()axes.set_xlim([-1.5, 2.5])axes.set_ylim([-1, 1.5])opt_utils.plot_decision_boundary(lambda x: opt_utils.predict_dec(parameters, x.T), train_X, np.squeeze(train_Y)) Adam算法12345678910111213layers_dims = [train_X.shape[0], 5, 2, 1]#使用Adam优化的梯度下降parameters = model(train_X, train_Y, layers_dims, optimizer=\"adam\",is_plot=True)#预测preditions = opt_utils.predict(train_X,train_Y,parameters)#绘制分类图plt.title(\"Model with Adam optimization\")axes = plt.gca()axes.set_xlim([-1.5, 2.5])axes.set_ylim([-1, 1.5])opt_utils.plot_decision_boundary(lambda x: opt_utils.predict_dec(parameters, x.T), train_X,np.squeeze(train_Y)) 总结这里动量梯度下降与未使用优化算法的梯度下降效果差不多，应该是数据集不算大，拉不开差距 。 明显看到Adam算法的效果要完爆另外两个。","categories":[{"name":"神经网络","slug":"神经网络","permalink":"https://lluuiq.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"学习笔记","slug":"神经网络/学习笔记","permalink":"https://lluuiq.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"https://lluuiq.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]},{"title":"吴恩达深度学习编程作业2-1","slug":"吴恩达深度学习编程作业2-1","date":"2020-06-20T14:33:21.000Z","updated":"2020-06-24T22:12:59.637Z","comments":true,"path":"post/202006202233/","link":"","permalink":"https://lluuiq.com/post/202006202233/","excerpt":"吴恩达深度学习课程《改善深层神经网络：超参数调试、正则化以及优化》第一周（深度学习的实用层面）的编程作业。","text":"吴恩达深度学习课程《改善深层神经网络：超参数调试、正则化以及优化》第一周（深度学习的实用层面）的编程作业。 说明题目与参考作业的地址：【中文】【吴恩达课后编程作业】Course 2 - 改善深层神经网络 - 第一周作业(1&amp;2&amp;3) 相关数据集与前提代码在该博文中下载。 注：本文极其没有营养价值，还请看原文。 本次作业完成内容： 初始化内容： 使用0来初始化参数。 使用随机数来初始化参数。 使用抑梯度异常初始化参数（参见视频中的梯度消失和梯度爆炸）。 正则化模型： 使用二范数对二分类模型正则化，尝试避免过拟合。 使用随机删除节点的方法精简模型，同样是为了尝试避免过拟合。 梯度校验： 对模型使用梯度校验，检测它是否在梯度下降的过程中出现误差过大的情况。 导入相关库12345678910111213import numpy as npimport sklearnimport sklearn.datasetsimport init_utils #用于初始化import reg_utils #用于正则化import gc_utils #用于梯度校验import matplotlib.pyplot as plt%matplotlib inlineplt.rcParams['figure.figsize'] = (7.0, 4.0) # 设置画布的默认大小plt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray' 初始化读取数据并绘图init_utils的load_dataset函数中已有绘制图案代码。 用4个变量接收训练集、测试集的输入特征与标签。 1train_X, train_Y, test_X, test_Y = init_utils.load_dataset(is_plot=True) 说明建立一个分类器将红蓝点分开，使用之前的模型（因自己之前建立的模型与该作业的不同，故这里直接使用了原文的模型代码）。 使用三种初始化方法： 0初始化，传参的值为“zeros”，核心代码： parameters[‘W’ + str(l)] = np.zeros((layers_dims[l], layers_dims[l - 1])) 随机数初始化，传参的值为 “random”，核心代码： parameters[‘W’ + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 10 抑梯度异常初始化，传参的值为“he”，核心代码： parameters[‘W’ + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * np.sqrt(2 / layers_dims[l - 1]) 查看模型代码作用已写在注释， 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364def model(X,Y,learning_rate=0.01,num_iterations=15000,print_cost=True,initialization=\"he\",is_polt=True): \"\"\" 实现一个三层的神经网络：LINEAR -&gt;RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID 参数： X - 输入的数据，维度为(2, 要训练/测试的数量) Y - 标签，【0 | 1】，维度为(1，对应的是输入的数据的标签) learning_rate - 学习速率 num_iterations - 迭代的次数 print_cost - 是否打印成本值，每迭代1000次打印一次 initialization - 字符串类型，初始化的类型【\"zeros\" | \"random\" | \"he\"】 is_polt - 是否绘制梯度下降的曲线图 返回 parameters - 学习后的参数 \"\"\" grads = &#123;&#125; # 存储导数 costs = [] # 存储代价 m = X.shape[1] # 获取样本数 layers_dims = [X.shape[0],10,5,1] # 定义神经网络结构 # 根据传入的参数来进行初始化 if initialization == \"zeros\": parameters = initialize_parameters_zeros(layers_dims) # 0初始化 elif initialization == \"random\": parameters = initialize_parameters_random(layers_dims) # 随机初始化 elif initialization == \"he\": parameters = initialize_parameters_he(layers_dims) # 抑梯度异常初始化 else : print(\"错误的初始化参数！程序退出\") exit # 开始学习 for i in range(0,num_iterations): #正向传播 a3 , cache = init_utils.forward_propagation(X,parameters) # 计算成本 cost = init_utils.compute_loss(a3,Y) #反向传播 grads = init_utils.backward_propagation(X,Y,cache) #更新参数 parameters = init_utils.update_parameters(parameters,grads,learning_rate) #记录成本 if i % 1000 == 0: costs.append(cost) #打印成本 if print_cost: print(\"第\" + str(i) + \"次迭代，成本值为：\" + str(cost)) #学习完毕，绘制成本曲线 if is_polt: plt.plot(costs) plt.ylabel('cost') plt.xlabel('iterations (per hundreds)') plt.title(\"Learning rate =\" + str(learning_rate)) plt.show() #返回学习完毕后的参数 return parameters 使用0初始化定义函数123456789101112131415161718192021222324252627def initialize_parameters_zeros(layers_dims): \"\"\" 将模型的参数全部设置为0 参数： layers_dims - 列表，模型的层数和对应每一层的节点的数量 返回 parameters - 包含了所有W和b的字典 W1 - 权重矩阵，维度为（layers_dims[1], layers_dims[0]） b1 - 偏置向量，维度为（layers_dims[1],1） ··· WL - 权重矩阵，维度为（layers_dims[L], layers_dims[L -1]） bL - 偏置向量，维度为（layers_dims[L],1） \"\"\" parameters = &#123;&#125; L = len(layers_dims) #网络层数 for l in range(1,L): parameters[\"W\" + str(l)] = np.zeros((layers_dims[l],layers_dims[l-1])) parameters[\"b\" + str(l)] = np.zeros((layers_dims[l],1)) #使用断言确保我的数据格式是正确的 assert(parameters[\"W\" + str(l)].shape == (layers_dims[l],layers_dims[l-1])) assert(parameters[\"b\" + str(l)].shape == (layers_dims[l],1)) return parameters 训练模型1parameters = model(train_X, train_Y, initialization = \"zeros\",is_polt=True) 可以看到代价根本没有降下来，W初始化为0，梯度下降失效，神经网络再多的层数也只是在计算输入特征X的线性组合而已。 查看准确率调用init_utils的predict函数直接进行预测 1234print (\"训练集:\")predictions_train = init_utils.predict(train_X, train_Y, parameters)print (\"测试集:\")predictions_test = init_utils.predict(test_X, test_Y, parameters) 查看细节123456789print(\"predictions_train = \" + str(predictions_train))print(\"predictions_test = \" + str(predictions_test))# 绘制决策边界plt.title(\"Model with Zeros initialization\")axes = plt.gca()axes.set_xlim([-1.5, 1.5])axes.set_ylim([-1.5, 1.5])init_utils.plot_decision_boundary(lambda x: init_utils.predict_dec(parameters, x.T), train_X, np.squeeze(train_Y)) 注意这里最后一句代码，将原本的train_Y 使用np.squeeze(train_Y)换成压缩后的数组，否则绘图会因为维度报错。 预测值全是0，图像整体呈一个颜色，并没有进行分类。 随机初始化定义函数1234567891011121314151617181920212223242526def initialize_parameters_random(layers_dims): \"\"\" 参数： layers_dims - 列表，模型的层数和对应每一层的节点的数量 返回 parameters - 包含了所有W和b的字典 W1 - 权重矩阵，维度为（layers_dims[1], layers_dims[0]） b1 - 偏置向量，维度为（layers_dims[1],1） ··· WL - 权重矩阵，维度为（layers_dims[L], layers_dims[L -1]） b1 - 偏置向量，维度为（layers_dims[L],1） \"\"\" np.random.seed(3) # 指定随机种子 parameters = &#123;&#125; L = len(layers_dims) # 层数 for l in range(1, L): parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 10 #使用10倍缩放 parameters['b' + str(l)] = np.zeros((layers_dims[l], 1)) #使用断言确保我的数据格式是正确的 assert(parameters[\"W\" + str(l)].shape == (layers_dims[l],layers_dims[l-1])) assert(parameters[\"b\" + str(l)].shape == (layers_dims[l],1)) return parameters 训练模型1parameters = model(train_X, train_Y, initialization = \"random\",is_polt=True) 相比使用0初始化，随机初始化明显使代价函数降了下来。 查看准确率1234print(\"训练集：\")predictions_train = init_utils.predict(train_X, train_Y, parameters)print(\"测试集：\")predictions_test = init_utils.predict(test_X, test_Y, parameters) 结果正常多了。 查看细节同使用0初始化一样，最后绘图代码的tarin_Y要使用np.squeeze(train_Y)替换 123456789print(\"predictions_train = \" + str(predictions_train))print(\"predictions_test = \" + str(predictions_test))# 绘制决策边界plt.title(\"Model with large random initialization\")axes = plt.gca()axes.set_xlim([-1.5, 1.5])axes.set_ylim([-1.5, 1.5])init_utils.plot_decision_boundary(lambda x: init_utils.predict_dec(parameters, x.T), train_X, np.squeeze(train_Y)) 成功进行了分类，但是效果不够很好， 抑梯度异常初始化定义函数与随机初始化的区别是，这里用 np.random.randn初始化后乘上np.sqrt(2 / layers_dims[l - 1]) （ReLU使用这个效果比较好） 1234567891011121314151617181920212223242526def initialize_parameters_he(layers_dims): \"\"\" 参数： layers_dims - 列表，模型的层数和对应每一层的节点的数量 返回 parameters - 包含了所有W和b的字典 W1 - 权重矩阵，维度为（layers_dims[1], layers_dims[0]） b1 - 偏置向量，维度为（layers_dims[1],1） ··· WL - 权重矩阵，维度为（layers_dims[L], layers_dims[L -1]） b1 - 偏置向量，维度为（layers_dims[L],1） \"\"\" np.random.seed(3) # 指定随机种子 parameters = &#123;&#125; L = len(layers_dims) # 层数 for l in range(1, L): parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * np.sqrt(2 / layers_dims[l - 1]) parameters['b' + str(l)] = np.zeros((layers_dims[l], 1)) #使用断言确保我的数据格式是正确的 assert(parameters[\"W\" + str(l)].shape == (layers_dims[l],layers_dims[l-1])) assert(parameters[\"b\" + str(l)].shape == (layers_dims[l],1)) return parameters 训练模型1parameters = model(train_X, train_Y, initialization = \"he\",is_polt=True) 代价函数下降速度非常明显的变快了，并且代价函数最小值也得到了优化。 查看准确率1234print(\"训练集:\")predictions_train = init_utils.predict(train_X, train_Y, parameters)print(\"测试集:\")init_utils.predictions_test = init_utils.predict(test_X, test_Y, parameters) 准确率也得到了极大提高 查看细节123456789print(\"predictions_train = \" + str(predictions_train))print(\"predictions_test = \" + str(predictions_test))# 绘制决策边界plt.title(\"Model with He initialization\")axes = plt.gca()axes.set_xlim([-1.5, 1.5])axes.set_ylim([-1.5, 1.5])init_utils.plot_decision_boundary(lambda x: init_utils.predict_dec(parameters, x.T), train_X, np.squeeze(train_Y)) 总结 模型参数的初始化方式会导致不同的算法性能。 使用0初始化会使得梯度下降失效，神经网络在计算输入特征的线性组合。 使用随机初始化会正确执行梯度下降，但若初始化时的值过大，会导致算法性能过低。 正则化问题描述假设你现在是一个AI专家，你需要设计一个模型，可以用于推荐在足球场中守门员将球发至哪个位置可以让本队的球员抢到球的可能性更大。说白了，实际上就是一个二分类，一半是己方抢到球，一半就是对方抢到球，我们来看一下这个图： 读取数据并绘图注： 这里和前面的绘图一样，需要将train_Y修改为np.squeeze(train_Y)，但读取数据这里就不能在传参的时候修改了，需要打开reg_utils文件找到load_2D_dataset函数，将里面绘图部分的c=train_Y 改为 c=np.squeeze(train_Y)。 1train_X, train_Y, test_X, test_Y = reg_utils.load_2D_dataset(is_plot=True) 每个点都代表守门员将球发至的位置，蓝点为己方球员抢到球，红点为对手球员抢到球。 训练模型，找到适合我方球员抢球的位置。 查看模型123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384def model(X,Y,learning_rate=0.3,num_iterations=30000,print_cost=True,is_plot=True,lambd=0,keep_prob=1): \"\"\" 实现一个三层的神经网络：LINEAR -&gt;RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID 参数： X - 输入的数据，维度为(2, 要训练/测试的数量) Y - 标签，【0(蓝色) | 1(红色)】，维度为(1，对应的是输入的数据的标签) learning_rate - 学习速率 num_iterations - 迭代的次数 print_cost - 是否打印成本值，每迭代10000次打印一次，但是每1000次记录一个成本值 is_polt - 是否绘制梯度下降的曲线图 lambd - L2正则化的超参数，即λ的值，实数。默认值为0表示不使用L2正则化。 keep_prob - 随机删除节点的概率，默认为1表示不使用随机失活 返回 parameters - 学习后的参数 \"\"\" grads = &#123;&#125; costs = [] m = X.shape[1] layers_dims = [X.shape[0],20,3,1] #初始化参数 parameters = reg_utils.initialize_parameters(layers_dims) #开始学习 for i in range(0,num_iterations): #前向传播 ##是否随机删除节点 if keep_prob == 1: ###不随机删除节点 a3 , cache = reg_utils.forward_propagation(X,parameters) elif keep_prob &lt; 1: ###随机删除节点 a3 , cache = forward_propagation_with_dropout(X,parameters,keep_prob) else: print(\"keep_prob参数错误！程序退出。\") exit #计算成本 ## 是否使用二范数 if lambd == 0: ###不使用L2正则化 cost = reg_utils.compute_cost(a3,Y) else: ###使用L2正则化 cost = compute_cost_with_regularization(a3,Y,parameters,lambd) #反向传播 ##可以同时使用L2正则化和随机删除节点，但是本次实验不同时使用。 assert(lambd == 0 or keep_prob ==1) ##两个参数的使用情况 if (lambd == 0 and keep_prob == 1): ### 不使用L2正则化和不使用随机删除节点 grads = reg_utils.backward_propagation(X,Y,cache) elif lambd != 0: ### 使用L2正则化，不使用随机删除节点 grads = backward_propagation_with_regularization(X, Y, cache, lambd) elif keep_prob &lt; 1: ### 使用随机删除节点，不使用L2正则化 grads = backward_propagation_with_dropout(X, Y, cache, keep_prob) #更新参数 parameters = reg_utils.update_parameters(parameters, grads, learning_rate) #记录并打印成本 if i % 1000 == 0: ## 记录成本 costs.append(cost) if (print_cost and i % 10000 == 0): #打印成本 print(\"第\" + str(i) + \"次迭代，成本值为：\" + str(cost)) #是否绘制成本曲线图 if is_plot: plt.plot(costs) plt.ylabel('cost') plt.xlabel('iterations (x1,000)') plt.title(\"Learning rate =\" + str(learning_rate)) plt.show() #返回学习后的参数 return parameters 不使用正则化训练模型12345parameters = model(train_X, train_Y,is_plot=True)print(\"训练集:\")predictions_train = reg_utils.predict(train_X, train_Y, parameters)print(\"测试集:\")predictions_test = reg_utils.predict(test_X, test_Y, parameters) 绘制决策边界12345plt.title(\"Model without regularization\")axes = plt.gca()axes.set_xlim([-0.75,0.40])axes.set_ylim([-0.75,0.65])reg_utils.plot_decision_boundary(lambda x: reg_utils.predict_dec(parameters, x.T), train_X, np.squeeze(train_Y)) 通过无正则化的决策边界，可以看出过拟合现象 使用L2正则化定义L2函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# 计算正则化代价def compute_cost_with_regularization(A3,Y,parameters,lambd): \"\"\" 实现公式2的L2正则化计算成本 参数： A3 - 正向传播的输出结果，维度为（输出节点数量，训练/测试的数量） Y - 标签向量，与数据一一对应，维度为(输出节点数量，训练/测试的数量) parameters - 包含模型学习后的参数的字典 返回： cost - 使用公式2计算出来的正则化损失的值 \"\"\" m = Y.shape[1] # 获取各层的模型参数W W1 = parameters[\"W1\"] W2 = parameters[\"W2\"] W3 = parameters[\"W3\"] # 计算代价 cross_entropy_cost = reg_utils.compute_cost(A3,Y) # 计算L2正则化的代价 L2_regularization_cost = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) / (2 * m) # 计算总代价 cost = cross_entropy_cost + L2_regularization_cost return cost#因为改变了成本函数，所以必须改变向后传播的函数， 所有的梯度都必须根据这个新的成本值来计算。def backward_propagation_with_regularization(X, Y, cache, lambd): \"\"\" 实现我们添加了L2正则化的模型的后向传播。 参数： X - 输入数据集，维度为（输入节点数量，数据集里面的数量） Y - 标签，维度为（输出节点数量，数据集里面的数量） cache - 来自forward_propagation（）的cache输出 lambda - regularization超参数，实数 返回： gradients - 一个包含了每个参数、激活值和预激活值变量的梯度的字典 \"\"\" m = X.shape[1] # 获取样本数 # 获取cache的值 (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache dZ3 = A3 - Y # 正则化的反向传播 dW3 = (1 / m) * np.dot(dZ3,A2.T) + ((lambd * W3) / m ) db3 = (1 / m) * np.sum(dZ3,axis=1,keepdims=True) dA2 = np.dot(W3.T,dZ3) dZ2 = np.multiply(dA2,np.int64(A2 &gt; 0)) dW2 = (1 / m) * np.dot(dZ2,A1.T) + ((lambd * W2) / m) db2 = (1 / m) * np.sum(dZ2,axis=1,keepdims=True) dA1 = np.dot(W2.T,dZ2) dZ1 = np.multiply(dA1,np.int64(A1 &gt; 0)) dW1 = (1 / m) * np.dot(dZ1,X.T) + ((lambd * W1) / m) db1 = (1 / m) * np.sum(dZ1,axis=1,keepdims=True) gradients = &#123;\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3, \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1&#125; return gradients 训练模型12345parameters = model(train_X, train_Y, lambd=0.7,is_plot=True)print(\"使用正则化，训练集:\")predictions_train = reg_utils.predict(train_X, train_Y, parameters)print(\"使用正则化，测试集:\")predictions_test = reg_utils.predict(test_X, test_Y, parameters) 训练集准确率稍微降低了点，测试集提高了点，两者几乎持平。 绘制决策边界12345plt.title(\"Model with L2-regularization\")axes = plt.gca()axes.set_xlim([-0.75,0.40])axes.set_ylim([-0.75,0.65])reg_utils.plot_decision_boundary(lambda x: reg_utils.predict_dec(parameters, x.T), train_X, np.squeeze(train_Y)) 根据决策边界可以看出改善了过拟合现象。 实际上L2正则化的效果会根据超参数λ的的值而改变。 L2正则化依赖于较小权重的模型比具有较大权重的模型更简单这样的假设，因此，通过削减成本函数中权重的平方值，可以将所有权重值逐渐改变到到较小的值。权值数值高的话会有更平滑的模型，其中输入变化时输出变化更慢，但是需要花费更多的时间。 使用随机失活定义dropout正向传播函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869def forward_propagation_with_dropout(X,parameters,keep_prob=0.5): \"\"\" 实现具有随机舍弃节点的前向传播。 LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID. 参数： X - 输入数据集，维度为（2，示例数） parameters - 包含参数“W1”，“b1”，“W2”，“b2”，“W3”，“b3”的python字典： W1 - 权重矩阵，维度为（20,2） b1 - 偏向量，维度为（20,1） W2 - 权重矩阵，维度为（3,20） b2 - 偏向量，维度为（3,1） W3 - 权重矩阵，维度为（1,3） b3 - 偏向量，维度为（1,1） keep_prob - 随机删除的概率，实数 返回： A3 - 最后的激活值，维度为（1,1），正向传播的输出 cache - 存储了一些用于计算反向传播的数值的元组 \"\"\" np.random.seed(1) W1 = parameters[\"W1\"] b1 = parameters[\"b1\"] W2 = parameters[\"W2\"] b2 = parameters[\"b2\"] W3 = parameters[\"W3\"] b3 = parameters[\"b3\"] #LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID Z1 = np.dot(W1,X) + b1 A1 = reg_utils.relu(Z1) #下面的步骤1-4对应于上述的步骤1-4。 D1 = np.random.rand(A1.shape[0],A1.shape[1]) #步骤1：初始化矩阵D1 = np.random.rand(..., ...) D1 = D1 &lt; keep_prob #步骤2：将D1的值转换为0或1（使用keep_prob作为阈值） A1 = A1 * D1 #步骤3：舍弃A1的一些节点（将它的值变为0或False） A1 = A1 / keep_prob #步骤4：缩放未舍弃的节点(不为0)的值 \"\"\" #不理解的同学运行一下下面代码就知道了。 import numpy as np np.random.seed(1) A1 = np.random.randn(1,3) D1 = np.random.rand(A1.shape[0],A1.shape[1]) keep_prob=0.5 D1 = D1 &lt; keep_prob print(D1) A1 = 0.01 A1 = A1 * D1 A1 = A1 / keep_prob print(A1) \"\"\" Z2 = np.dot(W2,A1) + b2 A2 = reg_utils.relu(Z2) #下面的步骤1-4对应于上述的步骤1-4。 D2 = np.random.rand(A2.shape[0],A2.shape[1]) #步骤1：初始化矩阵D2 = np.random.rand(..., ...) D2 = D2 &lt; keep_prob #步骤2：将D2的值转换为0或1（使用keep_prob作为阈值） A2 = A2 * D2 #步骤3：舍弃A1的一些节点（将它的值变为0或False） A2 = A2 / keep_prob #步骤4：缩放未舍弃的节点(不为0)的值 Z3 = np.dot(W3, A2) + b3 A3 = reg_utils.sigmoid(Z3) cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) return A3, cache 定义dropout反向传播函数1234567891011121314151617181920212223242526272829303132333435363738394041def backward_propagation_with_dropout(X,Y,cache,keep_prob): \"\"\" 实现我们随机删除的模型的后向传播。 参数： X - 输入数据集，维度为（2，示例数） Y - 标签，维度为（输出节点数量，示例数量） cache - 来自forward_propagation_with_dropout（）的cache输出 keep_prob - 随机删除的概率，实数 返回： gradients - 一个关于每个参数、激活值和预激活变量的梯度值的字典 \"\"\" m = X.shape[1] (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache dZ3 = A3 - Y dW3 = (1 / m) * np.dot(dZ3,A2.T) db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True) dA2 = np.dot(W3.T, dZ3) dA2 = dA2 * D2 # 步骤1：使用正向传播期间相同的节点，舍弃那些关闭的节点（因为任何数乘以0或者False都为0或者False） dA2 = dA2 / keep_prob # 步骤2：缩放未舍弃的节点(不为0)的值 dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0)) dW2 = 1. / m * np.dot(dZ2, A1.T) db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True) dA1 = np.dot(W2.T, dZ2) dA1 = dA1 * D1 # 步骤1：使用正向传播期间相同的节点，舍弃那些关闭的节点（因为任何数乘以0或者False都为0或者False） dA1 = dA1 / keep_prob # 步骤2：缩放未舍弃的节点(不为0)的值 dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0)) dW1 = 1. / m * np.dot(dZ1, X.T) db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True) gradients = &#123;\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1&#125; return gradients 训练模型123456parameters = model(train_X, train_Y, keep_prob=0.86, learning_rate=0.3,is_plot=True)print(\"使用随机删除节点，训练集:\")predictions_train = reg_utils.predict(train_X, train_Y, parameters)print(\"使用随机删除节点，测试集:\")reg_utils.predictions_test = reg_utils.predict(test_X, test_Y, parameters) 这里使用keep_prob=0.86，表示对于每个神经单元，每次迭代有86%的概率保留，24%的概率消除。 代价更小，训练集的准确率下降一点，测试集的准确率得到提高。 绘制决策边界12345plt.title(\"Model with dropout\")axes = plt.gca()axes.set_xlim([-0.75, 0.40])axes.set_ylim([-0.75, 0.65])reg_utils.plot_decision_boundary(lambda x: reg_utils.predict_dec(parameters, x.T), train_X, np.squeeze(train_Y)) 相比使用L2正则化，随机失活的决策边界更加平滑。 梯度校验问题描述 一维线性定义正向传播函数123456789101112131415def forward_propagation(x,theta): \"\"\" 实现图中呈现的线性前向传播（计算J）（J（theta）= theta * x） 参数： x - 一个实值输入 theta - 参数，也是一个实数 返回： J - 函数J的值，用公式J（theta）= theta * x计算 \"\"\" J = np.dot(theta,x) return J 定义反向传播函数1234567891011121314def backward_propagation(x,theta): \"\"\" 计算J相对于θ的导数。 参数： x - 一个实值输入 theta - 参数，也是一个实数 返回： dtheta - 相对于θ的成本梯度 \"\"\" dtheta = x return dtheta 定义校验函数梯度检测的过程为： $θ^+ = θ + ε$ $ θ^− = θ − ε $ $J^+ = J(θ^+) $ $J^- = J(θ^-) $ $gradapprox = \\frac{J^+ − J^−}{2∗ε}$ 然后计算反向传播的值grad，然后计算误差 $ difference = \\frac{||grad−gradapprox||_2}{||grad||_2 + ||gradapprox||_2}$ 若 $ difference &lt; 10^−7 $，则grad是正确的。 1234567891011121314151617181920212223242526272829303132333435def gradient_check(x,theta,epsilon=1e-7): \"\"\" 实现图中的反向传播。 参数： x - 一个实值输入 theta - 参数，也是一个实数 epsilon - 使用公式（3）计算输入的微小偏移以计算近似梯度 返回： 近似梯度和后向传播梯度之间的差异 \"\"\" #使用公式（3）的左侧计算gradapprox。 thetaplus = theta + epsilon # Step 1 thetaminus = theta - epsilon # Step 2 J_plus = forward_propagation(x, thetaplus) # Step 3 J_minus = forward_propagation(x, thetaminus) # Step 4 gradapprox = (J_plus - J_minus) / (2 * epsilon) # Step 5 #检查gradapprox是否足够接近backward_propagation（）的输出 grad = backward_propagation(x, theta) numerator = np.linalg.norm(grad - gradapprox) # Step 1' denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox) # Step 2' difference = numerator / denominator # Step 3' if difference &lt; 1e-7: print(\"梯度检查：梯度正常!\") else: print(\"梯度检查：梯度超出阈值!\") return difference 进行测试12345#测试gradient_checkprint(\"-----------------测试gradient_check-----------------\")x, theta = 2, 4difference = gradient_check(x, theta)print(\"difference = \" + str(difference)) 高维度梯度校验定义正向传播函数高维度梯度校验与一维类似，区别在于不只是校验一个导数，而是对每一层都进行校验。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081def forward_propagation_n(X,Y,parameters): \"\"\" 实现图中的前向传播（并计算成本）。 参数： X - 训练集为m个例子 Y - m个示例的标签 parameters - 包含参数“W1”，“b1”，“W2”，“b2”，“W3”，“b3”的python字典： W1 - 权重矩阵，维度为（5,4） b1 - 偏向量，维度为（5,1） W2 - 权重矩阵，维度为（3,5） b2 - 偏向量，维度为（3,1） W3 - 权重矩阵，维度为（1,3） b3 - 偏向量，维度为（1,1） 返回： cost - 成本函数（logistic） \"\"\" m = X.shape[1] W1 = parameters[\"W1\"] b1 = parameters[\"b1\"] W2 = parameters[\"W2\"] b2 = parameters[\"b2\"] W3 = parameters[\"W3\"] b3 = parameters[\"b3\"] # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID Z1 = np.dot(W1,X) + b1 A1 = gc_utils.relu(Z1) Z2 = np.dot(W2,A1) + b2 A2 = gc_utils.relu(Z2) Z3 = np.dot(W3,A2) + b3 A3 = gc_utils.sigmoid(Z3) #计算成本 logprobs = np.multiply(-np.log(A3), Y) + np.multiply(-np.log(1 - A3), 1 - Y) cost = (1 / m) * np.sum(logprobs) cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) return cost, cachedef backward_propagation_n(X,Y,cache): \"\"\" 实现图中所示的反向传播。 参数： X - 输入数据点（输入节点数量，1） Y - 标签 cache - 来自forward_propagation_n（）的cache输出 返回： gradients - 一个字典，其中包含与每个参数、激活和激活前变量相关的成本梯度。 \"\"\" m = X.shape[1] (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache dZ3 = A3 - Y dW3 = (1. / m) * np.dot(dZ3,A2.T) dW3 = 1. / m * np.dot(dZ3, A2.T) db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True) dA2 = np.dot(W3.T, dZ3) dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0)) #dW2 = 1. / m * np.dot(dZ2, A1.T) * 2 # Should not multiply by 2 dW2 = 1. / m * np.dot(dZ2, A1.T) db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True) dA1 = np.dot(W2.T, dZ2) dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0)) dW1 = 1. / m * np.dot(dZ1, X.T) #db1 = 4. / m * np.sum(dZ1, axis=1, keepdims=True) # Should not multiply by 4 db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True) gradients = &#123;\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3, \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1&#125; return gradients 定义校验函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def gradient_check_n(parameters,gradients,X,Y,epsilon=1e-7): \"\"\" 检查backward_propagation_n是否正确计算forward_propagation_n输出的成本梯度 参数： parameters - 包含参数“W1”，“b1”，“W2”，“b2”，“W3”，“b3”的python字典： grad_output_propagation_n的输出包含与参数相关的成本梯度。 x - 输入数据点，维度为（输入节点数量，1） y - 标签 epsilon - 计算输入的微小偏移以计算近似梯度 返回： difference - 近似梯度和后向传播梯度之间的差异 \"\"\" #初始化参数 parameters_values , keys = gc_utils.dictionary_to_vector(parameters) #keys用不到 grad = gc_utils.gradients_to_vector(gradients) num_parameters = parameters_values.shape[0] J_plus = np.zeros((num_parameters,1)) J_minus = np.zeros((num_parameters,1)) gradapprox = np.zeros((num_parameters,1)) #计算gradapprox for i in range(num_parameters): #计算J_plus [i]。输入：“parameters_values，epsilon”。输出=“J_plus [i]” thetaplus = np.copy(parameters_values) # Step 1 thetaplus[i][0] = thetaplus[i][0] + epsilon # Step 2 J_plus[i], cache = forward_propagation_n(X,Y,gc_utils.vector_to_dictionary(thetaplus)) # Step 3 ，cache用不到 #计算J_minus [i]。输入：“parameters_values，epsilon”。输出=“J_minus [i]”。 thetaminus = np.copy(parameters_values) # Step 1 thetaminus[i][0] = thetaminus[i][0] - epsilon # Step 2 J_minus[i], cache = forward_propagation_n(X,Y,gc_utils.vector_to_dictionary(thetaminus))# Step 3 ，cache用不到 #计算gradapprox[i] gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon) #通过计算差异比较gradapprox和后向传播梯度。 numerator = np.linalg.norm(grad - gradapprox) # Step 1' denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox) # Step 2' difference = numerator / denominator # Step 3' if difference &lt; 1e-7: print(\"梯度检查：梯度正常!\") else: print(\"梯度检查：梯度超出阈值!\") return difference","categories":[{"name":"神经网络","slug":"神经网络","permalink":"https://lluuiq.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"学习笔记","slug":"神经网络/学习笔记","permalink":"https://lluuiq.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"https://lluuiq.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]},{"title":"网易云代理(听灰色歌曲)","slug":"网易云代理(听灰色歌曲)","date":"2020-06-19T14:33:21.000Z","updated":"2020-06-24T22:13:41.764Z","comments":true,"path":"post/202006192233/","link":"","permalink":"https://lluuiq.com/post/202006192233/","excerpt":"想听一听网易云的灰色歌曲","text":"想听一听网易云的灰色歌曲 说明使用的工具地址：UnblockNeteaseMusic 分为本地代理与服务器代理两种方法： 使用服务器代理较为方便，所有设备的代理地址填服务器的即可。 本地的话只能在本机上运行，且更换设备需要重新配置等等。 这里就记录下自己使用服务器配置代理的过程。 项目官方说法 安装宝塔若安装过宝塔的可直接跳过 宝塔官网：宝塔 根据安装指令，CentOS输入： 1yum install -y wget &amp;&amp; wget -O install.sh http:&#x2F;&#x2F;download.bt.cn&#x2F;install&#x2F;install_6.0.sh &amp;&amp; sh install.sh 途中需要输入Y进行确认。 安装成功会出现下图提示： 网址 用户名 密码 安装好宝塔后，服务器的安全组中配置一下规则，放行默认端口8888 如图填写即可，端口填8888，源填0.0.0.0/0，描述或备注随意。然后保存。 本机上进入安装好宝塔输出的网址，填写默认用户名与密码即可进入。 按需配置，若仅仅是为了执行node.js的话可以不安装 网址后面的端口、用户名、密码，均可在宝塔上进行修改。 建议修改宝塔的端口，修改端口需要在服务器上放行端口。 点击宝塔中的立即修改 随后会跳转到设置界面。 顺便建议修改： 安全入口：默认生成的网址的后缀，修改一个好记的 面板用户：用户名 面板密码：密码 最后保存即可。 宝塔上安装PM2在面板左侧进入软件商店 然后搜素PM2进行安装 安装完成后，会顺带在服务器上安装git。 可以在右方开启首页显示，这样以后想管理PM2直接在首页就可以进入，不用再进软件商店了。 安装UnblockNeteaseMusic克隆项目文件，路径随意，记得就行。 1git clone https:&#x2F;&#x2F;github.com&#x2F;nondanee&#x2F;UnblockNeteaseMusic.git 例如我在 ~/lluuiq/下进行克隆，则项目路径为~/lluuiq/UnblockNeteaseMusic/ 配置PM2设置PM2： 项目所在根目录即克隆后的 项目路径，例如我的为/lluuiq/UnblockNeteaseMusic/ 启动文件名称填 app.js 项目名称随意。 最后点击添加即可。 放行端口服务器上和[安装宝塔](# 安装宝塔)中的放行端口一样。 UnblockNeteaseMusic的默认端口为8080，因此需要放行8080端口。 在宝塔面板的防火墙中也要放行8080端口。在左侧菜单栏的安全里，设置防火墙 开启代理以windows为例，在网易云的设置里，进入工具，选择自定义代理，然后输入 服务器：自己的服务器公网IP 端口：UnblockNeteaseMusic的端口 输入完成后，点击测试，会出现该代理可用，然后点击确定重启网易云。 甚至可以听一些不存在的歌曲 只要服务器一直在运行，就可以随时设置代理来听一些灰色歌曲了。 不同系统设置代理的方法，参考说明文档： 修改UnblockNeteaseMusic端口号（可选）如果不想使用默认的8080端口，则可以修改配置。 编辑UnblockNeteaseMusic项目路径下的 src/app.js，将红框中的8080改成自己想要的端口号，假设为3XX9 然后在PM2中删除之前添加的任务 重新添加一次 端口变成了修改配置文件中的端口。 然后分别到服务器和宝塔面板中放行自定义的端口 服务器： 宝塔： 修改网易云的代理测试一下： 接下来正常使用就可以了。 也可以把之前8080端口的放行给删掉","categories":[{"name":"妙妙工具箱","slug":"妙妙工具箱","permalink":"https://lluuiq.com/categories/%E5%A6%99%E5%A6%99%E5%B7%A5%E5%85%B7%E7%AE%B1/"}],"tags":[{"name":"超能力","slug":"超能力","permalink":"https://lluuiq.com/tags/%E8%B6%85%E8%83%BD%E5%8A%9B/"}]},{"title":"吴恩达深度学习编程作业1-4","slug":"吴恩达深度学习编程作业1-4","date":"2020-06-14T14:33:21.000Z","updated":"2020-06-24T22:12:56.283Z","comments":true,"path":"post/202006142233/","link":"","permalink":"https://lluuiq.com/post/202006142233/","excerpt":"吴恩达深度学习课程《神经网络和深度学习》第三周（深层神经网络）的编程作业。","text":"吴恩达深度学习课程《神经网络和深度学习》第三周（深层神经网络）的编程作业。 说明题目与参考作业的地址：【中文】【吴恩达课后编程作业】Course 1 - 神经网络和深度学习 - 第四周作业(1&amp;2) 相关数据集与前提代码在该博文中下载。 查看下载的资料下载后如图所示： 打开datasets，可以发现是h5文件，与第二周的作业文件相同，是猫的图片数据 文件因为内容过多，就不截图了。 打开dnn_utils.py，可以看到提供了神经网络的ReLU激活函数与sigmoid函数（正向与反向） 打开lr_utils.py，可以看到是第二周作业的读取训练集、测试集数据的代码 打开testCases.py， 该文件提供了各个函数的测试用参数（测试用的，该文章中没用到） 根据以上可知，这次作业是使用深层神经网络来完成第二周的作业：识别猫的图片。 读取数据与第二周的作业步骤相同。 12345678910# 加载 lr_utils的load_dataset函数from lr_utils import load_dataset# 定义变量来获取load_dataset函数的返回值train_set_x , train_set_y , test_set_x , test_set_y , classes = load_dataset()print(\"训练集特征的维度：\",train_set_x.shape,\"训练集特征的类型\",type(train_set_x))print(\"训练集标签的维度：\",train_set_y.shape,\"训练集标签的类型\",type(train_set_y))print(\"测试集特征的维度：\",test_set_x.shape,\"测试集特征的类型\",type(test_set_x))print(\"测试集标签的维度：\",test_set_y.shape,\"测试集标签的类型\",type(test_set_y))print(\"classes的维度：\",classes.shape,\"classes的类型\",type(classes)) 数据处理与第二周的作业步骤相同。 12345678train_set_x_flatten = train_set_x.reshape(train_set_x.shape[0],-1).Ttest_set_x_flatten = test_set_x.reshape(test_set_x.shape[0],-1).Ttrain_set_x_flatten_normalization = train_set_x_flatten/255test_set_x_flatten_normalization = test_set_x_flatten/255print(\"处理后的训练集维度：\",train_set_x_flatten_normalization.shape)print(\"处理后的测试集维度：\",test_set_x_flatten_normalization.shape) 处理后图片的存储矩阵就变为（64643，样本个数） 构建神经网络导入numpy1import numpy as np 初始化模型参数函数12345678910111213141516171819202122def initialization_parameters(layers): ''' 参数： layers 隐藏层列表，元素为对应层的隐藏单元个数 返回： params 存储W[i] 与b[i] 的字典 ''' params = &#123;&#125; # 遍历隐藏层 for i in range(1, len(layers)): # 为每一层初始化模型参数 params['W'+str(i)] = np.random.randn(layers[i], layers[i-1]) / np.sqrt(layers[i-1]) params['b'+str(i)] = np.zeros((layers[i], 1)) assert(params['W'+str(i)].shape == (layers[i], layers[i-1])) assert(params['b'+str(i)].shape == (layers[i], 1)) # 返回值字典 内容为[W1,b1,W2,b2,W3,b3 ..... WL,bL] return params 这里传入一个变量layers，用来保存隐藏层的信息，元素为各层的隐藏单元数目，不包括输入层。 例如：创建一个5层的神经网络，对应隐层分别为 5、5、4、4、1。 则 layers=[5,5,4,4,1] ，图如下 这样，len(layers)为神经网络的层数。leyers[i]为第i层的隐藏单元数目。 用params来存储各隐藏层的模型参数。 因为与无隐层、单隐层不同，这是一个有L个隐层的神经网络，所以使用遍历的方式来逐层初始化。 注： 这里生成随机数后不再 *0.01，该为 / np.sqrt(layers[i-1])，否则深层的话代价函数更新不下去。 定义正向传播函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import dnn_utils as dnndef forward(X, params, activations): ''' 参数： X 输入特征，即A[i-1] params 模型参数字典 activation 激活函数列表 返回： cache 存储A[i] 与 Z[i] 的字典 ''' # 定义激活函数字典，根据输入的activation来选择对应的激活函数 activation_function = &#123; \"relu\": dnn.relu, \"sigmoid\": dnn.sigmoid &#125; # 获取层数 L = len(activations) # cache['A0']初始化为输入特征X cache = &#123; 'A0': X &#125; # 遍历隐藏层进行正向传播 for i in range(1, L+1): # 获取当前层的W与Z W = params['W'+str(i)] b = params['b'+str(i)] #获取当前层的上一层的A，即当前层的输入值 A_pre = cache['A'+str(i-1)] # 获取当前层选择的激活函数 activation = activations[i-1] # 计算Z Z = np.dot(W, A_pre)+b # 使用dnn_utils中的激活函数来计算当前层的A cache['A'+str(i)], cache['Z'+str(i) ] = activation_function[activation](Z) # 断言 assert cache['Z'+str(i)].shape == Z.shape, 'error：维度错误' assert cache['A'+str(i)].shape == cache['Z'+str(i) ].shape, 'error: A与Z的维度不相等' return cache 导入dnn_utils来使用其中的激活函数。 传入的参数有一个activations，是一个列表，其元素为从第一个隐藏层开始的各层使用的激活函数，因为本次作业的dnn_utils文件只有ReLU与sigmoid激活函数，所以值为relu或sigmoid。 activation_function字典的value值为dnn_utils文件中的函数，这样后面执行activation = activations[i-1]后，只需使用activation_functionactivation来将Z传入激活函数即可 。 关于activation_function[activation]，假如是使用ReLU函数，那么activation=“relu”，结果为activation_function[“relu”]，即dnn.relu，后面加上(Z)，即组成dnn.relu(Z)，调用函数。 遍历隐藏层，根据激活函数列表来实现对应的激活函数。 定义代价函数123456789101112131415161718192021222324def compute_cost(AL,Y): \"\"\" 计算交叉熵代价函数 J=1/m * ∑损失函数 损失函数= -[Y*log(A2)+(1-Y)*log(1-A2)] 这里 ∑损失函数 可以用向量来表示。 参数： AL - 正向传播最后的输出结果，即损失函数公式中的y帽 Y - 标签 params - 初始化模型参数函数的字典返回值 返回： 代价 - 交叉熵成本给出方程（13） \"\"\" # 获取样本数 m = AL.shape[1] #计算代价 logprobs = np.multiply(np.log(AL), Y) + np.multiply((1 - Y), np.log(1 - AL)) cost = - np.sum(logprobs) / m cost = float(np.squeeze(cost)) return cost 用于计算每一次迭代的代价。 定义反向传播函数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def model_backward(X, Y, params, cache, activations): ''' 参数： Y 训练集标签 params 模型参数字典 cache A与Z的字典 activations 激活函数列表 返回： grads 存储dA[i] dW[i] db[i] ''' # 获取样本数 m = Y.shape[1] # 获取层数 L = len(activations) # 获取输出层的A （正向传播的输出） AL = cache['A'+str(L)]# AL = np.random.randn(1, 2) # 激活函数字典 activation_function = &#123; \"relu\": dnn.relu_backward, \"sigmoid\": dnn.sigmoid_backward &#125; cache['A0'] = X # 将cache['A0']定义为输入特征X # 初始化dAL grads = &#123; 'dA'+str(L): -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) &#125; # 反向传播从后往前计算，所以range(L,0,-1)，使用-1来实现区间(0,L]的反向遍历 for i in range(L, 0, -1): # 根据传入的激活函数列表选择对应的激活函数 activation = activation_function[activations[i-1]] # 获取当前层的W与Z W = params['W'+str(i)] Z = cache['Z'+str(i)] #获取当前层的上一层的A，即当前层的输入值 A_pre = cache['A'+str(i-1)] # 计算导数dA与dZ dA = grads['dA'+str(i)] dZ = activation(dA, Z) # 计算dW与db 并将dW、db、dA存到grads中 grads['dW'+str(i)] = np.dot(dZ, A_pre.T)/m grads['db'+str(i)] = np.sum(dZ, axis=1, keepdims=True)/m grads['dA'+str(i-1)] = np.dot(W.T, dZ) return grads 这里同样使用了activations，与正向不同的是这里是根据激活函数列表来选择对应的激活函数的求导函数。 其余关于activations的部分与正向传播相同。 定义更新参数函数1234567891011121314def update_params(params,grads,L,lr): ''' 参数： parameters 模型参数字典 grads 反向传播得到的导数字典 lr 学习率 L 神经网络的层数 返回值： params 更新后的模型参数字典 ''' for i in range(1,L+1): params['W'+str(i)] = params['W'+str(i)] - lr * grads['dW'+str(i)] params['b'+str(i)] = params['b'+str(i)] - lr * grads['db'+str(i)] return params 打包训练模型12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def nn_model(X, Y, layers, activations, lr, num_iterations, print_cost=False): ''' 深层神经网络模型 参数： X 训练集输入特征 Y 训练集标签 layers 神经网络隐藏层列表(不包含输入层)，元素为各隐藏层的隐藏单元数量 activations 激活函数列表，元素为对应层的激活函数（作业里只有relu和sigmoid） lr 学习率 num_iterations 迭代次数 seed 随机数种子 print_cost 是否输出代价，默认为否 返回： parameters 各层更新后的模型参数 字典 ''' nx = X.shape[0] # 获取输入特征的维数 ny = Y.shape[0] # 获取标签的维数 assert layers[-1] == ny, 'error：请确定输出层的单元数与标签的维数相等' L = len(layers) # 获取神经网络层数（隐藏层的层数） assert len(activations) == L, 'error: 请确定激活函数的个数与隐藏层的层数相等' layers.insert(0, nx) # 输入层的节点数 n0 print(\"初始化模型参数\") params = initialization_parameters(layers) # 梯度下降 for i in range(1,num_iterations+1): # 正向传播 cache = forward(X, params, activations) # 计算代价 AL = cache['A'+str(L)] cost = compute_cost(AL, Y) # 输出代价 if print_cost and i%100 ==0: print(\"第\",i,\"次迭代，当前代价为：\",cost) # 反向传播 grads = model_backward(X,Y, params, cache, activations) # 更新参数 params = update_params(params,grads,L,lr) return params 调用模型函数，传入训练集输入特征X、输出特征Y 与神经网络结构layers、激活函数列表、学习率、迭代次数即可获得训练完成后的模型参数W与b。 预测定义预测函数12345678910111213141516171819202122232425262728def predict(X,params,activations): ''' 预测函数，调用正向传播函数来得到Y帽，即预测值 参数： X 测试集输入特征 params 由神经网络模型得到的模型参数 activations 激活函数列表 返回： Y 预测的标签 ''' # 执行正向传播，params存储的为更新完成后的W与b cache = forward(X,params,activations) # 获取层数 L = len(activations) # 获取正向传播后最后的预测值 A = cache['A'+str(L)] # 初始化标签Y Y = np.zeros((1, X.shape[1])) # 遍历所有预测值，若预测大于0.5则标签为1，否则为0 for i in range(A.shape[1]): Y[0][i] = 1 if A[0][i] &gt; 0.5 else 0 return Y 接下来只需训练模型得到params，将测试集输入特征与params、激活函数列表传入预测函数即可得到预测的标签。 训练模型123456789101112# 设置随机数种子np.random.seed(1)# 定义神经网络结构layers = [20,7,5,1]# 定义各层使用的激活函数activations = ['relu','relu','relu','sigmoid']# 定义学习率lr = 0.0075# 定义迭代次数num_iterations = 2500# 执行模型函数，获得更新后的模型参数字典paramsparams = nn_model(train_set_x_flatten_normalization, train_set_y, layers, activations, lr, num_iterations, print_cost=True) 执行后结果如图： 进行预测1234# 预测训练集train_predict_y = predict(train_set_x_flatten_normalization, params, activations)# 预测测试集test_predict_y = predict(test_set_x_flatten_normalization, params, activations) 查看准确率12print(\"训练集准确率：\", format(100-np.mean(np.abs(train_predict_y-train_set_y))*100), \"%\")print(\"测试集准确率：\", format(100-np.mean(np.abs(test_predict_y-test_set_y))*100), \"%\") 可以看出训练集的准确率远高于测试集，明显有过拟合现象。","categories":[{"name":"神经网络","slug":"神经网络","permalink":"https://lluuiq.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"学习笔记","slug":"神经网络/学习笔记","permalink":"https://lluuiq.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"https://lluuiq.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]},{"title":"使用WebStack为Hexo博客添加网址导航页面","slug":"使用WebStack为Hexo博客添加网址导航页面","date":"2020-06-13T14:33:21.000Z","updated":"2020-07-04T01:15:56.877Z","comments":true,"path":"post/202006132233/","link":"","permalink":"https://lluuiq.com/post/202006132233/","excerpt":"因为经常把一些网站添加到浏览器的书签栏，目前已经多到眼花缭乱了，故想个办法为博客新建一个分页，把一些网址存在博客上，自己浏览器上存一部分。","text":"因为经常把一些网站添加到浏览器的书签栏，目前已经多到眼花缭乱了，故想个办法为博客新建一个分页，把一些网址存在博客上，自己浏览器上存一部分。 说明WebStack的github地址：WebStack 配置过程中有很多的坑， WebStack是个开源项目，在查阅使用方法时发现基本都是基于wordpress与typecho进行使用，但个人是用的Hexo，所以想在Hexo上实现它。 基于源代码 需要： 解析二级域名到服务器上 安装PHP≥7.2 在服务器上克隆源代码克隆地址随意。这里因为我用的是宝塔面板，所以放在了wwwroot下面，若不在这里，后面更改创建站点时的路径。 安装PHP并配置到宝塔面板的软件商店搜索PHP，然后安装≥7.2的版本即可。这里我安装的是7.4。 安装好后，点击设置 ，在禁用函数里把proc_open、passthru、putenv删除。这里是我删除过后又重新加回来的，所以看着顺序不一样，找到自己的删除即可。 然后在安装拓展里，安装fileinfo 创建网站站点 在 站点设置/网站目录 里面修改一下运行目录，改为public，然后保存 在伪静态里将规则改为laravel5，否则后台是404· 安装依赖包输入下面这条指令切换composer源为华为源。 1composer config -g repo.packagist composer https://mirrors.huaweicloud.com/repository/php 到克隆下来的WebStack-Laravel路径 1cd 你的WebStack-Laravel路径 然后执行以下语句安装composer 12rm -rf composer.lockcomposer install 第一次安装会提示失败，出现下图，提示Carbon版本过低。 打开WebStack-Laravel文件夹下的composer.json 然后在require代码块里插入以下代码： 注意插入后，原本的最后一句末尾加个逗号。 12\"kylekatarnls/laravel-carbon-2\": \"^1.0.0\",\"nesbot/carbon\": \"2.16.3 as 1.34.0\" 然后再次执行 12rm -rf composer.lockcomposer install 若出现如图所示信息就没问题了，下面连接数据库的信息不用管 编辑配置在WebStack-Laravel下执行 1cp .env.example .env 然后编辑 .env，需要修改的配置如下 123456APP_ENV=productionAPP_URL=站点二级域名（若没开启SSL则使用http）DB_DATABASE=数据库名DB_USERNAME=数据库用户名DB_PASSWORD=数据库密码 示例图如下： 生成KEY，执行 1php artisan key:generate 迁移数据，执行 1php artisan migrate:refresh --seed 其中提示输入的地方都输入yes即可。最后应该会报错，这时需要修改AppServiceProvider.php 修改WebStack-Laravel/app/providers/AppServiceProvider.php： 1234567891011121314151617181920212223242526272829303132333435363738&lt;?php namespace App\\Providers; use App\\Observers\\SiteObserver;use App\\Site;use Encore\\Admin\\Config\\Config;use Illuminate\\Support\\Facades\\Schema;use Illuminate\\Support\\ServiceProvider; class AppServiceProvider extends ServiceProvider&#123; /** * Bootstrap any application services. * * @return void */ public function boot() &#123; Schema::defaultStringLength(191); Site::observe(SiteObserver::class); $table = config('admin.extensions.config.table', 'admin_config'); if (Schema::hasTable($table)) &#123; Config::load(); &#125; &#125; /** * Register any application services. * * @return void */ public function register() &#123; // &#125;&#125; 然后再次执行 12php artisan key:generatephp artisan migrate:refresh --seed 这里我出现了一个问题： 提示我admin_config已存在，应该是之前执行php artisan migrate:refresh --seed时导致的 或是在创建站点时初始化导致的。 故在宝塔面板删除当前的数据库，然后重新创建了一个。 下图为重新创建的，这里貌似宝塔要求数据库名与用户名要相同。 当然这里的数据库名、用户名、密码对应着上述修改 .env文件的内容。 创建好后点击右边的工具确认没有admin_congfig，正常来讲应该是全空的。 然后执行 12php artisan key:generatephp artisan migrate:refresh --seed 为站点赋予权限打开网站进行查看以及后面一些操作时，会经常提示报错信息，都是XXX拒绝访问等等。 故直接在宝塔上赋予站点目录所有权限。 效果 后台管理在主页的url后面加上/admin即可进入。 默认的用户名与密码都是admin，进去后可以修改 清除原有站点（可选）执行下方语句进行清除。 1php artisan webstack:clean","categories":[{"name":"笔记","slug":"笔记","permalink":"https://lluuiq.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://lluuiq.com/tags/hexo/"},{"name":"WebStack","slug":"WebStack","permalink":"https://lluuiq.com/tags/WebStack/"}]},{"title":"吴恩达深度学习编程作业1-3","slug":"吴恩达深度学习编程作业1-3","date":"2020-06-11T14:33:21.000Z","updated":"2020-06-24T22:12:52.678Z","comments":true,"path":"post/202006112233/","link":"","permalink":"https://lluuiq.com/post/202006112233/","excerpt":"吴恩达深度学习课程《神经网络和深度学习》第三周（浅层神经网络）的编程作业。","text":"吴恩达深度学习课程《神经网络和深度学习》第三周（浅层神经网络）的编程作业。 说明感谢作者阿宽提供作业题目【中文】【吴恩达课后编程作业】Course 1 - 神经网络和深度学习 - 第三周作业 数据集以及题目可在该博文中查看与下载。 题目为建立 单隐藏层的神经网络 对平面数据进行分类。 下载的文件中： testCases 提供一些测试案例来评估正确性，可打开该文件来查看一些测试用例（我这里就不写测试函数了，原博文中有编写）。 planar_utils 提供在该作业中需要使用的一些功能。 需要的库： numpy 数据科学计算的库 matplotlib 绘图 sklearn 机器学习工具包 导包12345678910111213import numpy as npimport matplotlib.pyplot as pltfrom testCases import *import sklearnimport sklearn.datasetsimport sklearn.linear_modelfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasetsplt.rcParams['axes.unicode_minus']=Falseplt.rcParams['font.family']='SimHei'%matplotlib inline# 设置一个随机数种子，与原博文保持一致np.random.seed(1) 加载数据集查看一下X与Y的维度和类型 123X,Y=load_planar_dataset()print(\"X的维度为：\",X.shape,\"X的类型为：\",type(X))print(\"Y的维度为：\",Y.shape,\"Y的类型为：\",type(Y)) 结果如图： 可知一共有400个样本。其中X的维度中的2为对应的平面图的X轴、Y轴坐标。 Y的取值为0或1，为样本的标签。 将数据集可视化1plt.scatter(X[0, :], X[1, :], c=np.squeeze(Y), s=40, cmap=plt.cm.Spectral) 结果如图： 其中y=0的标签在图中为红色点。 y=1的标签在图中为蓝色点。 查看逻辑回归的准确率123456789# 使用sklearn库建立逻辑回归模型clf = sklearn.linear_model.LogisticRegressionCV()clf.fit(X.T,Y.T)plot_decision_boundary(lambda x: clf.predict(x), X, np.squeeze(Y)) #绘制决策边界plt.title(\"逻辑回归\") #图标题LR_predictions = clf.predict(X.T) #预测结果print (\"逻辑回归的准确性：\",float((np.dot(Y, LR_predictions) + np.dot(1 - Y,1 - LR_predictions)) / float(Y.size) * 100), \"%\") 结果如图： 47%表示的为预测正确的所占百分比。 由结果图可知，逻辑回归的线性决策边界并不能很好的区分这些数据，故正确率仅有47%。 构建单隐藏层神经网络流程： 定义神经网络结构 初始化模型参数 进行梯度下降（正向、反向传播、计算代价函数、更新参数） 其中正向传播与反向传播的公式： 正向传播（左），反向传播（右） 可以把这些功能整合到一个模型函数中。 接下来先根据步骤来逐一编写实现代码。 定义神经网络结构在这一步定义好神经网络的输入层单元的数量、隐藏层单元的数量、输出层单元的数量。 1234567891011121314151617def layer_sizes(X , Y): \"\"\" 参数： X - 输入特征,维度为（输入特征的数量，样本数） Y - 标签，维度为（输出的数量，样本数） 返回： n_x - 输入层单元的数量 n_h - 隐藏层单元的数量 n_y - 输出层单元的数量 \"\"\" n_x = X.shape[0] # 手动定义为4个隐藏单元 n_h = 4 n_y = Y.shape[0] return (n_x,n_h,n_y) 定义模型参数初始化函数 定义了随机数种子为2，确保结果一致。 不像第二周作业一样用0初始化，改为随机数初始化，不然梯度下降失去作用。 1234567891011121314151617181920212223242526272829303132def initialize_parameters( n_x , n_h ,n_y): \"\"\" 参数： n_x - 输入层单元的数量 n_h - 隐藏层单元的数量 n_y - 输出层单元的数量 返回： params - 包含参数的字典： W1 - 权重矩阵,维度为（n_h，n_x） b1 - 偏向量，维度为（n_h，1） W2 - 权重矩阵，维度为（n_y，n_h） b2 - 偏向量，维度为（n_y，1） \"\"\" # 定义随机数种子，确保结果一致 np.random.seed(2) # 使用随机数初始化 W1 = np.random.randn(n_h,n_x) * 0.01 b1 = np.zeros(shape=(n_h, 1)) W2 = np.random.randn(n_y,n_h) * 0.01 b2 = np.zeros(shape=(n_y, 1)) parameters = &#123; \"W1\" : W1, \"b1\" : b1, \"W2\" : W2, \"b2\" : b2 &#125; return parameters 定义正向传播函数 这里将初始化函数的返回值（字典作为传入参数） 隐藏层使用的激活函数是tanh。输出层使用sigmoid，因为想要输出结果的取值范围在[0,1]·。 12345678910111213141516171819202122232425262728def forward_propagation( X , parameters ): \"\"\" 参数： X - 维度为（n_x，样本数）的输入特征。 parameters - 初始化函数（initialization_parameters）的输出 返回： A2 - 使用sigmoid()函数计算的第二次激活后的数值 cache - 包含“Z1”，“A1”，“Z2”和“A2”的字典类型变量 \"\"\" # 读取初始化后的参数 W1 = parameters[\"W1\"] b1 = parameters[\"b1\"] W2 = parameters[\"W2\"] b2 = parameters[\"b2\"] # 正向传播，最终输出A2 Z1 = np.dot(W1 , X) + b1 A1 = np.tanh(Z1) # 隐藏层使用numpy自带的tanh函数 Z2 = np.dot(W2 , A1) + b2 A2 = sigmoid(Z2) # 输出层使用sigmoid激活函数 cache = &#123;\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2&#125; return (A2, cache) 定义计算代价函数的函数12345678910111213141516171819202122232425262728def compute_cost(A2,Y,parameters): \"\"\" 计算交叉熵代价函数 J=1/m * ∑损失函数 损失函数= -[Y*log(A2)+(1-Y)*log(1-A2)] 这里 ∑损失函数 可以用向量来表示。 参数： A2 - 正向传播最后的输出结果，即损失函数公式中的y帽 Y - 标签 parameters - 初始化模型参数函数的字典返回值 返回： 代价 - 交叉熵成本给出方程（13） \"\"\" # 获取样本数 m = X.shape[1] # 获取模型参数W1与W2 W1 = parameters[\"W1\"] W2 = parameters[\"W2\"] #计算代价 logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2)) cost = - np.sum(logprobs) / m cost = float(np.squeeze(cost)) return cost 定义反向传播函数这里需要注意的是tanh函数 A=g(Z) 的导数为 1−A2 12345678910111213141516171819202122232425262728293031323334353637def backward_propagation(parameters,cache,X,Y): \"\"\" 参数： parameters - 包含我们的参数的一个字典类型的变量。 cache - 包含“Z1”，“A1”，“Z2”和“A2”的字典类型的变量。 X - 输入数据，维度为（2，数量） Y - “True”标签，维度为（1，数量） 返回： grads - 包含W和b的导数一个字典类型的变量。 \"\"\" # 获取样本数 m = X.shape[1] # 获取模型参数W1与w2 W1 = parameters[\"W1\"] W2 = parameters[\"W2\"] # 获取激活函数的输出A1与A2 A1 = cache[\"A1\"] A2 = cache[\"A2\"] # 计算导数 dZ2= A2 - Y dW2 = (1 / m) * np.dot(dZ2, A1.T) db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True) dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2)) dW1 = (1 / m) * np.dot(dZ1, X.T) db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True) grads = &#123; \"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2 &#125; return grads 定义更新参数的函数12345678910111213141516171819202122232425262728293031def update_params(parameters,grads,lr=1.2): \"\"\" 参数： parameters - 包含模型参数的字典类型的变量。 grads - 包含导数的字典类型的变量。 lr(learning rate) - 学习速率 返回： parameters - 更新后的参数的 字典类型的变量。 \"\"\" # 读取原模型参数 W1,W2 = parameters[\"W1\"],parameters[\"W2\"] b1,b2 = parameters[\"b1\"],parameters[\"b2\"] # 读取模型参数的导数 dW1,dW2 = grads[\"dW1\"],grads[\"dW2\"] db1,db2 = grads[\"db1\"],grads[\"db2\"] # 更新参数 W1 = W1 - lr * dW1 b1 = b1 - lr * db1 W2 = W2 - lr * dW2 b2 = b2 - lr * db2 parameters = &#123; \"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2 &#125; return parameters 打包成模型函数12345678910111213141516171819202122232425262728293031323334353637def nn_model(X,Y,n_h,num_iterations,print_cost=False): \"\"\" 参数： X - 输入特征,维度为（2，样本数） Y - 标签，维度为（1，样本数） n_h - 隐藏层的数量 num_iterations - 梯度下降的迭代次数 print_cost - 如果为True，则每1000次迭代打印一次成本数值 返回： parameters - 模型的参数，用于预测。 \"\"\" np.random.seed(3) #指定随机种子 # 调用设置网络结构函数 n_x = layer_sizes(X, Y)[0] n_y = layer_sizes(X, Y)[2] # parameters = initialize_parameters(n_x,n_h,n_y) W1 = parameters [\"W1\"] b1 = parameters [\"b1\"] W2 = parameters [\"W2\"] b2 = parameters [\"b2\"] for i in range(num_iterations): A2 , cache = forward_propagation(X,parameters ) cost = compute_cost(A2,Y,parameters ) grads = backward_propagation(parameters ,cache,X,Y) parameters = update_params(parameters ,grads,lr = 0.5) if print_cost: if i%1000 == 0: print(\"第 \",i,\" 次循环，成本为：\"+str(cost)) return parameters 出现的问题：根据【中文】【吴恩达课后编程作业】Course 1 - 神经网络和深度学习 - 第三周作业的步骤，在测试模型代码时发现与原文不同。 这是我的结果： 这是原文的结果： 前面的测试结果都一致，这里以为是自己哪一步出了问题导致在打包好的模型函数里调用错误，以至于把代码全改回和原文一致，结果还是不一样。 最后看评论和与同学进行讨论发现都存在此问题，但用在训练集与测试集时是正常的。 推测原因应该是此时 numpy相同的随机数种子，生成的随机数却不一样导致的吧。所以现在生成的随机数用在该段代码中会出现错误，如结果提示中的除数为0。 进行预测编写预测函数123456789101112131415def predict(parameters,X): \"\"\" 参数： parameters - 包含模型参数的字典类型的变量。 X - 输入特征（n_x，m） 返回: predictions - 预测的向量（红色：0 /蓝色：1） \"\"\" # 计算A2 A2 , cache = forward_propagation(X,parameters) # np.round 求平均 predictions = np.round(A2) return predictions 查看准确率根据博主最后提出的更改隐藏单元数问题，这里设置隐藏单元为5准确率最高，但出于对比，还是设置为了4。 12345678parameters = nn_model(X, Y, n_h = 4, num_iterations=10000, print_cost=True)#绘制边界plot_decision_boundary(lambda x: predict(parameters, x.T), X, np.squeeze(Y))plt.title(\"隐藏单元数为\" + str(4)+\"时的决策边界\")predictions = predict(parameters, X)print ('准确率:', float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100),'%') 结果：","categories":[{"name":"神经网络","slug":"神经网络","permalink":"https://lluuiq.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"学习笔记","slug":"神经网络/学习笔记","permalink":"https://lluuiq.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"https://lluuiq.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]},{"title":"吴恩达深度学习编程作业1-2","slug":"吴恩达深度学习编程作业1-2","date":"2020-06-08T14:33:21.000Z","updated":"2020-06-26T04:36:09.096Z","comments":true,"path":"post/202006082233/","link":"","permalink":"https://lluuiq.com/post/202006082233/","excerpt":"吴恩达深度学习课程《神经网络和深 度学习》第二周（神经网络基础）的编程作业。","text":"吴恩达深度学习课程《神经网络和深 度学习》第二周（神经网络基础）的编程作业。 说明感谢作者阿宽提供作业题目【中文】【吴恩达课后编程作业】Course 1 - 神经网络和深度学习 - 第二周作业 数据集以及题目可在该博文中查看与下载。 同时参考了作者iSunwish的博文深度学习（三）实战：动手实现猫图识别 题目是建立神经网络来通过图片识别猫。虽然说是通过图片来识别，但给出的数据集已经将图片转为矩阵数据存为h5文件。所以读取出来的数据即可直接用来操作，不用进行处理图片的操作。 lr_utils.py文件已经实现了对数据的读取，我们要做的就是在代码中执行该文件中的函数来获取返回值即可。 将下载好的数据集与lr_utils.py 放在与自己的代码相同的文件夹中。 需要用到的库： numpy 数据科学计算的库 matplotlib 绘图 h5py 用于交互H5文件（作业的数据集是h5文件） scikit-image 用于图像处理 查看自带的lr_utils.py文件lr_utils.py的代码如下，中文注释为自己添加的说明 1234567891011121314151617181920212223242526import numpy as npimport h5py def load_dataset(): # 读取训练集数据 train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\") # train_set_x_orig 存储训练集的特征 train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features # train_set_y_orig 存储训练集的标签 train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels # 读取测试集数据 test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\") # test_set_x_orig 存储测试集的特征 test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features # test_set_y_orig 存储测试集的标签 test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels # 用classes存储其是猫还是不是猫的二进制字符串（感觉没什么意义？） classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0])) test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0])) return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes 读取数据1234# 加载 lr_utils的load_dataset函数from lr_utils import load_dataset# 定义变量来获取load_dataset函数的返回值train_set_x , train_set_y , test_set_x , test_set_y , classes = load_dataset() 执行上述代码后，可以输出变量的shape来查看维度 12345print(\"训练集特征的维度：\",train_set_x.shape,\"训练集特征的类型\",type(train_set_x))print(\"训练集标签的维度：\",train_set_y.shape,\"训练集标签的类型\",type(train_set_y))print(\"测试集特征的维度：\",test_set_x.shape,\"测试集特征的类型\",type(test_set_x))print(\"测试集标签的维度：\",test_set_y.shape,\"测试集标签的类型\",type(test_set_y))print(\"classes的维度：\",classes.shape,\"classes的类型\",type(classes)) 结果： 可以看到： 所有变量都是ndarray类型（数组），用数组来表示的矩阵。 训练集特征一共有209条数据，都是64*64的矩阵（即64x64像素的图片），后面的3表示为图像的RGB三通道（红绿蓝）。 训练集标签是一维数组，有209个数据。 测试集特征一共有50条数据，其余与训练集相同。 测试集标签同训练集标签是一维数组，有50条数据。 classes的shape为(2,)，其是一个一维数组，有两个数据。 输出一下classes，即可看到其数据 1print(classes) 结果为： 其是一个二进制的值，值为 ‘non-cat’ 或 ’cat’ 。 若有兴趣也可以输出训练、测试集的特征与标签变量来查看具体的值。 使用 matplotlib 查看图片（可选）吴恩达教授说过，人类擅长处理非结构化数据，而机器擅长处理结构化数据。通过一堆数字我们不能判断这一个样本是一张什么样的图片，故可以使用matplotlib来通过数据绘出原本的图片。 目前所知，train_set_x里存储的都是图片在计算机上的表示，一共有209个样本。我们可以使用 matplotlib 库的imshow函数来直接绘制图片。这里我使用numpy来生成一个随机数进行随机查看。 1234567import matplotlib.pyplot as pltimport numpy as npnum = np.random.randint(0,209)print(\"第\",num,\"个样本的图片为：\")plt.imshow(train_set_x[num])print(\"其标签为\",np.squeeze(train_set_y)[num],\"是一张\",classes[np.squeeze(train_set_y)[num]].decode(\"utf-8\"),\"图片\") 多次运行后挑选两个结果： ) 注意： 使用numpy的randint函数来生成一个范围为 [0,209) 的一个随机数，传入的范围是左闭右开区间，且因为是数组所以要从0开始，下标最大值为208，所以应该传入的范围是 0与209。 numpy的squeeze函数来压缩数组，它会将数组中单维度的条目去掉，进行压缩。比如[[1],[2],[3]] 这是一个维度为(1,3)的数组，压缩后会将无意义的维度去掉，转为[1,2,3]的一维数组。 classes[X]，指定classes中的值后，要加个decode(“utf-8”)来将原本的二进制数据转为utf-8来输出。否则输出结果会变成 b’cat’ 或者 b’non-cat’ 。（当然仅仅是个人查看的话可以不加） 数据处理转换图片的存储方式图片在计算机中的存储方式为 像素3 ，而原本的特征矩阵的维度显然不符合该标准，故应该转为维度为（6464*3，209或50）的数组。（训练集为209，测试集为50） 使用numpy的reshape与 .T 来进行处理。 12train_set_x_flatten = train_set_x.reshape(train_set_x.shape[0],-1).Ttest_set_x_flatten = test_set_x.reshape(test_set_x.shape[0],-1).T 以训练集举例： train_set_x.shape[0]出第一个 维数，即样本个数。 reshap(train_set_x.shape[0],-1)指定行数为样本个数，列数设置为-1让numpy自行计算像素与三色道的组合 。最终转为行数为样本个数，多维数组每一行都是 64643的图片矩阵 的一个二维数组。 红框内即图片矩阵，然后一共有209个图片矩阵。此时维度为（209，64643） 最后通过.T来进行转置，变为维度为（64643，209）的矩阵。然后存储在新变量中。 输出一下看看维度 12print(\"转换后的训练集维度：\",train_set_x_flatten.shape)print(\"转换后的测试集维度：\",test_set_x_flatten.shape) 结果为： 转换数据取值范围对数值进行归一化 ，将其取值范围定为[0,1]之间。图片数据的处理直接除以255即可。 12train_set_x_flatten_normalization = train_set_x_flatten/255test_set_x_flatten_normalization = test_set_x_flatten/255 构建无隐藏层的神经网络流程： 准备好输入特征 初始化模型参数 通过正向传播、反向传播进行梯度下降 导入numpy库1import numpy as np 若在[使用 matplotlib 查看图片（可选）](# 使用 matplotlib 查看图片（可选）)中已经导入过numpy库，则直接执行下一步即可。 定义sigmoid函数在吴恩达深度学习的课程中，该作业属于第二周 神经网络基础，故使用的是sigmoid函数，暂时未使用tanh与ReLU函数。 123def sigmoid(z): a = 1 / (1 + np.exp(-z)) return a 定义初始化模型参数函数模型参数w要与输入特征的维度对应。即w的维度应该为（输入特征x的特征数n_x，1） 模型参数b的维度应该为 （1，输入特征x的样本数m），不过b可以直接赋值0，利用python广播来计算。 输入特征x的特征数n_x可以通过 输入特征向量.shape[0]来获得。 输入特征x的样本数m可以通过 输入特征向量.shape[1]来获得。 123456def initialization_parameters(nx):# # 随机初始化# w = np.random.rand(nx,1)*0.01 w = np.zeros(shape = (nx,1)) b = 0 return (w , b) 这里其实可以用在第三周（浅层神经网络）中的课程 随机初始化中提到的知识点，直接将w初始为随机数。 但由于没有隐藏层，或者说只有一个输出单元，所以初始化为0或者随机数都一样。 为了方便与别人的作业观测差距，故这里还是使用了初始化为0。 关于梯度下降的说明先看下伪代码： 12345678for i in range(迭代次数): Z=np.dot(w.T,X)+b A=sigmoid(Z) dz=A-Y dw=(1/m)*np.dot(X,dz.T) db=np.sum(dz)/m w = w - α*dw b = b - α*db 目前已知： 输入特征X 初始化的参数w与b 激活函数sigmoid 输入特征X的个数 m 可以计算得出： dz dw db 未知： 学习率α（需要自定义） 需要添加： 代价函数（用于判断是否最小化） 这里可以在每次梯度下降时都存储一次代价函数的值，用于后面绘图来观测代价函数的变化情况。 定义梯度下降函数关于详细的解释放在了代码中，逐一拿出来说明容易混乱。 12345678910111213141516171819202122232425262728293031323334353637# 传入参数为：模型参数w、b，训练集的特征、标签，学习率，迭代次数def gradient_descent(w, b, X, Y, alpha, iterations_num): # 获取输入特征个数 m = X.shape[1] X = X.reshape(X.shape[0], m) cost_list = [] for i in range(iterations_num): # **正向传播** Z = np.dot(w.T, X)+b # 计算Z A = sigmoid(Z) # 计算激活函数值 cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A))) # 计算代价函数 # **正向end** # **反向传播** dz = A-Y # 计算dz dw = (1/m)*np.dot(X, dz.T) # 计算dw db = np.sum(dz)/m # 计算db # **反向end** # 更新参数 w = w-alpha*dw b = b-alpha*db # 每迭代100次存一次代价函数值，并且输出当前的迭代次数与代价函数值 if(i % 100 == 0): cost_list.append(cost) print(\"迭代次数：\", i, \"，代价：\", cost) # 记录参数，作返回值 param = &#123; \"costs\": cost_list, \"w\": w, \"b\": b, \"alpha\": alpha &#125; return param 执行梯度下降说明一下，训练集特征的维度此时为(12288,209) 课程中表示为 （n_x，m），模型参数w的维度应该为(12288,1) ，这样在计算假设函数Z时，w.T的维度为(1,12288)，符合定义，且能够与 X进行矩阵内积。最后Z的维度为(1,209)，即课程中的 1xm。 首先获取训练集特征的特征数（非样本数），用nx来存储。 初始化模型参数w与b。 进行梯度下降，并用param来接收最后结果的返回值 1234567# 通过.shape[0]获得训练集输入特征x的特征数nx = train_set_x_flatten_normalization.shape[0]# 初始化模型参数w与bw, b = initialization_parameters(nx)# 执行梯度下降，并用param来获取返回值param = gradient_descent(w, b, train_set_x_flatten_normalization, train_set_y, alpha=0.005, iterations_num=2000) alpha为学习率，这是一个自定义的值，需要用户手动选择来输入，目前无法直接确定最优的学习率，现在先通过人为的调试与观察来选择较优的学习率，关于更好的方法日后再讨论。 iterations_num为迭代次数，也是用户手动输入，但基本有一个阈值，达到迭代次数后，代价函数值就基本不会改变（已最小化或接近最小化）。 结果： 进行预测定义预测函数传入的参数为由梯度下降得到的模型参数w、b，以及进行预测用的输入特征。 首先初始化向量Y，维度为（1，输入特征的样本数m），用于存放预测结果。 走一遍逻辑回归模型（计算Z、激活函数A），最后A中的结果即为预测概率。 如果概率大于0.5，则Y的对应值置为1（表示是猫），否则置为0（表示不是猫）。 1234567def predict(w, b, X): Y = np.zeros((1, X.shape[1])) Z = np.dot(w.T, X)+b A = sigmoid(Z) for i in range(A.shape[1]): Y[0][i] = 1 if A[0][i] &gt; 0.5 else 0 return Y 对训练集与测试集进行预测通过param[‘w’]与param[‘b’]即可获得梯度下降结果的模型参数w、b，并将要进行预测的输入特征传入预测函数中。 1234# 对训练集进行预测train_predict_y = predict(param['w'], param['b'], train_set_x_flatten_normalization)# 对测试集进行预测test_predict_y = predict(param['w'], param['b'],test_set_x_flatten_normalization) 查看准确率采用的准确率公式 ： 预测的标签值原数据集正确的标签值的绝对值样本个数100−[(预测的标签值−原数据集正确的标签值)的绝对值样本个数]∗100 12print(\"训练集准确率：\", format(100-np.mean(np.abs(train_predict_y-train_set_y))*100), \"%\")print(\"测试集准确率：\", format(100-np.mean(np.abs(test_predict_y-test_set_y))*100), \"%\") 结果： 观测代价函数变化情况使用matplotlib进行绘图，在之前观察图片时已经导入过matplotlib库。 12345678910111213141516# 设置字体，使得能够显示中文plt.rcParams['font.family']='SimHei'# 设置字体大小fontsize=13# 传入代价函数数组plt.plot(param['costs'])# 设置y轴标题与y轴字体大小plt.ylabel('代价函数值',fontsize=fontsize)plt.yticks(fontsize=fontsize)# 设置x轴标题与x轴字体大小plt.xlabel('迭代次数（单位：百）',fontsize=fontsize)plt.xticks(fontsize=fontsize)# 设置图片标题plt.title(\"学习率：\" + str(param['alpha']),fontsize=fontsize)plt.show() 结果： 打包模型定义一个函数，用于传入参数后进行构建神经网络并直接返回结果 1234567891011121314151617181920def model(train_set_x,train_set_y,test_set_x,test_set_y,alpha,iterations_num): nx = train_set_x_flatten_normalization.shape[0] w, b = initialization_parameters(nx) param = gradient_descent(w, b, train_set_x, train_set_y, alpha=alpha, iterations_num=iterations_num) train_predict_y = predict(param['w'], param['b'], train_set_x) test_predict_y = predict(param['w'], param['b'],test_set_x) print(\"训练集准确率：\", format(100-np.mean(np.abs(train_predict_y-train_set_y))*100), \"%\") print(\"测试集准确率：\", format(100-np.mean(np.abs(test_predict_y-test_set_y))*100), \"%\") result=&#123; 'costs':param['costs'], 'w':param['w'], 'b':param['b'], 'alpha':param['alpha'], 'train_predict_y':train_predict_y, 'test_predict_y':test_predict_y, 'iterations_num':param['iterations_num'] &#125; return result 将上述流程集合进该函数中，这样只需要传入归一化后的训练集测试集的输入特征、以及训练集测试集的标签，再给定学习率与迭代次数，执行函数后，会输出相应迭代次数的代价（可以在梯度下降函数中将输出代价的代码去掉，简化输出内容），并且返回字典数据类型来存储模型中的一些值。 如果要进行预测，则执行[定义预测函数](# 定义预测函数)中的预测函数，参数为建模后的w、b，以及要进行预测的数据集的输入特征即可，返回值为预测的标签数组。 寻找较优学习率（待补充）pass 上传图片进行测试安装scikit-image库三种安装库的方法。 windows终端命令： 1pip install scikit-image conda终端命令： 1conda install scikit-image 使用Anaconda Navigator进行安装： 安装好后，从skimage中导入io、transform与img_as_ubyte io：读取与保存图片 transform：使用resize转换图片像素 img_as_ubyte：转换图片数据类型 1from skimage import io,transform,img_as_ubyte 准备图片首先准备好一张图片放在代码文件的同目录下（不在同一个目录也可以，修改path即可） 以名为cat.jpg的下图为例（百度“猫”，结果第一张） 其像素可以鼠标悬浮在文件上查看，1056*1065 然后使用plt的imread函数读取图片。 123path = 'cat.jpg'img = io.imread(path)print(\"读取图片的维度为：\",img.shape,\"\\n输出图片：\\n\",img) 结果为： 可以看到其存储方式为 (像素，3)，3即三色道矩阵。 转换图片像素并归一化使用transform.resize来转换图片的像素，使其与训练集的图片像素一致。 transform.resize会自动对图片的值进行归一化。 需要注意的是transform.resize会将图片的值的数据类型转为float，故使用img_as_ubyte函数转回uint8 123px = train_set_x.shape[1]image = transform.resize(img,(px,px))image = img_as_ubyte(image) 转为对应输入特征x的存储方式。12X_flatten = img.reshape(1,-1).Tprint(X_flatten.shape) 即 （64643，1），这样就与训练集的输入特征x的存储方式一致了。 对上传图片进行预测假若已经建立好模型（得到模型变量model），则直接执行[进行预测](# 进行预测)中的流程即可。 需要注意的是返回值Y是一个维度为（1，m）的向量。m为输入特征的样本数，故查看结果使用Y[0][0]即可。 若是多张图片则需要遍历查看对应的预测值，或直接输出Y来观察预测值。 123Y = predict(model['w'],model['b'],X)result = int(Y[0][0])print(\"预测值为：\",result,\"\\n这是一张 \",classes[result].decode('utf-8'),\" 图片\") classes这个返回值我终于用上了 预测结果 打包预测上传图片代码12345678910111213from skimage import io,transform,img_as_ubytedef predict_cat(path): img = io.imread(path) px = train_set_x.shape[1] image = transform.resize(img,(px,px)) image = img_as_ubyte(image)/255 X_flatten = image.reshape(1,-1).T Y = predict(model['w'],model['b'],X_flatten) result = int(Y[0][0]) print(\"预测值为：\",result,\"\\n这是一张 \",classes[result].decode('utf-8'),\" 图片\") return Y 总结准备了两张猫的图片，两张狗的图片，一张马，一张豹子，一张白菜，一张纯黑色的图，一张纯白色的图。 结果预测为猫的有一张猫的图片，豹子、纯黑与纯白的图。 准确率还是不够高，后尝试通过更改学习率进行优化。","categories":[{"name":"神经网络","slug":"神经网络","permalink":"https://lluuiq.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"学习笔记","slug":"神经网络/学习笔记","permalink":"https://lluuiq.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"https://lluuiq.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]},{"title":"Valine配置邮件与QQ提醒","slug":"Valine配置邮件与QQ提醒","date":"2020-05-29T14:33:21.000Z","updated":"2020-07-04T01:15:37.808Z","comments":true,"path":"post/202005292233/","link":"","permalink":"https://lluuiq.com/post/202005292233/","excerpt":"valine在配置好后没有邮件提醒功能，需要到LeanCloud登陆控制台进行一系列设置。 最开始是只想配置个邮件提醒，但直到看到了这位大佬的博文 Valine评论之Valine-admin配置攻略 该版本基于Valine-admin的二次开发，加入了微信提醒、QQ提醒功能。 因为个人常用QQ，微信很少登陆，故只配置了QQ提醒功能。微信提醒的配置方法与QQ类似。 QQ消息推送机器人：Qmsg酱 本文基于上述配置攻略博文，详细记录自己的配置过程。请先确定自己已经配置好了基础的valine评论服务。","text":"valine在配置好后没有邮件提醒功能，需要到LeanCloud登陆控制台进行一系列设置。 最开始是只想配置个邮件提醒，但直到看到了这位大佬的博文 Valine评论之Valine-admin配置攻略 该版本基于Valine-admin的二次开发，加入了微信提醒、QQ提醒功能。 因为个人常用QQ，微信很少登陆，故只配置了QQ提醒功能。微信提醒的配置方法与QQ类似。 QQ消息推送机器人：Qmsg酱 本文基于上述配置攻略博文，详细记录自己的配置过程。请先确定自己已经配置好了基础的valine评论服务。 获取Qmsg酱的key进入官网Qmsg酱，使用QQ号一键登录即可。 在首页如下 添加完成后，打开你添加的QQ号，添加机器人为好友 选了哪个就加哪个为好友，添加好友后是自动通过的，不加好友怎么收到提醒呢。 接下来到右上角点击文档， 复制接口地址后面的key值， 即send/ 后面的所有字符，可以暂时将该key记录在笔记本等位置，稍后配置要用。 配置valine实例云引擎部署Leancloud官网，进入后登陆到自己的控制台。进入自己创建的应用。然后进入云引擎/部署 大佬二次开发的valine-admin-server的github地址：https://github.com/sviptzk/Valine-Admin-Server 在下面的部署中的Deploy from中填入该地址。 注：若是打算自己定义邮件通知的模板或者QQ提醒的模板的，fork一份到自己的github来修改。 这样填入的地址使用fork到自己github中的地址，后面修改模板时修改代码即可。 填入后点击部署。出现如下所示的log信息（最后提示1个实例部署成功）即可。 设置变量进入到云引擎/设置中 在页面中添加新变量 首先添加几个基本变量： 括号中说明根据使用情况来选填，没有括号的为必填项。 变量名 变量值 变量值举例 ADMIN_URL 博客网址 https://lluuiq.com QMSG_KEY （使用QQ提醒必填）之前获得的Qmsg的key 7b04XXXXXXXXfc5 QQ （使用QQ提醒必填）Qmsg中添加的自己的QQ号 844520941 QQ_SHAKE （想在提醒时被戳一下就设置）是否开启QQ提醒时戳一戳 true SENDER_NAME 发件人昵称 lluuiq SITE_NAME 博客网站名称 lluuiq’s Blog SITE_URL 网站地址 https://lluuiq.com SMTP_SERVICE SMTP服务器提供商 163 SMTP_PASS SMTP授权码 CSTXXXXXXXOUC SMTP_USER 用于发邮件的邮箱，需要与SMTP服务商对应 lluuiq@163.com TEMPLATE_NAME 邮件模板 custom2 TO_EMAIL （在valine评论填邮箱的情况下好像没什么用，但是可以加上用于后面自定义邮件模板时使用）博主的收件邮箱 mail@lluuiq.com 一些说明： ADMIN_URL：开启网页后台管理功能才需要添加的变量，若不需要通过网页来管理的话可以不加，因为可以登陆LeanCloud来进行管理，开启方法后面介绍。 QQ：自己接收消息的QQ号，要存在之前获取key时添加的QQ列表中 SMTP_SERVICE：这是valine支持的一些服务商：Supported，选择对应的服务商后，要更改对应的发件邮箱与授权码，关于授权码的获取方法各个服务商的直接百度即可，后面我以网易163的举例。 关于为什么使用网易，我使用QQ邮箱结果发件一直550 mail content denied，被腾讯服务器当成垃圾广告邮件给退回，故换成了163邮箱。想使用QQ邮箱来接收消息的话，可以设置163邮箱的自动转发。 SMTP_PASS：163为例的获取方法，点击获取SMTP授权码，并设置自动转发（以163举例）跳转到下方查看。 TEMPLATE_NAME：valine自带的主题有默认default、彩虹rainbow，该版本新增的有custom1、custom2，其中custom2的样式如下： 当然，如果是fork的主题的话，这个是可以自行修改的，后面再讲自己的修改过程。 注意事项： 我没有配置垃圾过滤、微信提醒。以及并没有使用自定义服务商（我配置的总是失败，想使用自定义服务商的可以参考原博文进行配置 ）。 每次修改变量后，点击保存后是不会生效的。需要重新部署一次云引擎（回到云引擎部署，不用重新填github地址，直接点击部署即可）。 配置完成后并且重新部署后，即可尝试在valine评论里发送一条评论来进行尝试。 博主收到的效果： 访客收到的效果： 这是默认的模板，若需要修改的话后面有介绍。 休眠策略配置LeanCloud现在每天必须停机6个小时，并且若30分钟内无访问会自动休眠。 详细的配置方法还是建议参考原文Valine评论之Valine-admin配置攻略 。 自动唤醒任务登陆该网址：https://cron-job.org/ 注册帐号后，创建任务。 第一个title 就是给这个任务起个名字，随意填 第二个http链接输入LeanCloud的后台管理地址，例如我通过https://abc.lluuiq.com来进入后台，则填进去 时间使用自定义，然后日、星期、月都全选（先选中第一个，然后拉到最下面按shift点一下最后一个） 小时我选择的是7点到23点，详细根据自己需要来设置，总之中间要有6小时的时间让LeanCloud停机 分钟可以使用ctrl键来选择多个，我设置的是0、10、20、30、40、50，即在0分、10分、20分。。的时候访问一次后台。 这样设置的结果就是每天的7:00开始到晚上12:00，每隔10分钟进行一次访问来唤醒机器。 最后如果要保存日志则勾选Save responses，创建任务即可。 创建完成后，任务列表会出现目前进行的任务。 点击右方的History可以查看历史状态的信息 前面有对号的标志即表示成功访问了后台，后面可以看到具体的时间以及状态码等等。 重发邮件任务使用LeanCloud自带的定时任务来完成即可。进入云引擎/定时任务 创建定时任务， 关于Cron表达式，因为我设置的是每天7点到晚上12点之间来不停唤醒机器，所以我的表达式为 10 5 7 * * * 秒 分 时 天 月 星期，假如Cron表达式为 a b c d e f 则在 e月份的第d天的c时b分a秒，且仅为星期f时进行该任务。 *表示全部。 所以我的表达式意思为 在每天的7点05分0秒时，进行resend_mails函数，即检测未发送的邮件进行补发。 这样就解决了在0点到7点机器停机时假如有回复不能进行提醒的问题。 （目前仅是理论，因为并未实际测试过） 更新上述配置已成功。7点05收到了邮件补发。 查看日志可以发现 获取SMTP授权码，并设置自动转发（以163举例）登陆163邮箱，注意登陆的邮箱必须要是变量SMTP_USER填的值。 在设置里可以找到POP3/SMTP/IMAP 随便开启一个服务，主要是需要SMTP服务，IMAP与POP3无所谓。 我记得是开启时就会获得授权码，该授权码即为SMTP_PASS的值。 也可以在下面添加新的授权码 在设置里的常规设置中,找到自动回复/转发 填写转发到哪个邮箱，然后验证一下即可。若打算在该邮箱内保留邮件，则勾选保留原邮件。 配置好后返回 设置变量继续配置即可。 ====以下有需要则看====开启网页后台管理需要有个域名，进入设置里的域名绑定 这里因为我已经绑定过，所以显示的不一样，总之点击绑定新域名的那个按键。 二级域名可以自定义 ，例如 abc.lluuiq.com，我的域名是lluuiq，二级域名为abc。 这里就填个abc.lluuiq.com，稍后到域名管理处进行解析。 填好后大概会稍等一会，然后会给出CNAME记录值。记录值为CNAME:后面的内容，即 1kkfwnugs.cn-n1-cname.leanapp.cn 接下来到域名管理处进行解析CNAME 解析完成后，并且在设置变量中有添加ADMIN_URL的话，即可通过 https://绑定的云引擎域名 来进行访问 需要注意的是开启了自动管理SSL才能使用https访问，否则只能通过http访问。 在访问之前，先登陆https://绑定的云引擎域名/sign-up注册一个管理员帐号 邮箱是SMTP设置的邮箱，无法修改。 登录名使用只读里面的邮箱（登陆时要输入邮箱，我用自定义的昵称无法登陆，并且只能让登录名等于只读中的邮箱才有效。。），密码自定义。这样登录名即为SMTP的邮箱，密码为自定义的密码。 确认设置后，再通过https://绑定的云引擎域名来登陆，即可访问后台。 管理界面如图所示，样式可以更改，参考修改右键、QQ提醒的模板去修改后台管理的即可。 修改邮件、QQ提醒的模板说明修改的前提是在云引擎部署中使用的github地址是fork的地址。 克隆fork的代码仓库到本地，对文件进行修改。 public/stylesheets中的css文件是 views/comments.ejs的样式表 template内的文件夹名即设置变量中的TEMPLATE_NAME的值，引入对应的模板。这里我copy了一份custom2文件夹重命名为lluuiq来进行自定义，保证原本样式的情况下来修改。 check-spam.js为过滤邮件的脚本，基本不用管。 send-mail.js中设置QQ提醒的模板，并且可以修改发送邮件时的一些设置和邮件标题 其余的能不动就不动。 其余说明： 最好copy一份主题文件到新的文件夹中来对副本进行修改，这样能保持原主题的内容。 详细的修改可以自己尝试，可以重写一份页面来全面替换，这里例举个人对原有的主题进行修改的内容。 以下出现的process.env.XXX 表示获取云引擎设置的变量名内容。 &lt;%=XXX%&gt;为获取send-email.ejs里的对应代码块的变量。notice.ejs获取站长提醒的，send.ejs获取访客提醒的。 修改完后，要提交和push到github上，然后LeanCloud的云引擎再重新部署才能生效。 修改发送邮件的标题站长邮件提醒打开utilities/send-mail.js 站长自己发的评论不需要通知的内容为 comment.get(“mail”)来获取新评论的邮箱，如果是站长自己的邮箱则不发送邮件，毕竟自己回复别人干嘛发邮件来提醒自己。（QQ也不会提醒） 这里因为我设置了163邮箱的转发，所以SMTP_USER=163邮箱，但自己使用的不是这个邮箱。所以如果不进行其他设置的话，这段代码不会生效。这也是为什么当时加了TO_EMAIL变量指向自己的邮箱。当判定获取的邮箱为自己的邮箱时，就不会再进行提醒。 如果设置的SMTP_USER邮箱与自己接收提醒的邮箱为同一个，那么TO_EMAIL就可以去掉，这部分就不用管了。 第二个红框的部分，emailSubject为邮件的主题，可以根据自己的喜好来进行修改。 我在修改主题的下方的变量名中添加了一个变量senderName来获取LeanCloud中的SENDER_NAME变量，用来当自己的昵称，可以放在邮件提醒里的称呼中，也可以放在结尾的 @2020 lluuiq 中。 其余的部分为LeanCloud的日志输出内容，可以不用管，想修改输出信息的可以进行修改。 访客邮件提醒还是在utilities/send-mail.js中，代码块在下方，注意和站长提醒的类似，但代码是不一样的。 站长被@不需要提醒，是当访客回复自己时不触发这个回复提醒（毕竟站长提醒那里已经发一次了）。 第二个红框同上来修改 发送给访客的 邮件的 主题。 其余同上为输出日志，想修改的自行修改即可。 修改QQ提醒的模板还是utilities/send-mail.js里。 第一个红框的部分为戳一戳功能，基本不用修改。 第二个红框的部分为QQ机器人发送消息的模板，这里是我修改之后的样子，原来的样式是有一堆QQ表情的，我给删除了，并添加一些语句。[CQ:face,id=63]为鲜花表情，详细的id可以百度搜索。 $(text) 获取评论内容、$(url) 获取文章地址 、axios.get() 调用Qmsg接口 。。。等这样的内容就不用修改了。 最后一个红框是输出日志，不管就行了。 微信的模板修改与QQ的同理，找到对应的内容进行修改即可。 我修改后的简约提醒模板如图： 修改发送邮件的内容模板说明内容模板都在template/主题文件夹 中，若在原主题上进行修改则进入对应文件夹修改ejs文件就行了。这里我copy的custom2来进行修改的。 notice.ejs为发送给自己的评论提醒模板。 send.ejs为发送给访客的评论回复模板。 站长提醒进入notice.ejs 其中需要说明的是&lt;%=sendName%&gt;，为获取在站长邮件提醒里设置的sendName变量。 注意notice.ejs获取的是站长邮件提醒的代码块的内容。 稍后的send.ejs获取的是访客邮件提醒的代码块内容，也就是说如果访客邮件提醒的代码块里没有设置这个变量。那么send.ejs里使用&lt;%=sendName%&gt;是无效的，并且不会发送邮件给访客。（当初发现没有发邮件找了好久的原因，后来才发现是没在访客的代码块里添加该变量） 部分获取评论内容的东西就不用修改，修改一些自己想要的样式什么的就行了。 从这里再往下基本都是CSS，修改样式的。 效果图： 访客提醒进入send.ejs 代码内容与notice.ejs几乎一模一样，但是要注意的这里的内容是让访客看到的，根据自己喜好来修改或添加想让访客收到回复时看到的内容就行了。 Dear &lt;%=pname%&gt;为 Dear 访客昵称。 我删掉了原来的h3标签，因为显得过于冗余了，直奔主题显示曾经的评论、收到的回复。 这里在页脚处添加了一些信息，注意copy原来的标签语句后把id去掉，html里的id只能有一个。 效果图：","categories":[{"name":"笔记","slug":"笔记","permalink":"https://lluuiq.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Valine","slug":"Valine","permalink":"https://lluuiq.com/tags/Valine/"}]},{"title":"CLion配置成LeetCode做题工具","slug":"CLion配置成LeetCode刷题工具","date":"2020-04-07T05:21:56.000Z","updated":"2020-07-04T01:16:50.210Z","comments":true,"path":"post/202004071321/","link":"","permalink":"https://lluuiq.com/post/202004071321/","excerpt":"LeetCode自带的编译感觉并不好用，而且代码补全与提示是收费，故使用本地IDE作为LeetCode做题的工具。","text":"LeetCode自带的编译感觉并不好用，而且代码补全与提示是收费，故使用本地IDE作为LeetCode做题的工具。 前言关于CLion的C/C++环境配置在另一篇文章里有说明。 PyCharm、intellij IDEA等JetBrains公司的IDE都可以使用相同的配置流程。 安装插件leetcode-editor： 打开CLion设置，在Plugins搜索leetcode即可找到插件： 若网速过慢，或无法下载，也可以下载压缩包然后加载本地插件来安装。 在Git Hub的Releases下载插件压缩包 https://github.com/shuzijun/leetcode-editor 在JetBrains的插件库下载压缩包 https://plugins.jetbrains.com/plugin/12132-leetcode-editor 下载完成后在Plugins的上方点击齿轮处，选择Install Plugin from Disk，然后选择下载的压缩包路径安装即可。 配置插件leetcode editor 中文文档 安装好插件后，在IDE的右方侧边栏的下方，可以打开leetcode editor 打开后先点击齿轮进入配置 也可以通过 settings -&gt; Tools -&gt;leetcode plugin进入设置。 URL选择境内还是境外的leetcode Code Type选择C语言（若是其他环境的IDE选择对应语言） LoginName输入登陆的用户名（邮箱）和密码 TempFilePath为存放代码的位置，注：选择路径后，会创建leetcode\\editor\\cn目录来将代码存放在cn，例如我选择路径D:\\code\\，则会生成路径``D:\\code\\leetcode\\editor\\cn`，然后将题目代码放在cn文件夹下 LevelColour为划分题目难度的颜色设置，默认即可，也可以修改。 CodeFileName为创建题目代码时的文件名，默认为[题目标号]题目名.c，如：[1]两数之和.c CodeTemplate为代码编辑处的默认模板，先是题目描述，然后是题目给的默认代码 两部分都可以根据下方给出的参数来修改模板 注：若需要Debug，则先创建一个project，然后将TempFilePath的路径改为该项目。推荐设置项目名为leetcode，然后将TempFilePath设置为项目的父级文件夹，这样可以少一层插件创建的leetcode文件夹 例如：我在D:/code下创建一个project名为leetcode，然后插件的路径改为D:/code，则生成路径为下图： 但如果我的项目名不是leetcode，然后插件路径设置为D:/code/项目名，则效果为下图： 可以看到插件自己生成了leetcode文件夹来当做父级目录 回到正题，配置完成后保存，然后点击第一个图标登录 若有提示使用cookie登录，则进入LeetCode，按F12点击加载的文件，找到cookie复制粘贴然后login即可 登陆成功后便会刷新出题目，可以浏览全部题目 也可以根据难度、做题状态、以及一些活动来获取题目 配置Debug因leetcode给的默认代码为一个类或一个方法，因此需要自己编写main()函数来调用函数，实现Debug。 自定义代码块详细参考：Pycharm自定义模板 打开设置，搜索Live Templates，进入自定义代码块配置，选择C/C++，点击+号添加模块，这里我已经添加过一个main和一个lmain，默认是没有的。 填入缩写、描述以及代码块，下方的Applicable选择C。 这里的代码块在调用后，会按$No$（题号）、$FILE$（题目名）、$CODE$（代码块）的顺序进行填写，填写完一项后按TAB切换到下一项，参数的名称是自定义的 。 关于#include &quot;editor/cn/[$No$]$FILE$.c&quot;，调用目标.c文件，其中文件名应与配置插件时的自定义名称的格式相同。 1234567#include \"stdio.h\"#include \"editor/cn/[$No$]$FILE$.c\"int main() &#123; $CODE$ return 0;&#125; 这样当以后想对一道题进行Debug时，只需要新建一个.c文件，然后输入lmain调用该代码块，填写题号和题目后就可以编写对应代码然后调用题目中的函数了，Java、Python应该是调用类 。接下来就是解决C语言一个项目只能有一个main()函数的问题，需要修改CMakeLists。 实现多个main()函数文件添加一个add_executable，内容可以看到前面为一个项目名，后方为main函数的文件 这个项目名可以不存在，但需要与其他项目名不同，故可以在项目名后方加上数字来表示第几题即可，.c文件的名字自定义就行。这样以后想Debug时只需要在项目中新建一个.c文件，生成自定义代码块然后调用题目代码，编译前添加到CMakeLists即可。","categories":[{"name":"笔记","slug":"笔记","permalink":"https://lluuiq.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"CLion","slug":"CLion","permalink":"https://lluuiq.com/tags/CLion/"}]},{"title":"CLion配置C/C++开发环境","slug":"CLion配置C-C-开发环境","date":"2020-04-06T14:33:21.000Z","updated":"2020-07-04T01:12:11.080Z","comments":true,"path":"post/202004062233/","link":"","permalink":"https://lluuiq.com/post/202004062233/","excerpt":"记录配置CLion的过程","text":"记录配置CLion的过程 安装MinGWMinGW下载地址：MinGW 在中间点击图标下载windows版 下载后打开安装，一直下一步最后安装完成后点击Continue。 在打开的界面中，Basic Setup中勾选MinGW32-gcc-g++-bin。 然后进入All Packages，找到mingw32-make-bin勾选。 然后点击菜单栏的Installation -&gt; Apply Changes安装勾选的包 安装完成后点击Close关闭即可。 若不小心安装后关掉了或者没有自动打开MinGW，可以在MinGW的安装目录中打开bin文件夹下的 MinGW-get.exe 报错问题若在安装时出现输入错误: 没有文件扩展“.js”的脚本引擎的报错，则打开运行，输入regedit 在路径HKEY_CLASSES_ROOT\\.js下，更改默认值为JSFile 添加环境变量右键我的电脑，点击属性，然后进入高级系统设置 点击环境变量 找到系统变量的Path，双击进入编辑 点击新建，添加路径： 1C:\\MinGW\\bin 其中C:\\MinGW为安装MinGW时的默认路径，若有修改则换为对应路径 然后打开CMD，输入 1gcc -v 查看是否配置成功 CLion配置安装CLion后，并进行基础配置后，会提示配置C/C++环境，确保Environment为MinGW的安装目录即可。 Make、C语言编译器、C++编译器会自动搜索bin目录下上文中安装好的包（若提示没找到则自行选取路径，若在目录中没找到说明没有安装成功） 其他无需更改，点击OK即可。 然后就可以新建一个C语言项目 创建完成后会默认生成初始化代码块 然后在菜单栏的Run中点击Run… 初次运行会进行编译，等待编译完成后便会输出结果","categories":[{"name":"笔记","slug":"笔记","permalink":"https://lluuiq.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"CLion","slug":"CLion","permalink":"https://lluuiq.com/tags/CLion/"},{"name":"C","slug":"C","permalink":"https://lluuiq.com/tags/C/"},{"name":"C++","slug":"C","permalink":"https://lluuiq.com/tags/C/"}]},{"title":"PyCharm自定义模板","slug":"PyCharm自定义模板","date":"2020-03-30T00:41:07.000Z","updated":"2020-06-24T20:33:37.116Z","comments":true,"path":"post/202003300841/","link":"","permalink":"https://lluuiq.com/post/202003300841/","excerpt":"自定义文件模板： ​ 新建一个文件时自动添加一些头信息。 自定义代码块模板 ： ​ 当有的代码自己需要多次使用时，一次次重复编写效率过低，故可以自定义一个缩写来实现按TAB自动将代码块补全。","text":"自定义文件模板： ​ 新建一个文件时自动添加一些头信息。 自定义代码块模板 ： ​ 当有的代码自己需要多次使用时，一次次重复编写效率过低，故可以自定义一个缩写来实现按TAB自动将代码块补全。 文件模板详细说明可查看：官方文档 打开PyCharm的设置，搜索File and Templates，即可看到设置模板的界面 不同的文件可以设置不同的模板，这里设置python文件，选中Python Script，在右边的编辑初添加信息。 首先添加两行代码用于指定python环境以及采用utf-8编码 12# !/usr/bin/env python3# _*_coding:utf-8 _*_ 接下来自定义文件的信息。 不同的变量名： ${PROJECT_NAME} - the name of the current project. ${NAME} - the name of the new file which you specify in the New File dialog box during the file creation. ${USER} - the login name of the current user. ${DATE} - the current system date. ${TIME} - the current system time. ${YEAR} - the current year. ${MONTH} - the current month. ${DAY} - the current day of the month. ${HOUR} - the current hour. ${MINUTE} - the current minute. ${PRODUCT_NAME} - the name of the IDE in which the file will be created. ${MONTH_NAME_SHORT} - the first 3 letters of the month name. Example: Jan, Feb, etc. ${MONTH_NAME_FULL} - full name of a month. Example: January, February, etc. 示例： 1234# @Author : lluuiq# @File : $&#123;NAME&#125;.py# @project : $&#123;PROJECT_NAME&#125;# @Time : $&#123;DATE&#125; $&#123;TIME&#125; 最终设置结果： 设置后保存，新建一个python文件即可看到效果 代码块模块打开PyCharm的设置。搜索Live Template，找到python，点击右方的+添加自定义代码块 选择第一个 设置界面如图 Abbreviation为缩写，即输入什么内容后按TAB会输出该代码块 Template text为自定义的代码块 在下方点击Define，勾选python Description为描述，可以在输入缩写时弹出的信息中看到该描述 示例： 在python文件中输入demo，可看到如下信息。 按TAB后，会自动输出该部分自定义的代码 优化：回到设置界面，将代码中要输入的内容修改为参数类型 这样输出自定义的代码后，$text1$以及$text2$的内容默认为空，且光标会自动定位在$text1$处，当输入内容后按回车，光标会自动移动到$text2$处，最后移动到$code$处。 也可以用相同的参数名，这样输出代码后，光标会自动选中两个$text$，此时会对两处同时进行修改，按回车后跳转到$code$处。","categories":[{"name":"笔记","slug":"笔记","permalink":"https://lluuiq.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"PyCharm","slug":"PyCharm","permalink":"https://lluuiq.com/tags/PyCharm/"}]},{"title":"PyCharm+Anaconda3+PyQt环境配置","slug":"PyCharm+Anaconda3+PyQt环境配置","date":"2020-03-28T16:13:44.000Z","updated":"2020-07-04T01:12:53.548Z","comments":true,"path":"post/202003290013/","link":"","permalink":"https://lluuiq.com/post/202003290013/","excerpt":"记录用Pycharm搭建PyQt环境的过程","text":"记录用Pycharm搭建PyQt环境的过程 配置Qt Designer下载PyCharm与Anaconda3后，Anaconda3的包中集成了PyQt，并且也安装了Qt Designer 其中Qt Designer位于Anaconda3根目录/Library/bin/designer.exe（windows） 打开PyCharm设置，在Tools中点击External Tools，然后点击+号添加 然后在设置的界面输入名称与描述（自定义） Program为designer.exe路径，比如我的路径为C:\\Anaconda3\\Library\\bin\\designer.exe Working directory设置为项目路径，可以直接复制粘贴该变量或者在右边的Insert Macro中找到ProjectFileDir，点击添加（结果都一样）。最后点击OK即可。 1$ProjectFileDir$ 回到External Tools界面点击OK保存。 配置完成后，即可通过右键菜单栏启动Qt designer 或者点击PyCharm上方菜单栏的Tools来启动 配置pyuic添加pyuic用于将Qt designer生成的.ui文件转为.py文件 在设置的External Tools添加一个新的Tool 名称与描述自定义，Program为Anaconda3根目录下的python.exe路径 Arguments填以下命令 1-m PyQt5.uic.pyuic $FileName$ -o $FileNameWithoutExtension$.py Working directory使用当前文件所在的目录 1$FileDir$ 添加后保存即可。这样每次用designer生成的代码只需要右键-&gt;External Tools-&gt;pyuic即可生成python文件在相同目录下。","categories":[{"name":"笔记","slug":"笔记","permalink":"https://lluuiq.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Anaconda3","slug":"Anaconda3","permalink":"https://lluuiq.com/tags/Anaconda3/"},{"name":"PyCharm","slug":"PyCharm","permalink":"https://lluuiq.com/tags/PyCharm/"},{"name":"PyQt","slug":"PyQt","permalink":"https://lluuiq.com/tags/PyQt/"}]},{"title":"Volantis主题DIY笔记","slug":"Volantis主题DIY笔记","date":"2020-03-17T04:29:46.000Z","updated":"2020-06-24T23:30:47.864Z","comments":true,"path":"post/202003171229/","link":"","permalink":"https://lluuiq.com/post/202003171229/","excerpt":"主题首页：Volantis 记录一些DIY的过程","text":"主题首页：Volantis 记录一些DIY的过程 主题文件目录在主题的根目录下。 layout为页面、卡片、各部位的布局代码 source存放css、js、字体、图片等代码与资源 _config.yml为主题配置文件 顶部导航栏设置顶部导航栏始终显示。 打开source/css/_layout/navbar.styl 找到最下面的代码，将其注释 123456// .cover-wrapper// .l_header// trans(0.5s)// transform: translateY(-2 * $navbar-height)// &amp;.show// transform: translateY(0) 搜索栏修改搜索结果中的输入内容的上边距，使内容显示在合适位置 原来的样式： 修改后： 打开source/css_layout/search.styl 使用搜索功能搜索关键词input 修改margin，将$gap改为5px 12#u-search-modal-input margin: 5px 50px 修改卡片的内外边距打开_config.yml 找到gap的设置，添加base值，大小为要设置的内外边距 1base: 10px 该方法是同时修改卡片之间的距离以及卡片内容与卡片边框之间的距离。 若要分开修改需要到css文件逐一修改，可以用该方法添加一个新的变量来传参。 例如： 在gap中添加一个变量，变量名自定义，这里用test示例，后面的大小为要设置的大小。 1test: 10px 接着打开source/css/_defines/layout.styl，找到gap的设置 添加自定义的变量 1$test = convert(hexo-config('style.gap.test')) || 16px $test为CSS设置中的变量名，style.gap.test中的test为在_config.yml中设置的变量名，||后面为默认值 设置完成后，在对应的CSS设置中仅需要将间距大小改为$test，就会使用设置中test的值 封面界面去掉搜索效果如图 打开layout/_cover/index.ejs 找到如图所示代码，将其注释 添加QQ在线联系打开_config.yml，搜索social，找到社交功能设置部分 添加内容如下 12- icon: fab fa-qq url: http://wpa.qq.com/msgrd?v=3&amp;uin=【你的QQ号】&amp;site=qq&amp;menu=yes 第一次点击会提示未开通，到提示网站登入一次即可。 将侧边栏移动到左边打开layout文件夹，对以下文件统一修改 打开一文件后，搜索代码 1&lt;%- partial(&#39;_partial&#x2F;side&#39;) %&gt; 将该行代码复制到第一行代码&lt;%- partial(&#39;_pre&#39;) %&gt;的下方，然后将原来位置的代码注释（方便以后若需要的话改回），示例： 然后修改CSS文件， 打开source/css/_layout/main.styl 搜索.l_main，找到padding-right，将right改为left，使原本的右边距变为左边距 修改后为 为适应手机端，还需要修改此处 若想要手机端的卡片有外边距，则将padding-right: 0按需求修改边距 ，例如若想要左右边距都为10px，则应该改成 12padding-right: 10pxpadding-left: 10px 效果如图： 若满意现在的效果则到此即可。接下来将侧边栏移动到网页的边缘处，然后让文章列表、其余页面的主要内容居中。 注：修改后，在主题的配置文件中的最大宽度max_width将只修改页面内容卡片的宽度 进一步修改首先在_config.yml中修改最大宽度为100%，但注意这样会导致顶部导航栏的宽度也随之变成100%。 打开source/css/_layout/main.styl，将.body-wrapper中的justify-content: space-between注释。 该语句会使页面内的flex布局为flex项目之间的距离相等，若只有两个项目的情况下会导致一个在最左边，一个在最右边。注释后会采取默认的布局方式即左对齐。 页面内容的修改在该文件中的.l_main，将width中的100%调小以更改页面内容的宽度，修改padding-left来更改与侧边栏的距离。 侧边栏的宽度可以在source/css/_defines/layout.styl中进行修改 修改透明度使用css两种更改透明度方法，其中透明度取值为0~1，越小则透明程度越高 使用 background-color:rgba(R,G,B,透明度) 使用 opacity: 透明度 若使用rgba，则需要传入RGB颜色代码，并且修改的仅有div元素的背景透明度 若使用opacity，则只需要传入透明度，修改的是整个div元素的透明度（包括图片、文字等也会变透明） 在source/css中创建一个新的stylus文件，名字随意 编辑内容，其中颜色代码、透明度自行修改 12345.widget background-color:rgba(255,255,255,0.9).post background-color:rgba(255,255,255,0.9) 然后打开source/css/style.styl，插入引用语句 1@import 'diy_css.styl' 同理可以用此方式进行CSS的自定义，可以做到在不修改原本代码的情况下更改css 实现pjax感谢大佬的博客开源，让我能参考源代码进行修改：Material X主题pjax使用 感谢大佬的讲解 ：用pjax让你的页面加载飞起来! 在layout/_partial/head.ejs的末尾 &lt;/head&gt;的上方引用pjax 1&lt;script src=\"https://cdn.jsdelivr.net/npm/pjax/pjax.js\"&gt;&lt;/script&gt; 在layout/layout.ejs的末尾&lt;/body&gt;的上方插入代码 1234567891011&lt;script&gt;var pjax = new Pjax(&#123; elements: \"a\", selectors: [ \"title\", //pjax加载标题 \".l_main\", //pjax加载主内容 \".l_side\", //pjax加载侧边栏 \".switcher .h-list\", // 使手机端的搜索框与菜单栏生效 ]&#125;)&lt;/script&gt; elements选择触发器，即点击什么来触发pjax（只能用a或者form），a为链接。 selectors根据css选择器来选择更新的节点（即当触发elements时哪些内容会刷新，其余部分保持不变） 可以用节点、类、id等选择元素。用法参考：CSS 选择器 这里我设置的为：点击链接，则刷新标题、类为l_main与l_side的元素（页面内容与侧边栏）会刷新，其余不变。 pjax优化-自动生成Fancybox在定义pjax的脚本代码中插入语句（放在&lt;script&gt;与&lt;/script&gt;中） 123456789101112131415161718192021//加载fancyboxfunction LoadFancybox()&#123; $(\".article-entry\").find(\"img\").each(function () &#123; //渲染fancy box var t = document.createElement(\"a\"); $(t).attr(\"data-fancybox\", \"gallery\"), $(t).attr(\"href\", $(this).attr(\"src\")), $(t).attr(\"margin\",\"0 auto\"), $(this).wrap(t) &#125;)&#125;// pjax加载结束后执行函数document.addEventListener('pjax:complete', function ()&#123; LoadFancybox();&#125;);// 窗口监听load(加载、刷新)事件，执行LoadFancybox()函数window.addEventListener('load',function()&#123; LoadFancybox();&#125;); 插入后结果如图： 修改后发现实现了fancybox，但是图片不再居中，故进行修改。 打开source/css/_layout/main.styl 搜索img 找到位于l_main&gt;post&gt;a下的img，将display: inline注释 BUG： 放大图片再关闭后，页面位置与放大前不对应，会返回到放大前一张图片时的位置 。 更新： 将原来代码中的$(t).attr(&quot;data-fancybox&quot;,&quot;gallery&quot;),中的&quot;gallery&quot;改为&quot;&quot; 实现pjax后发现导航栏在点击不同部分时，主题的下划线不会改变，进行修改没有修改成，故直接将CSS代码注释 打开source/css/_layout/navbar.styl 搜索&amp;:active,&amp;.active 将该部分代码注释 pjax刷新评论问题：实现pjax后发现通过pjax刷新页面后评论无法成功加载，推测是因为评论由js脚本引入，pjax刷新页面不会加载js。 解决思路：当pjax执行完成后再次调用评论js脚本来生成评论。 在pjax脚本中插入以下代码，其中调用脚本的方式有两种，用哪一个都可以。 12345678910111213141516171819202122232425262728293031323334function LoadValine()&#123; // 两种调用方式，一个调用配置里的js路径，一个调用本地，用哪个都可以 // $.getScript(\"&lt;%= theme.comments.valine.js %&gt;\", function() &#123; $.getScript(\"/js/Valine.js\", function() &#123; // 生成评论的代码 var GUEST_INFO = ['nick','mail','link']; var guest_info = '&lt;%= theme.comments.valine.meta %&gt;'.split(',').filter(function(item)&#123; return GUEST_INFO.indexOf(item) &gt; -1 &#125;); var notify = '&lt;%= theme.comments.valine.notify %&gt;' == true; var verify = '&lt;%= theme.comments.valine.verify %&gt;' == true; var valine = new Valine(); valine.init(&#123; el: '#valine_container', notify: notify, verify: verify, guest_info: guest_info, &lt;% if (page.valine &amp;&amp; page.valine.path) &#123; %&gt; path: \"&lt;%= page.valine.path %&gt;\", &lt;% &#125; else if (theme.comments.valine.path) &#123; %&gt; path: \"&lt;%= theme.comments.valine.path %&gt;\", &lt;% &#125; %&gt; appId: \"&lt;%= theme.comments.valine.appId %&gt;\", appKey: \"&lt;%= theme.comments.valine.appKey %&gt;\", placeholder: \"&lt;%= (page.valine &amp;&amp; page.valine.placeholder) ? page.valine.placeholder : theme.comments.valine.placeholder %&gt;\", pageSize:'&lt;%= theme.comments.valine.pageSize %&gt;', avatar:'&lt;%= theme.comments.valine.avatar %&gt;', lang:'&lt;%= theme.comments.valine.lang %&gt;', visitor: '&lt;%- theme.comments.valine.visitor %&gt;', highlight:'&lt;%= theme.comments.valine.highlight %&gt;' &#125;) &#125;);&#125; 然后在监听pjax完成后的函数里调用函数 123456// 加载pjax后执行的函数document.addEventListener('pjax:complete', function ()&#123; LoadFancybox(); // 调用刚刚设置的函数 LoadValine();&#125;); 结果应该如图： 源码位置：layout\\_partial\\scripts.ejs，上述代码中生成评论的部分即红框内的部分 添加百度统计首先到百度统计注册帐号，然后到管理界面，新增网站 新增后，会给一段代码，将其复制，并在外面加上ejs模板的语句 1234567891011&lt;% if (theme.baidu_analytics)&#123; %&gt; &lt;script&gt; var _hmt &#x3D; _hmt || []; (function() &#123; var hm &#x3D; document.createElement(&quot;script&quot;); hm.src &#x3D; &quot;https:&#x2F;&#x2F;hm.baidu.com&#x2F;hm.js?【你的key】&quot;; var s &#x3D; document.getElementsByTagName(&quot;script&quot;)[0]; s.parentNode.insertBefore(hm, s); &#125;)(); &lt;&#x2F;script&gt;&lt;% &#125; %&gt; 打layout\\_partial\\scripts.ejs，将该段代码插入到最后 再打开_config.yml，加入一条语句 1baidu_analytics: true 因为用js文件加载的方式，所以百度统计上的代码检查功能失效，需要手动检查。 保存后，打开网站，按F12打开开发者工具，然后刷新页面，在js文件中看到以hm开头的js文件说明配置成功","categories":[{"name":"笔记","slug":"笔记","permalink":"https://lluuiq.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://lluuiq.com/tags/hexo/"}]},{"title":"Scrapy笔记","slug":"Scrapy笔记","date":"2020-03-03T20:05:53.000Z","updated":"2020-07-04T01:13:02.092Z","comments":true,"path":"post/202003040405/","link":"","permalink":"https://lluuiq.com/post/202003040405/","excerpt":"记录学习Scrapy时遇到的坑 Scrapy官方文档 学习时看的视频：【python爬虫_从入门到精通（高级篇）】scrapy框架、反爬、分布式爬虫","text":"记录学习Scrapy时遇到的坑 Scrapy官方文档 学习时看的视频：【python爬虫_从入门到精通（高级篇）】scrapy框架、反爬、分布式爬虫 使用笔记创建爬虫项目CMD命令： 1scrapy startproject 【项目文件夹名】 会在输入命令的目录中生成一个名称为【项目文件夹名】的文件夹用来存放爬虫项目 创建爬虫CMD命令： 12cd 【项目文件夹名】scrapy genspider 【爬虫的名字】 &quot;【爬取的域名】&quot; 进入创建好的爬虫项目中创建爬虫，名字不能与其他爬虫重复，且不能与项目名相同。 爬取的域名指定爬取范围，使该爬虫仅仅在该域名内的页面中进行爬取。 更改设置首先更改设置中的一些值 善用``CTRL+F`搜索参数，快速找到位置 打开爬虫根目录中的settings.py，进行更改。 12345678910111213141516171819# 设置爬取的延迟，单位为秒，即间隔1秒爬取一次DOWNLOAD_DELAY &#x3D; 1# 不遵守协议ROBOTSTXT_OBEY &#x3D; False# 设置默认的请求头，如有需要可以添加其他信息。DEFAULT_REQUEST_HEADERS &#x3D; &#123; &#39;User_Agent&#39; :&#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.132 Safari&#x2F;537.36&#39;&#125;# 设置pipelines自动运行，用于自动根据代码保存数据，找到后取消注释即可，不用更改ITEM_PIPELINES &#x3D; &#123; # 执行优先级300，值越小，优先级越高 &#39;【爬虫项目名称】.pipelines.【爬虫项目名称】Pipeline&#39;: 300,&#125;# 不显示warning以下级别的输出日志LOG_LEVEL&#x3D;&quot;WARNING&quot; 创建运行脚本创建后，不用每次运行爬虫都要输入终端命令，只需要执行python文件即可。 在项目中的任意位置创建一个python文件，名字自定义 输入内容： 123from scrapy import cmdlinecmdline.execute(&#39;scrapy crawl 【爬虫的名字】&#39;.split()) 爬虫的名字指的是创建爬虫时用的名字，而不是项目的名字。 创建CrawlSpider爬虫scrapy爬虫的进化版 CMD命令： 1scrapy genspider -t crawl 【爬虫名字】 “【域名】” Scrapy Shellcmd终端中，进入爬虫项目文件夹，输入 1scrapy shell 【链接】 即可打开scrapy shell， 用于进行一些不调用scrapy引擎进行一些测试（如正则表达式，检查获取信息的内容等等） 关闭windows的快捷编辑模式有时scrapy会在跳转到另一页面时莫名奇妙的自动终止，关掉windows的快捷编辑模式，问题得以解决。 报错笔记ERROR: Spider error processing response.body返回的是二进制文件，不能用w方式进行写入，应改为wb","categories":[{"name":"笔记","slug":"笔记","permalink":"https://lluuiq.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"python","slug":"python","permalink":"https://lluuiq.com/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://lluuiq.com/tags/%E7%88%AC%E8%99%AB/"},{"name":"Scrapy","slug":"Scrapy","permalink":"https://lluuiq.com/tags/Scrapy/"}]},{"title":"hexo部署到服务器","slug":"hexo博客部署到服务器","date":"2020-02-23T01:09:25.000Z","updated":"2020-07-04T01:12:23.996Z","comments":true,"path":"post/202002230909/","link":"","permalink":"https://lluuiq.com/post/202002230909/","excerpt":"不使用github pages，改为自己的服务器。将代码托管到gtihub，使用webhook实现github与服务器同步推送代码，并且安装hexo-admin来实现后台管理与服务器到github上的推送。","text":"不使用github pages，改为自己的服务器。将代码托管到gtihub，使用webhook实现github与服务器同步推送代码，并且安装hexo-admin来实现后台管理与服务器到github上的推送。 购买服务器服务器商千千万，自行挑选一个靠谱的。 注意：国内各服务商有学生优惠套餐，但是只能购买大陆服务器，且大陆服务器需要备案才能使用，备案很麻烦，要花十几到二十天左右。但是便宜稳定。 进入服务商的购买页面，选择配置。 若是一键选择套餐购买，需要选择地区、选择服务器的配置、选择服务器镜像系统、服务器带宽(越大则服务器能承受的压力越大)、购买时长。 若是自定义配置，需要选择购买的计时方式、配置的详细选择，更多的系统选择以及自定义镜像、存储空间。 总结就是越强越贵，时间越长越贵。个人博客不需要非常好的配置，推荐系统选择linux 注意：大陆服务器需要至少购买3个月时长才可以进行备案。 假如是国外服务器，则购买后可以直接进行部署了。如果是头铁选了国内，则需要先进行备案(大概20天左右。备案流程放到最后。) 我使用的宝塔控制面板，简单方便无脑。安装宝塔后，可以在控制面板进行操作来操作自己的服务器。 注：若服务器操作过程出现无法挽回的问题，可以在服务器控制台重装系统。 连接服务器首先下载putty、xhell等连接工具，我使用的是finalshell，用哪个都可以，原理都一样，用服务器商自带的登陆功能也可以。打开后界面如图。 点击文件夹，新建一个连接，选择SSH连接 名称自定义，主机为上一步的ip地址。填写完后点击确定。 (国外服务器可勾选智能加速，国内则不勾选)此时我使用的是美国服务器，所以勾选了海外加速。 可以看到新建了一个服务器连接，ssr为刚刚自定义的名称，后面有ip地址和端口号，和服务器主机名。 双击该连接，进入服务器，连接成功出现如图所示界面。 如果出现该窗口 点接受并保存。 部署到服务器解析域名注：若github page有绑定域名的话，先把绑定解除，并且将解析暂停，否则会冲突。 到域名管理处，点击解析 按照如图所示添加两条记录，一个主机记录选@（直接输入域名即可进入网站），一个选www（前面加上www可进入网站），也可以自定义一个前缀，但在后面站点配置时要加进去。 记录类型选A，指向一个IP地址，然后在记录值处填自己服务器的公网IP。 主机记录说明： 安装宝塔面板根据官方教程进行安装。 宝塔Linux面板安装教程 若安装完成不能使用，则需要放行端口： 腾讯云：https://www.bt.cn/bbs/thread-1229-1-1.html 阿里云：https://www.bt.cn/bbs/thread-2897-1-1.html 华为云：https://www.bt.cn/bbs/thread-3923-1-1.html 进入服务器，根据自身系统输入安装命令，命令失效则看官方教程，有备用命令 Centos安装命令： 1yum install -y wget &amp;&amp; wget -O install.sh http://download.bt.cn/install/install_6.0.sh &amp;&amp; sh install.sh Ubuntu/Deepin安装命令： 1wget -O install.sh http://download.bt.cn/install/install-ubuntu_6.0.sh &amp;&amp; sudo bash install.sh Debian安装命令： 1wget -O install.sh http://download.bt.cn/install/install-ubuntu_6.0.sh &amp;&amp; bash install.sh Fedora安装命令: 1wget -O install.sh http://download.bt.cn/install/install_6.0.sh &amp;&amp; bash install.sh 以cenOS为例： 输入命令进行安装 输入y，安装宝塔，然后稍等片刻 如图安装完成，会显示控制面板的url以及用户名和密码 安装完成后，就可以关闭finalShell了，打开浏览器，输入该地址，并输入刚刚显示的用户名和密码，进入面板 这里建议进入后修改登陆面板的帐号与密码，不使用默认生成的。并且将面板的网址保存起来，方便以后直接输入网址进入面板。 进入面板后，会出现如下提示，进行安装软件。可以根据需求进行安装，也可以直接点一键安装，推荐LNMP，且选择极速安装，编译安装太慢。 创建站点宝塔面板安装完成后，在导航栏点击网站，选择添加站点。 设置如下，在域名处填写自己的域名，可填写多个 （填写进的域名在解析完成后可以访问到该网站） 生成站点后，点击网站名或者后面的设置，可以进入站点设置来添加一些新的域名以及二级域名等等。 其中abc为自定义的二级域名，想不想设置随意。设置了就需要在解析时填写对应的主机记录。 点击添加后如下，这样用户可以通过以下域名访问到该服务器部署的网站，如果不想让github.io的地址也访问到该服务器，可以删除。 添加的域名需要解析到服务器才可以正常使用。 添加完成后关闭设置即可。 最后到域名管理处。进行解析。添加记录如下 记录值：www和@各添加一次， 记录类型：选A 线路类型：默认 记录值：你的服务器IP地址 结果如下（若有自定义二级域名等等，需要再添加上。） 等待几分钟生效，即可通过域名访问该服务器。 使用webhook实现同步页面虽然服务器是配置好了，但是宝塔并不支持代码托管，也就是说使用hexo d命令推送的代码并不能推到宝塔上来实现更新网站。于是使用webhook来实现与github代码同步。 原理： 安装宝塔WebHook在宝塔面板导航栏点击软件商店。搜索 webhook 点击安装。 配置webhook安装完成后，点击设置，(可以勾选首页显示。) 点击添加，名称随意，执行脚本先随便填写一点内容，点击提交后再编辑脚本。 （若出现编辑不能保存的情况，则在宝塔面板的右上角点修复） 添加完成后点击编辑，删除原来随意填写的内容，添加如下代码并修改gitHttp。 注： 原代码有一个$1变量，在后面的密钥部分的URL地址最后通过param=X，将X作为 该变量传入脚本，但因为我无论怎么改都会出现参数错误，因此将该变量删掉了。然后改一下代码，将​$1的参数替换为固定的值，只要站点目录存在就没问题。 大概是因为自己设置了保存博客源代码的分支为默认分支的原因，用原代码里面的有一句git pull总是拉不过来文件，在服务器上手动拉取发现拉取的是博客源代码的分支。故在原代码中将git pull改成git pull origin master来指定拉取静态页面的分支。 123456789101112131415161718192021222324252627282930313233343536#!/bin/bashecho \"\"#输出当前时间date --date='0 days ago' \"+%Y-%m-%d %H:%M:%S\"echo \"Start\"#git项目路径gitPath=\"/www/wwwroot/【站点根目录】\"#git 网址gitHttp=\"【仓库git地址】\"echo \"Web站点路径：$gitPath\"#判断项目路径是否存在if [ -d \"$gitPath\" ]; then cd $gitPath #判断是否存在git目录 if [ ! -d \".git\" ]; then echo \"在该目录下克隆 git\" sudo -Hu www git clone $gitHttp gittemp --depth=1 sudo mv gittemp/.git . sudo rm -rf gittemp fi #拉取最新的项目文件 echo \"拉取最新文件\" sudo -Hu www git reset --hard origin/master sudo -Hu www git pull origin master #设置目录权限 echo \"设置目录权限\" sudo chown -R www:www $gitPath echo \"End\" exitelse echo \"该项目路径不存在\" echo \"End\" exitfi 更新： 在指令语句前加入sudo，授予管理员权限，避免执行失败 gitHttp路径为github代码仓库的地址，需要到代码库中查看(带.git尾缀的地址，即hexo部署时配置 _config.yml时填写的地址。)，点击复制键进行复制 粘贴后格式应该如下 1gitHttp&#x3D;&quot;https:&#x2F;&#x2F;github.com&#x2F;XXXXXX.git&quot; 随后点击右下角的保存即可。 github配置hook编辑完成后，点击查看密钥 复制好密钥以及下方URL的&amp;前面的内容（全复制也可以） 进入github的网站代码库的设置，导航栏选webhook，然后点击Add webhook 填写内容 填写好后点击Add webhook。 然后回到面板，点击测试，然后在日志中查看结果。 同时回到github的webhook界面，编辑刚刚的钩子 在最下面可以看到成功的信息。 这样以后在本地进行的deploy，服务器最自动钩取github仓库的master分支，实现将博客部署到服务器上。然后使用域名就可以进行访问(不要加https://)。 添加SSL证书，开启https协议注：若出现访问拒绝的话，注意是否加了https协议。 没有配置SSL证书的话，用https访问会被拒绝。SSL证书有收费的也有免费的。 宝塔自带免费证书，但要求实名认证。 接下来以个人服务商腾讯云的免费证书为例， 按步骤填写即可注册证书。添加后下载证书，然后解压。 若关闭了界面，可以在SSL证书处下载证书 打开下载好的文件夹，打开Apache 打开.crt和.key结尾的文件。 然后在宝塔面板进入站点的设置，图中点击网站名或者设置都可以 如图，在SSL界面其他证书里，将.key结尾的密钥放入左边，.crt结尾的证书放入右边，保存，然后强制HTTPS，即可开启https协议。 源代码同步到服务器（舍弃）更新（必看）：本来是想的利用hexo-admin实现在服务器上开启后台管理，然后就能实现理想中的功能： 本地操作完博客后，源代码+静态页面+文章都可以同步到服务器上 服务器操作完博客后，源代码+静态页面+文章也都同步到本地 这样的话，我可以在本地尝试修改主题与配置，满意后进行推送同步到服务器。在没有hexo与git环境时（如临时用其他电脑或者用手机），然后又不能安装或者不想安装的话，可以通过hexo-admin后台来写文章并deploy，等到了自己的电脑上直接克隆就可以完成同步了。 但在配置时发现了一个问题，hexo-admin提供的deploy功能貌似仅支持以hexo开头的命令，如hexo d、hexo new &quot;文章名&quot;，我试图加入git命令然后就报错了。 这就导致我能成功将本地的源代码、静态页面推送到服务器，但是用hexo-admin新建然后写的文章是没办法自动保存到github上来克隆到本地的。再考虑到安装插件的问题，一些功能需要安装一些插件（如音乐播放器、live2D、加入视频等等）。插件是不会被推送的，若想推送插件的话，每次推送都要等待很长时间。 综上，故暂时舍弃源代码同步到服务器，让服务器仅保存静态页面就行了。所以下面的内容可以不用看了。 PS：关于能否使用git命令的问题已经提交到github上的Issues，如果有解决办法的话希望能留言告诉我。 在服务器上部署hexo关于CentOS的安装git、Node.js我遇到一些问题，参考了下面两篇文章完美解决，故这里就不说安装方法了，直接看这两篇就行了。 CentOS安装Git参考链接：https://www.cnblogs.com/imyalost/p/8715688.html 注：Hexo需要10版本以上才能支持，默认安装的话版本开头为6，需要安装最新版本才可以，我就用默认安装Node.js的方法结果最后hexo安装失败。 CentOS安装最新版Node.js参考链接： https://www.jianshu.com/p/4a9449506924 安装hexo指令： 1npm install hexo-cli -g 警告不用理会。 克隆保存在github上的博客代码通过cd命令切换到想保存博客根目录的文件夹下，例如我打算把博客的根目录放在 /www下，则输入cd /www， 然后克隆在github上的博客源代码，例如我的命令为 git clone https://github.com/lluuiq/lluuiq.github.io.git，后面是仓库地址，改为自己的。 1git clone 【仓库地址】 这样在输入该指令的文件夹下会出现【yourusername】.github.io的文件夹，里面放着博客的源代码。 若想改个名字的话，在包含根目录的文件夹下（我的是/www）输入下面指令 1mv 【yourusername】.github.io 【newname】 我将文件夹名称改为wa2000，下面的wa2000文件夹都是博客根目录。 在服务器上的hexo根目录已经建成，接下来就是同步github上的源代码仓库，使得当github仓库的文件更新时，服务器上的博客源代码也会同步更新。 服务器上的hexo同步github宝塔与webhook安装步骤同 部署到服务器 设置webhook与上方内容相同，但需要更改的是分支名选择存放博客目录的分支 个人的配置（参照修改） git项目路径：gitPath=&quot;/www/wa2000&quot; git网址：gitHttp=&quot;https://github.com/lluuiq/lluuiq.github.io.git&quot; 分支名：source 12345678910111213141516171819202122232425262728293031323334#!&#x2F;bin&#x2F;bashecho &quot;&quot;#输出当前时间date --date&#x3D;&#39;0 days ago&#39; &quot;+%Y-%m-%d %H:%M:%S&quot;echo &quot;Start&quot;#git项目路径gitPath&#x3D;&quot;【你的博客根目录地址】&quot;#git 网址gitHttp&#x3D;&quot;【你的仓库地址】&quot;echo &quot;Web站点路径：$gitPath&quot;#判断项目路径是否存在if [ -d &quot;$gitPath&quot; ]; then cd $gitPath #判断是否存在git目录 if [ ! -d &quot;.git&quot; ]; then echo &quot;在该目录下克隆 git&quot; git clone $gitHttp gittemp mv gittemp&#x2F;.git . rm -rf gittemp fi #拉取最新的项目文件 git reset --hard origin&#x2F;【你的分支名】 git pull #设置目录权限 chown -R www:www $gitPath echo &quot;End&quot; exitelse echo &quot;该项目路径不存在&quot; echo &quot;End&quot; exitfi 添加webhook后，参考上方内容与仓库进行链接即可。 创建站点如图点击添加站点。 添加域名可以加前缀，但加了前缀也要加入对应的解析。根目录后面一定要是public文件夹，生成的静态页面会保存在该文件夹中，只有将该文件夹设为根目录网站才会正常显示，最后点击提交即可创建。 然后回到服务器，安装必要包 12npm install hexo --savenpm install --save hexo-deployer-git 输入github的用户名与密码 12git config --global user.email &quot;【github邮箱】&quot;git config --global user.name &quot;【github用户名】&quot; 配置SSH配置SSH免去每次推送都要帐号密码，任意地方输入下方指令，【注释】部分可用自己邮箱，github,也可以不加-C 1ssh-keygen -t rsa -C &quot;【注释】&quot; 中间都按回车就行了，然后到ssh存放的地方 12cd ~&#x2F;.sshvim id_rsa.pub 打开id_rsa.pub后，复制里面的公钥（用finalshell的话直接在下方的文件目录里双击打开也可以） 然后到如图地方添加公钥 然后到仓库的地方，复制ssh地址 （若本来就是ssh地址则跳过）打开根目录的配置文件_config.yml，将deploy下的repository改为ssh地址。 在博客的根目录下生成静态页面并尝试推送到github 123hexo cleanhexo ghexo d 注：若出现下方情况则使用下方命令删除.user.ini 12chattr -i 【.user.ini路径】rm -rf 【.user.ini路径】 若出现下方情况则修改ssh配置 1vim &#x2F;etc&#x2F;ssh&#x2F;sshd_config 找到 RSAAuthentication与PubkeyAuthentication，将注释去掉，并确定AuthorizedKeysFile后面为.ssh/authorized_keys且可用。 然后重启ssh服务。 1&#x2F;sbin&#x2F;service sshd restart 再次执行hexo d可能会出现下方警告 该警告是让你将github的IP加入host，可以无视，也可以加进去去掉警告。 1vim &#x2F;etc&#x2F;hosts 然后在里面加入【警告中的IP地址】 github.com，保存并退出即可。 清除、生成、推送都成功后，重启宝塔面板 1&#x2F;etc&#x2F;init.d&#x2F;bt restart 重启后，在本地使用推送将博客源代码推到github时，服务器上会自动拉取进行同步。 接下来就是在服务器上开启远程后台管理了。 开启后台管理（舍弃）使用hexo-admin插件开启后台管理功能，可以通过网页进入后台管理文章，也可以通过内置的脚本实现部署与推送。 放行4000端口到宝塔面板的安全里放行4000端口，说明随意填。 以腾讯云为例，到服务器的管理处放行4000端口。 注意：hexo s命令不支持https，在输入网站时不要加https 测试推送git操作官方文档，或者百度git指令查询详细用法。 在根目录处执行下方执行指令 123git add .git commit -m &quot;【更新说明，随便填】&quot;git push origin 【博客源代码分支名】 出现上方提示说明可以进行推送。 如果保存源代码的分支不是默认的，要在git push后面空一格加上origin 【博客源代码分支名】 若提示 则输入 1git remote add origin 【github仓库的ssh地址】 编写脚本在根目录创建一个文件夹存放脚本（直接放根目录也行，但后面记得更改配置的路径），名称随意，我用admin作为文件夹名，deploy作为脚本名。然后进入创建的文件夹，新建并编辑一个脚本文件 我的代码为mkdir admin与vim deploy.sh 123mkdir 【新建的文件夹】cd 【创建的文件夹】vim 【脚本名】.sh 在脚本文件中输入下方指令，“save blog”里面是更新说明，自定义填写。 123#!/usr/bin/env shhexo ghexo d 然后按ESC，接着输入:wq保存并退出。 这样当用后台管理时，会先生成静态页面，再进行部署，部署完成后会将源代码推送到github。 可自行修改需要的命令，例如加入hexo clean，也可以加一些其他自己需要的命令。 为刚刚脚本授予权限 1chmod +x 【脚本名】.sh 然后执行该脚本看看效果，如果一切顺利，则可以进行下一步。效果大概如图： 脚本配置好后，开启开机自动启动hexo s命令来启动后台，因为hexo-admin 在创建的存放脚本的文件夹中新建自启脚本（同样名称自定义） 12cd 【存放脚本的文件夹】vim 【自启脚本文件名】.sh 然后输入命令，cd后面的路径要换成自己的博客根目录路径，例如我的为cd /www/wa2000 123#!&#x2F;bin&#x2F;bashcd 【博客根目录路径】hexo s 写完后同样按ESC，输入:wq保存并退出 添加权限 1chmod +x 【自启脚本名】.sh 然后返回服务器的根目录，打开etc/rc.d/rc.local 12cd ~vim &#x2F;etc&#x2F;rc.d&#x2F;rc.local 在下面添加脚本路径，如我的脚本存放目录为/www/blog/admin/server.sh，添加效果如下 然后保存并退出。 执行下方命令授权 1chmod +x &#x2F;etc&#x2F;rc.d&#x2F;rc.local 然后重启服务器 1reboot 稍等片刻进入http://【域名】:4000，若能进入，说明配置成功。 安装hexo-admin并配置参考文档：Easy Hexo hexo-admin官网 服务器上进入博客根目录，然后输入命令安装hexo-admin 1npm install --save hexo-admin 安装完成后即可通过http://【域名】:4000/admin 进入后台。接下来进行一些配置。 进入后台的设置界面，点击红框内的链接。 如图，填入登陆后台用的用户名与密码，secret能为密码进行加密，随意填自己喜欢的短语就行。 将生成的代码拷贝到博客根目录下的配置文件_config.yml中，并添加deployCommand deployCommand的内容单引号内为一条指令 ./admin/deploy.sh指运行保存在admin文件夹内的deploy.sh，将路径改为自己的脚本运行指令即可。 保存，然后重启服务器。 关于网站备案的琐事到购买服务器/域名的服务商那里有详细的备案流程说明，这里仅记录个人备案遇到的坑。以腾讯云举例。 腾讯云备案流程 国家规定服务器需要购买至少3个月才能备案 学生套餐优惠只能购买三次，一次最短购买一个月时长，最长为一年时长，本来抱着试试的心态只买了一个月，结果发现居然要至少3个月才能备案，于是只能再去用学生套餐来延长时长，本来三次机会，结果花了两次机会只买了一年一个月，如果确定想用大陆服务器并且想长时间使用的话，建议一次购买最长时长，10元/月算非常便宜了。 审核时间长 首先需要先为域名进行实名认证，实名认证后备案需要申请幕布进行拍照(给你邮过去，免邮)，等幕布到了以后按要求拍照并上传，填写关于网站的资料以及服务器的资料，最后提交。 大概腾讯那边审核一两天，然后腾讯帮你提交到审核局审核个10天左右(最晚一个月，个人经历时间为10天左右)，审核通过后会给你工信部备案号，但还要到全国互联网安全管理服务平台再提交一次备案，不过这个审核较快，一天就通过了。通过后会给你联网备案号。 以百度为例上面的为工信部备案号，下面的为联网备案号。 关于在全国互联网安全管理服务平台备案 注意：里面有一个为网站填写信息的，有个选择网站是否为交互类型，如果不是论坛、有注册功能的那种一定要选择否，选是的话需要到当地公安局一趟进行当面审核。这是为了保证网站安全所以需要到公安局进行审核，并且填写大概7-8页厚的表格。","categories":[{"name":"笔记","slug":"笔记","permalink":"https://lluuiq.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://lluuiq.com/tags/hexo/"},{"name":"github","slug":"github","permalink":"https://lluuiq.com/tags/github/"},{"name":"webhook","slug":"webhook","permalink":"https://lluuiq.com/tags/webhook/"}]},{"title":"hexo写作简易化","slug":"hexo写作简易化","date":"2020-02-21T16:37:41.000Z","updated":"2020-06-24T20:33:37.156Z","comments":true,"path":"post/202002220037/","link":"","permalink":"https://lluuiq.com/post/202002220037/","excerpt":"hexo写文章不像WordPress与Typecho那样拥有后台管理系统，每次都要新通过指令新建文章（若本地创建的话没有配置的front-matter），并且写完后还要清除缓存，生成页面，然后部署。而且hexo上传到网上的文章要考虑一个图床的问题，图片若不是url链接是无法在网上被访问到的。hexo也缺少一个方便的、随时随地修改文章的后台管理。于是写篇文章记录我简化写作的的过程。","text":"hexo写文章不像WordPress与Typecho那样拥有后台管理系统，每次都要新通过指令新建文章（若本地创建的话没有配置的front-matter），并且写完后还要清除缓存，生成页面，然后部署。而且hexo上传到网上的文章要考虑一个图床的问题，图片若不是url链接是无法在网上被访问到的。hexo也缺少一个方便的、随时随地修改文章的后台管理。于是写篇文章记录我简化写作的的过程。 使用HexoEditor进行写作点击进入项目地址：HexoEditorHexoEditor的优点： 1. 读取hexo的post模板生成文章 2. 可以在软件上清除缓存、生成页面、推送，也可以直接一键部署 3. 支持图床链接HexoEditor的缺点： 1. 没有文件目录 2. 没有文章目录 3. 个人遇到了腾讯云无法配置图床以及七牛云图床链接缺少一个’/‘的问题 安装：到HexoEditor的github地址，下载压缩包解压即可。 使用Typora进行写作Typora官方网站Typora具有写作即样式的效果，使用markdown的语法后会直接将该部分文字转为markdown的样子，真·所见即所得。这是刚写完的一句话 当光标没有聚焦在该部分时，就会自动转为markdown样式，当光标再次回来时又会变成文本。当然，因为Typora是一个markdown写作软件，不是专为hexo订制，所以写完后还是要通过命令行指令进行部署。 关于图床有很多能存储图床的服务商，有域名的话可以选择自己域名的服务商提供的空间存储，也可以使用七牛云（个人目前使用），免费的图床有sm.ms。下面以个人使用的七牛云为例优化写作时的图片插入问题。 使用HexoEditor（舍弃）因七牛云的链接会缺少一个’/‘符号，导致要手动添加，腾讯云又无法配置，故不推荐使用，若用sm.ms作为图床的话可以使用（个人没有sm.ms，所以没有测试是否存在问题) HexoEditor仅支持截图，不支持复制图片。HexoEditor自带上传然后转化功能。只需要填好该三项，存储空间与域名会自动显示出，然后自行选择即可。进入七牛云的密钥管理（HexoEditor右键空白处有快速打开能进入七牛云主页）。可以看到对应的密钥，隐藏的话点击显示即可复制。AK粘贴到AccessKey，SK粘贴到SecretKey，就完成配置了。用法举例：当截图后，会在本地保存图片，如下图所示， 右键空白处，点击上传 七牛，链接会自动转化为七牛云存储空间里的URL，并且是整篇文章的图片全部转化。 使用MpicMpic官网这是我一直在用的类似小工具一样的东西，支持截图（看更新日志说是QQ截图，不知其他截图是否支持）、复制、拖拽均可自动上传并复制URL，还可以查看上传目录并且复制链接或者删除（但没有预览图，可以复制URL地址粘贴到网址栏然后查看） 配置方法参考上方HexoEditor的配置，但域名、空间名需要自己填写。 使用Hexo-admin实现后台管理hexo-admin有个缺点是只能在开启hexo server时才能进行管理。 安装hexo-admin官网在下方可以看到安装流程 若已经安装过hexo和创建好博客根目录了，则直接安装插件即可。 在博客根目录内右键空白处，点击Git Bash Here 12npm install --save hexo-adminhexo server -d 然后打开浏览器输入 1http:&#x2F;&#x2F;localhost:4000&#x2F;admin&#x2F; 即可进入后台管理。 Posts为文章列表Pages管理hexo的页面About是关于插件与hexo的信息Deploy可以部署一些脚本左边为写作区，右边为预览区，右上角有删除、文章设置与发布（这个发布仅仅部署在本地，未推送）。文章会自动保存，同时也支持复制图片URL（但是我用的Mpic,hexo-admin的图片老是裂开）点击发布后，会变成Unpublish，再次点击会取消在本地的部署。并且发布后，在这里编辑的内容可以在本地服务器上事实预览。 为后台添加密码可以设置是否需要帐号密码进入后台。（设置后每次hexo clean后登陆都要帐号密码）点击Settings，点击如图所示的链接然后填好帐号与密码，Secret是加密用的，内容不用改。 最下面会生成一段代码，将这些拷贝到网站根目录下的配置文件_config.yml，然后保存即可。 实现一键部署按照写脚本部署的方法windows不知道是因为权限问题还是不支持的问题，部署一直报错，查阅github上的issue后找到了作者给出的解决办法。issue链接：https://github.com/jaredly/hexo-admin/issues/94直接打开根目录下的node_modules文件夹，再打开hexo-admin文件夹，编辑器打开deploy.js，将 1var proc &#x3D; spawn(command, [message], &#123;detached: true&#125;); 改为（把原来的注释掉，换成下面这行代码即可） 1var proc &#x3D; spawn((process.platform &#x3D;&#x3D;&#x3D; &quot;win32&quot; ? &quot;hexo.cmd&quot; : &quot;hexo&quot;), [&#39;d&#39;, &#39;-g&#39;]); 结果如图： 改完后要通过在网站根目录打开Git Bash 然后输入hexo clean清除缓存，之后输入’hexo g’重新生成静态页面，启动本地服务器即可。回到hexo admin的部署界面，直接点击Deploy，效果如下可以看到 Std Output里的内容说明部署成功，下方的警告是指windows与linux的换行符问题，可以无视。","categories":[{"name":"笔记","slug":"笔记","permalink":"https://lluuiq.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://lluuiq.com/tags/hexo/"}]},{"title":"考研成绩总结","slug":"考研成绩总结","date":"2020-02-20T11:05:23.000Z","updated":"2020-06-24T22:09:13.052Z","comments":true,"path":"post/202002201905/","link":"","permalink":"https://lluuiq.com/post/202002201905/","excerpt":"20届的考研，今天查了考研成绩，结果在预料之中，不算非常失望，但也没有带来意外的惊喜。总结下考研的过程，也可以给想考研的人一些参考（学霸就当看笑话就行了）。 成绩不高，毕竟人傻学习差，但已经满意了，要说遗憾的就是数学考砸了。","text":"20届的考研，今天查了考研成绩，结果在预料之中，不算非常失望，但也没有带来意外的惊喜。总结下考研的过程，也可以给想考研的人一些参考（学霸就当看笑话就行了）。 成绩不高，毕竟人傻学习差，但已经满意了，要说遗憾的就是数学考砸了。 成绩 关于准备时间因个人的专业方向选择了大数据相关，发现该专业相关岗位大部分要求学历要高，而本人仅仅是个三本渣渣，2019年7月初有了考研的想法，7月30号开始学习，而在此之前没有该想法时报了学校内的实训，导致中间还有一个月的时间参与实训，又缩短了学习的时长。但是感谢实训的老师能睁一只眼闭一只眼，让我在实训过程中也能抽空学习。 关于报考报考院校为南京信息工程大学 人工智能学院，双非，大气专业一流学科。 专业计算机科学与技术，学硕。 考虑自己准备时间仅仅4个月，赶不上别人一两天准备的进度，故放弃985与211。 目标确定为双非后，筛掉高人气的学校，如杭电、南邮，因为竞争激烈，如果我是准备了一年的时间的话还可以考虑。在7月份看了很多学校，如浙江工业大学、浙江工商大学、宁波大学、南京理工大学、江苏科技大学、南京工业大学、南京审计大学、南京信息工程大学、东南大学(???)、成都理工大学、重庆邮电大学、河南工业大学、安徽理工大学、青岛大学、燕山大学等等。 因为比较宅(上海举办的活动多)加上想去长三角，所以目标确定为浙江、江苏。为什么不考虑上海嘞？因为上海是个旱区，而且我有自知之明复试会被刷。 最后考虑到自己时间不够，所以报考院校的专业课最好只有一门，而且是简单点的，学校又要好，计算机专业又强的(又想好，又想好考)最后确定目标为南京信息工程大学，计科学硕考试科目：政治、英一、数一、C语言。为什么选学硕不选专硕嘞？因为当时的相关信息是只招学硕，到9月学校发出招生目录时发现今年新增了专硕，同时有的学校专业课改考408，这也导致了南信的报考人数突然激增了。而且新建了一个学院(人工智能学院)，以前的30+名额划分了一半给新学院….. 这就导致了计算机学院就17个名额、人工智能学院14个名额(研招网数据)。南信大实力越来越强了，今后考研选南信大的话难度会上升。 考虑自己的情况，数学成绩平时还算不错，但英语是真的渣，政治还可以，C语言是强项(课本内容限定)，最后决定还是报学硕，因为专硕人太多，鬼知道有什么妖魔鬼怪。 我英语很差，属于初中词汇有的都还不认识的，比如分不清 影响affect、effect、期望expect、专家expert。英一英二对我来说都一个样，(后来发现如果考英二的话，成绩还是会比英一高10分左右的，不过也差不多了)。 数学一相比数学二多了高数部分内容以及概率，线代多的那点部分比较简单，自认为数学应该不会太差于是确定最后还是报考学硕，事实证明数学选学硕专硕最后结果好像差不多。 学习过程首先自己是个晚上经常睡不着的人，晚上1点–3点睡已经成了日常了，要闭眼至少10分钟才能入睡，经常因为睡觉时闭上眼睛感觉眼前一片发白难以入眠，有几次一晚都睡不着(失眠)。一般都睡到中午12点左右，不管早睡晚睡。后来干脆晚上都是4–5点睡觉，然后12点起来吃饭，除了吃饭时会看看假面骑士、番剧、星际炉石直播等，其余时间一直学习。 推荐一个微信公众号：考研学子，分享很多爱心分享版课程，包括一些难搞到的。 资源下载网站：http://kyxz.ys168.com/ 数学主要学习内容就是数学，毕竟是个重点项目。在大学学习时，高数和线代起码还有听过课。高数一般，线代学的不错。高数属于快考试了赶紧学学，平时作业都是抄。线代就是学的比较好，题都能做出来，最后期末考90分。（然而后来全忘了。）。概率是我大学时唯一一个根本没听过的课，上课全在玩手机，期末突击了一下基本压着线过的。然而考研结束后，线代白给，概率成了拿分点。 最开始看的汤家凤老师的高数基础，然后看的李永乐老师的线代，王式安老师的概率，都说李永乐线代地位无可撼动，但个人有的地方听不懂，后来去看了方浩的线代，听懂了一些，然后又听不懂了，最后直接看书自学了。 概率是打从一开始就没听懂，最后问了朋友一些概率重点，理清了概率这部分的框架，然后看了汤老师的概率提高班。 第二遍时高数看的武钟祥老师，发现武钟祥的讲课内容与思路就非常适合我。线代在自学遇到不懂的地方就看李永乐的课。概率自学，整理知识点。 练习题因为时间短，没有选择1800题，最开始做的张宇的1000题，后来感觉这都什么J eight ，直接让书吃灰去了，然后选择了330+660（然而到考试都没做完）。在11月的时候发现某宝上李林老师的书卖的非常便宜，只要几块钱，就买了。主要是我室友不考了，给了我很多数二的书，导致我的书架上有好几个老师的各种数学书……光数学相关的就占了我一个桌子长度的1/3，等开学返校了拍个照看看都有什么。草稿纸在全力投入数学的时间内基本两天一本（草稿纸一张写的很乱，没有排版一样的去用，但是正反两面都写完了再用下一本）。笔芯是一天一杆左右，这里推荐下听雨轩这个牌子的笔芯，是真的好用，不会有断墨现象，笔摔地上了拿起来还能用。 11月份才直到关于李林老师的事，最后我也做了他的题，听了一点课（他的绝密押题班能听就听，虽然在互联网的伟大实力下已经不绝密了，而且我考试的这一年只压中了数二的一道大题，有人说数一压中了一道，但我怎么都没找到）。 我学习是几乎不做笔记的，因为做了也不会看，比较喜欢理解原理然后掌握内容，数学的笔记我只做了知识点框架和解题方法，把解题方法与对应的知识点写在一起，写在纸上然后夹在书里，因为写在书上我经常找不到或者为了找到对应的笔记浪费时间，而且我没有笔记本，因为感觉把笔记夹在书上的对应章节里更简单省事。 概率理清后我倒是自己写了大概4–5张纸的笔记 ，主要记随机变量分布、数字特征、样本统计量、大数定理中心极限定理的知识点以及公式、相关题型的求法，比如怎么求知X、Y的分布求Z的，然后对应各种情况的解法写上去，最后订起来忘了就看，临近考试时每天看一遍。 英语英语本来就没基础，时间又短，中间试过几天用那个什么遗忘曲线背多少单词，然后过段时间全忘了，已经没救了，直接放弃。 在最后还剩1个月的时间准备了下英语的大小作文模板，顺便准备了四级的。看的刘晓艳老师的课，模板讲的非常好，还会讲很多题型该怎么做，比如翻译该怎么翻译，不会翻的怎么办，她的保命班确实保了我英语刚好压线（大概过了），强力推荐，英语无论好的还是差的都可以看。 关于作文模版，我是参考刘晓艳老师讲的结构，然后自己想好中文，接着各种百度查、用有道、问同学翻译出优美点的，通用的句子，主题词的空一挖，到时候考啥填啥。比如考到积极乐观的品质相关的（勤奋、勇敢、乐观、孝顺等等），写一个：文学巨匠鲁迅曾经说过：“XX是一个人通往成功所必备的品质”（Lu Xun, a literary giant, once said that x is a necessary quality for a man to succeed），翻译是用百度翻译出来的，实际写的不是这个，做了修改。也可以换成其他人说过啥啥怎么样。四级同样可以用模板，对应不同的题型（书信、图画、议论等）各准备一个就行了。 政治政治我无脑推荐徐涛老师，本来政治的内容就很枯燥，但徐涛老师的课让我对哲学和近代史提起了很大兴趣，尤其是马哲部分，难以理解的内容他总能用恰当的例子让你明白这部分什么意思，也多亏了他我政治取得了自认为还算不错的成绩。做题就肖秀荣的1000题还有肖4肖8（然而今年纯背肖4的话有点危险，压中的没有往年多，但如果理解了肖4的答题原理还是轻松的），但我1000题没有做完，基本只做了一半，徐涛的优题库几乎做完了，毕竟喜欢看徐涛的视频，可以做完看他的讲解。 关于政治的背诵，我是几乎没有背，选择题一些经常考的匹配的背会就行了（我还绝大部分都没背，个人真不适合背东西，背了就忘，反复背还是能忘），比如XX时期对待富农的态度是，我国的基本政治是。最后狂背肖4，按肖老师给的优先顺序，把重要的背了，其他的实在记不住了，而且时间不也不够。 马哲部分一定要去理解，可以既背又理解，但理解了其实很多都不用背，所以千万不要死记硬背。马哲不像近代史、毛中特、思修那样答案出来直接选，它的题都是挖掘题目中的故事信息，然后选择符合这个题意的选项，也就是说答案可能全是对的，要找出题目想让你选什么。也有可能马哲里直接出现错误的选项，如人人创造历史，这种就相当于给放水了。 近代史一定要捋清时间线，历史很好的话近代史学起来会轻松很多，最好理解当时的国情去学习，用当时的思想去体会能好学很多，下面举个例子，可以直接不看，也可以当我讲故事。 鸦片战争后老外侵入我国，太平天国（农民）起身反抗最后失败，然后高官中也想救国（洋务运动），但因为当时思想腐朽，而且高官想维护自己的统治地位，所以只学西方的技术，不学西方的体制（也就是中体西用），然后一个甲午战争直接全没了，不仅把洋务运动给打没了，而且列强发现wocao原来中国这么好欺负，就来瓜分我们，中华民族也由此意识到再这样下去不行了，投入这么大的项目结果一战全没了，还是被当年的弟弟日本给打败的，普遍意识觉醒，当然战争结束后要签署条约进行赔款，所以签订了《马关条约》。后来戊戌变法，先进知识分子总结了失败的原因，认为洋务运动只学了技术，没有学体制，认为别人西方君主立宪制才会这么diao，所以主张立宪，这也是与洋务运动的辩论战的主要点（要不要君主立宪），当然最后失败了，因为他们企图让腐败的清政府悔改，但人家清政府好好的官坐着，怎么能说放弃就放弃。再后来辛亥革命时，孙中山先生认为指望政府没救了，打算领一帮精英要用暴力手段直接干掉清政府，强行改变，这也是第二次辩论战的主要点（要不要武力推翻清政府）。辛亥革命成功推翻了清政府，但实际是失败的，因为袁世凯隔岸观火最后渔翁得利，逼迫孙中山交出政权想称帝。可以发现，除了太平天国，其他的革命都没有动用人民群众的力量。徐涛也讲过这些运动失败的原因，里面都有一个阶级局限性，意思就是你们这个阶级就决定了你们都是five，只有我(dang)能救中国。毕竟政治考的是立场，不是研究对与错的问题，心中有dang，成绩理想是有道理的。 毛中特在学习近代史后结合国情去理解一些政策会好懂很多，因为毛中特学的并不好（有大量要记的东西），所以没法讲出有参考价值的东西。思修我就没怎么看过，除了法律很多用常识就可以解题（只要不反人类）。 近代史一般多选答案要么全对，要么对三个，习大大说的全是对的，不知道符合不符合题意的选项只要它说的对就选。 专业课这个就没有参考价值了，因为我本来C语言学的就不错，考研学习期间C语言看的时间不到一周，考试时又几乎全是练习题原题，稳过。 考完后数学选择和填空还可以，一共错了4道，有一道求行列式的第一遍做对了，结果检查一遍后写了个错的上去，草（中日双语）。在做出6道题后，靠总结的历年选择比例 3:2:2:1 以及 2:2:2:2把不会的蒙上去了。最后好像选择错了一道，填空错了3道。 大题直接白给。 高数第一道大题求最大值，把公式给忘了，但靠中间解题的步骤给了步骤分。求曲线积分的对了一道。证明和空间曲线就当场死亡，证明题本来就是放弃的，空间曲线加了个函数=白给。 线代今年改成了一个二阶矩阵，而且题改成了给出变化过程求参数（这个真不应该，准备了很久变换的写法，结果改成了求参数，按理说应该能算出来），脑子短路了，临近考试瞎写了一些上去。第二道看到证明就知道完了，汤家凤的1800题里是原题，但是我没做过。线代本应是个稳拿分点，结果成了稳失分点。（但除了学霸，大部分人线代都做的不好，应该是不适应这种题型） 应该是看到线代完了心态直接崩了，概率第一题的第一问做错了（写出来了，但最后对答案发现错了），好在第二问做对了（大概）。第二题就是全对了，毕竟概率的出题风格相对稳定，最后一道都是最大似然估计，平时做题也很注重这一点。 英语我的英语没啥好说的，能不能过全靠上天，模板写的还可以，就是把大作文的习惯写成了behavior，草（中日双语）。最后应该是压线过。 政治政治选择发挥的不错（指较平时做题），平时做题时差了22-25分，最高的一次32分，考完对答案估分选择大概在34~36左右。大题因为只背了肖4和15个坚持，答的不够好，全靠自己的政治精神补充内容。 在大题有一道写出社会主义核心价值观的，当时突然把社会的自由平等给忘了，抬头四顾心茫然，绝望之中发现在考场的黑板旁挂着牌子，上面写着鲜亮的社会主义核心价值观几个大字，直接对着抄上去了。 专业课选择填空写的非常顺利，可以确定全对。三道编程题一道10分，只写出了一道，另外两道需要学习一个算法。考完看群，有人说写的暴力破解的方法，但当时我没想到:cry:。最后随便写了一些上去，不管对不对了，认为起码给点步骤分吧，结果一份都没给。估分时认为130分起步，步骤分给一点能130+，结果成绩130。 后记国家线应该是过了，但复试线很悬，又不想参加调剂，毕竟学历差，成绩也不算高，调不到好的学校。打算如果没过复试线就找工作，找不到的话再二战。 更新： 已被录取。","categories":[{"name":"人生相谈","slug":"人生相谈","permalink":"https://lluuiq.com/categories/%E4%BA%BA%E7%94%9F%E7%9B%B8%E8%B0%88/"},{"name":"学习","slug":"人生相谈/学习","permalink":"https://lluuiq.com/categories/%E4%BA%BA%E7%94%9F%E7%9B%B8%E8%B0%88/%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"考研","slug":"考研","permalink":"https://lluuiq.com/tags/%E8%80%83%E7%A0%94/"}]},{"title":"markdown文章显示测试","slug":"markdown文章显示测试","date":"2020-02-20T10:56:45.000Z","updated":"2020-07-04T01:12:31.340Z","comments":true,"path":"post/202002201856/","link":"","permalink":"https://lluuiq.com/post/202002201856/","excerpt":"这是一个测试用的文档，该句也是一级标题这是二级标题这是三级标题这是四级标题这是五级标题接下来是阅读更多","text":"这是一个测试用的文档，该句也是一级标题这是二级标题这是三级标题这是四级标题这是五级标题接下来是阅读更多 接下来的测试内容用二级标题标注 分隔符：第一行，接下来加入分隔符 第二行，与第一行之间有分隔符 加粗：正常文字 vs 加粗文字 斜体：正常文字 vs *斜体文字 删除线：正常文字 vs 删除线文字 代码框：正常文字 vs 代码框内文字 高亮：正常文字 vs ==高亮== 添加注释：正常的内容正常^注释的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容正常的内容。 角标：上角标：写个X的平方+Y的平方：x^2^+y^2^ 下角标：写个H2O：H2O 数学公式：公式写法： 123x^2 + x^2 &#x3D; 2x \\\\f(x)&#x3D;\\frac&#123;1&#125;&#123;\\sqrt&#123;2\\pi\\sigma&#125;&#125;exp(-\\frac&#123;(x-\\mu)^2&#125;&#123;2\\sigma^2&#125;) 效果如下：$$x^2 + x^2 = 2x\\f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$$ 列表：有序列表： 第一行 第二行 第二行的子行1号 第二行的子行2号 突然无序的行 3.忽然有序的行 无序列表： 第一行 第二行 表格： one two three two three four 链接：百度 代码块：markdown原生： 12345678910import pandas as pdimport plotly.graph_objs as goimport plotly.offline as pyfrom plotly import subplotsimport random# 添加isPass列，得分为60及以上的学生设为几个，否则为不及格df['isPass']=df['score'].apply(lambda x: '及格' if x&gt;=60 else '不及格')# 将及格的学生赋值给 df_isPassdf_isPass = df[df['isPass'] == '及格'] Volantis2.2.1主题代码块： 标题1code snippet emoji：我:heart:你，:kiss: 引用：正常的内容 引用的内容 图片测试：正常内容。 正常内容。 Tab测试：第一个tab的名字第二个tab的名字1代码框 正常文字正常的文字 插入图片： 暂时想到了这么多，如有遗漏希望评论留言帮忙补充(估计也不会有人这么闲能看这个测试看到这里)，感谢。","categories":[{"name":"笔记","slug":"笔记","permalink":"https://lluuiq.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://lluuiq.com/tags/hexo/"},{"name":"markdown","slug":"markdown","permalink":"https://lluuiq.com/tags/markdown/"}]},{"title":"hexo本地部署流程","slug":"hexo本地博客部署流程","date":"2020-02-19T13:00:39.000Z","updated":"2020-07-04T01:12:40.701Z","comments":true,"path":"post/202002192100/","link":"","permalink":"https://lluuiq.com/post/202002192100/","excerpt":"一年前用hexo部署过博客，由于其他事情导致长时间未使用，后来又尝试了wordpress与typecho，但个人比较喜欢修改主题样式，感觉wordpress与typecho的封装性太强，于是又回来继续使用hexo。记录自己重新部署hexo的过程，更新以前的部署笔记，故有的时间显示为2019年，但流程都是一样的。 这篇笔记里可以看到 本地部署过程 静态页面部署到github过程 hexo修改配置 绑定域名 源代码保存到github","text":"一年前用hexo部署过博客，由于其他事情导致长时间未使用，后来又尝试了wordpress与typecho，但个人比较喜欢修改主题样式，感觉wordpress与typecho的封装性太强，于是又回来继续使用hexo。记录自己重新部署hexo的过程，更新以前的部署笔记，故有的时间显示为2019年，但流程都是一样的。 这篇笔记里可以看到 本地部署过程 静态页面部署到github过程 hexo修改配置 绑定域名 源代码保存到github 本地部署过程需要安装的东西：git、Node.js、hexo。 其中git安装完成后的Git Bash，其作用与系统自带的CMD命令行相同，系统中的CMD命令同样可以在Git Bash中完成。 链接： git官方下载地址 git淘宝镜像下载地址 Node.js官网下载地址 hexo使用命令安装。 hexo官方文档里面有关于hexo的各种使用方法，包括各个指令、文件的说明、如何更改网站的一些信息等等。 hexo常用指令：hexo new &quot;title&quot; 新建文章(md文件)，title为文章的标题hexo new page &quot;pagename&quot; 新建网页，pagename为网页的名称hexo clean 清除部署的緩存hexo n == hexo new 新建一篇文章hexo g == hexo generate 生成静态页面hexo s == hexo server 本地部署，可预览网站，默认端口为4000，浏览器输入localhost:4000即可进入网站进行预览，回到git-bash按ctrl+c退出预览(退出后localhost:4000失效)hexo d == hexo deploy 将网站部署到GitHubhexo g -d 生成页面并部署到GitHubhexo g -s 生成页面并本地部署进行预览 安装git：本文章书写日期时最新版本为2.22.0版本 因版本可能不同，因此安装过程中的组件选择可能会有所差异，基本默认选项即可 下载完成后进入安装界面 (注:以下安装的选项请以实际自身需求为准，仅供参考): 选择需要安装的组件。 选择git的默认编辑器: 配置环境变量选项,推荐默认第二项: 选择https传输协议 默认即可: 选择git的换行方式 请根据自身需求更改: 设置git命令行的样式: 设置选项：1.是否允许文件缓存 2.是否允许git许可证管理，默认勾选： 是否参与新的测试,貌似是会使git更快，但还不稳定: install 安装即可: git安装完成后，需要进行配置，在git安装目录或菜单栏中找到git-bash，打开后如图 输入如下，其中” “中的your name 和your email为你的Git Hub用户名(非昵称)与邮箱 12git config --global user.name &quot;your name&quot;git config --global user.email &quot;your email&quot; 并可通过以下命令查询用户名与邮箱 12git config user.namegit config user.email 结果如下 安装Node.js：该文章书写时，版本为10.16.0 安装界面: 选择安装模式,我选择了第四个，next即可: 以下过程命令行既可使用windows的cmd，也可以使用git安装过程中的 git-bash进行操作 命令行中输入node -v可查看node的版本 ,输入 npm -v查看npm包的版本 12node -vnpm -v 因为npm为国外源，下载速度感人，故使用cnpm使下载指向国内源。 使用淘宝镜像下载 cnpm: 1npm install -g cnpm --registry&#x3D;https:&#x2F;&#x2F;registry.npm.taobao.org 下载完后查看cnpm版本 1cnpm -v 查询成功则证明安装完成。 安装hexo：使用cnpm下载hexo,用hexo -v查看hexo的版本: 12cnpm install -g hexo-clihexo -v hexo部署博客：在我的电脑中创建文件夹用于存储博客网站，即网站的站点。文件夹名称自定义，我使用blog,目录为D:\\blog。打开blog文件夹，右键空白处点击Git Bash Here在该目录下打开Git Bash(或者用CMD切换到该目录也行)。 输入cnpm install hexo --save安装组件。 1cnpm install hexo --save 输入hexo init进行初始化，等待时间较长，约几分钟。 1hexo init 注：若blog文件夹非空，则会报错: 使用hexo s 在本地启动博客 1hexo s 如图所示显示本地部署成功 打开网页，地址栏输入http://localhost:4000即可从本地进入博客 目录内的各个文件的作用参考官方文档 静态页面部署到github过程在github上创建静态网站的存储库：通过Ctrl+C停止服务 登陆自己的Git Hub 点击进入登陆界面 登陆成功后，网页右上角个人头像旁边，点击 + 号 选择New repository创建一个新的仓库 输入的信息如下，其中 Repository name内容必须是 github的用户名，而不是昵称。点击Create repository创建项目。 创建成功后，界面如图，复制https的链接。 回到git-bash 使用cnpm安装git部署插件，插件名为:hexo-deployer-git 1cnpm install --save hexo-deployer-git 安装过程中若有警告可以忽略 修改 _config.yml 文件：打开网站目录的 _config.yml 移动到最低端，在deploy:后面写入内容 123type: gitrepo: 刚刚复制的https链接branch: master 推送到github page：修改完成后，保存文件，在Git Bash中输入 1hexo d 即可将本地的网站服务器渲染出的静态页面上传到github。 该过程可能需要输入github和coding的用户名和密码，若Git Hub配置过SSH则不需要输入。 如果有报错，检查之前的配置是否有误。 12git config --global user.name &quot;your name&quot;git config --global user.email &quot;your email&quot; 配置语句是否正确 your name为用户名(非昵称) 推送完成后，再次进入仓库，即可看到上传完成的静态网页。 并且可以通过 你的用户名.github.io 来进入网站，此时网站已经部署到github page，其他人也可以通过该地址访问你的网站。 hexo s指令仅启动本地服务，修改后只能通过localhost:4000来进行访问，此时没有推送页面到github。想要推送到github生成页面的话，需要通过hexo d进行推送。推送前输入指令hexo clean清除缓存，然后再输入hexo g重新生成静态页面，然后推送即可。 hexo修改配置官方文档里有基本的配置文件内容说明，例如修改博主名称、网站名称、副标题、描述等等。 永久链接：打开博客目录下的配置文件_config.yml，按Ctrl+F搜索 URL url内容不用改，修改permalink内容 其意思为：修改一篇文章的url 默认的设置为将一篇文章的创建日期+title作为永久链接，但这样并不美观，并且在分享链接时因为编码问题中文会被转码造成如下结果 链接的格式为 你的域名/permalink内容， 比如我修改后的post/:year:month:day:hour:minute/ 会将创建文件时的年月日时分作为永久链接，避免了分享中文时的乱码。 加一个post是为了将文章统一放在一个文件夹中，在生成静态网页时，会生成一个post文件夹，里面存放生成的文章。还有其余的样式，在官方文档的永久链接中有说明。 还有一种改法是使用:urlname，然后在文档的头信息中给urlname参数，让该参数的值为永久链接，但同样避免不了中文转码的问题，我个人就使用日期作为永久链接了。但这样其实有个问题就是以后管理文档时，post文件夹内显示的都是日期数字，不能直观的看到文章标题。 修改默认文章模板：打开站点目录下的scaffolds文件夹，打开post.md 该markdown文档的内容会在生成一个markdown文档后自动添加进去。 12345678910111213141516171819---title: &#123;&#123; title &#125;&#125; date: &#123;&#123; date &#125;&#125; comments: true # 是否开启评论mathjax: false # 是否开启数学公式渲染toc: true # 是否启用目录top: false # 是否置顶# 若使用urlname作为永久链接则添加该项urlname:categories: - [父类,子类]- 同级分类tags: [标签1,标签2]---&lt;!-- more --&gt; 提前设置好模板，这样生成一个新markdown文档后，只需修改urlname以及设置分类categories与标签tags即可，若主题支持，可设置是否有目录toc，是否置顶top，是否开启评论comments，不同的主题可能名称不同，根据自己的主题修改即可。 若有不支持的功能也不会出错，仅仅无法加载该内容。 hexo本地部署流程与1582117239000 为标题与文档创建日期，不需要改动 分类categories里，前面有减号-的表示为同级分类，中括号[]括起来的为父子分类。 例如 123categories: - [生活,笔记]- 娱乐 文档在推送后，分类为生活类中的笔记类，同时也是娱乐类。 也可以将娱乐类改为 [娱乐,音乐]，这样就同时是娱乐类中的音乐类。 绑定域名解析域名以我的域名为例，不同商家解析时都差不多。在域名管理处点击解析 点击添加记录。会出现如图的添加设置。 主机记录可理解为域名前缀，即用户输入什么样的网址访问到该解析目标。如果主机记录为www ，则用户需要输入www.[你的域名]才能访问到该解析目标。如果为@，则直接输入域名即可。如果不添加www ，则通过www+域名方式访问的用户将访问失败，@同理，其余的也同理。 记录类型为解析目标的类型，如果想把该域名绑定到一个ip地址，则选A，如果目标为一个网址，则选CNAME。 这里有两种绑定方法，一种是选CNAME然后在记录值填写 [yourname].github.io ，另一种是选A，然后通过cmd命令行输入 ping [yourname].github.io 获取ip地址，记录值里填入ip地址。 获取 [yourname].github.io 的ip地址，如图，ping通后会显示ip地址。 记录值根据选择的记录类型进行填写。线路选默认。TTL为缓存生效时间，默认600秒即可，即10分钟后生效(实际大约需要5 分钟)。填写完毕后点击保存。可以为域名填写多个记录， 如图 前两个是为github pages绑定时添加的记录，一个www、一个@，这样可以让用域名直接访问的和加了www访问的用户都能访问到自己的博客 (部署到服务器后就不再用了所以暂停了)。接下来两条A类型是将网站部署到自己的服务器时，把域名解析到了自己的服务器IP地址，这样可以通过www、或者直接输入域名的方式来访问自己的服务器。最后一条是绑定的七牛云，用来当做博客的图床。每条记录后面都有操作可以进行修改以及暂停和开启。 绑定到github pages登陆到自己的github，进入网站绑定的仓库，进入设置 往下找到GitHub Pages，在Custom domain填入刚刚购买的域名，点击save保存。勾选Enforce HTTPS则开启HTTPS安全协议。 然后到本地博客source文件夹下新建文件CNAME，输入内容为自己的域名，并将文件尾缀如.txt等删掉然后保存即可。(没有的话貌似每次将代码从本地推到github都会使域名访问404，因为每次推送都会覆盖原本的仓库代码。所以把CNAME文件放在source中，使每次推送都会建立一个CNAME) 至此，github pages的域名绑定完成了，稍等片刻即可尝试使用域名访问。 源代码保存到github创建分支在仓库中的文件列表的左上方，点击Branch。 搜索 source （分支名，自定义），会提示未找到，是否创建，点击即可创建该分支 设置新建分支为默认分支进入设置，左边的列表中选择 Branches，默认分支为master，改为新建的分支，然后点击Update更新。 同步配置首先随便找个地方新建一个文件夹，将你的仓库克隆下来。 打开新建的文件夹，右键空白处点击Git Bash Here 然后输入下方命令克隆文件 1git clone 【你的仓库地址】 仓库地址获取方法： 点击红框内的按钮复制，然后粘贴到clone后面即可，用空格与clone隔开。 克隆完成后，该文件夹内会出现【你的用户名】.github.io文件夹，进去拷贝.git文件夹到本地的博客根目录，然后这个新建的文件夹就可以删除了。 接下来在博客根目录右键空白处，打开git bash，输入下方命令，警告不用理会，若没出现报错就没问题。 会需要github的帐号密码，填一下就OK了。 1234git remote add origin 【你的仓库地址】git add .git commit -m &quot;【描述，随便写】&quot;git push origin 【你的保存源代码的分支名】 描述部分的效果如图，会将内容显示在该分支上。 每次推送时，输入的描述都会在这次推送时更新的文件后面显示出来。 接下来每次想保存时，输入下方指令即可， 123git add .git commit -m &quot;【描述】&quot;git push 但每次都要输入这么多很麻烦，可以创建一个脚本文件，在博客根目录下新建一个txt文本文件，名字随意自己能知道是保存用的就行，将上方三条指令写进去，描述写好后以后固定都是这个，然后将文件改为.sh结尾。也可以直接建一个.sh尾缀文件，然后用编辑器打开写入。这样以后每次运行这个脚本文件就会自动执行上面三条指令，完成推送。 本地同步到github就完成了，但要注意的是只保存了关键文件，如主题、文章、配置等。 node_modules文件夹和public文件夹是没有保存上去的，public文件夹是生成的静态页面，不需要保存，若迁移后直接生成就有了。 node_modules文件夹存放着需要用到的插件，如果想保存的话，打开.gitignore文件，把里面的node_modules删掉保存即可，但是这样会造成每次保存都需要很久时间，因为里面东西太多了，看个人需要决定是否需要保存。 只要配置文件里面的deploy里的branch的值是master的话，生成的静态页面会推送到master分支 配置完成后，若以后要迁移到其他的服务器或者电脑上，只需要安装好git、Node.js、hexo，然后使用hexo init命令初始化一个根目录，再克隆下来就行了，若不指定克隆分支的话，会克隆默认分支，即设置好的保存博客源代码的分支。","categories":[{"name":"笔记","slug":"笔记","permalink":"https://lluuiq.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://lluuiq.com/tags/hexo/"},{"name":"github","slug":"github","permalink":"https://lluuiq.com/tags/github/"}]}]}